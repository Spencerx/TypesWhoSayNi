\documentclass[format=acmsmall, screen, review, anonymous, timestamp]{acmart}
\bibliographystyle{ACM-Reference-Format}
\newcommand{\blind}[2]{#2}

%%%%% Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{MnSymbol}
\usepackage{proof}
\usepackage{upgreek}
\usepackage{pig} \ColourEpigram
\usepackage{stmaryrd}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsthm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%% Theorems etc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{thm}{Theorem}[section]
\newtheorem{defn}[thm]{Definition}
\newtheorem{eg}[thm]{Example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%% Misc %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\bsl}{\texttt{\symbol{92}}}
\newcommand{\memo}[1]{\textbf{#1}}
\newcommand{\fsl}{\texttt{/}}
\newcommand{\emp}{\varepsilon}

\newcommand{\A}[1]{\texttt{#1}}
\newcommand{\V}[1]{\purple{\mathit{#1}}}

\newcommand{\Pa}[1]{\texttt{(}\black{#1}\texttt{)}}
\newcommand{\Bk}[1]{\texttt{[}\black{#1}\texttt{]}}
\newcommand{\Bc}[1]{\texttt{\string{}\black{#1}\texttt{\string}}}
\newcommand{\dt}{\texttt{.}}
\newcommand{\cn}[2]{\Pa{#1 \dt #2}}
\newcommand{\hb}{\texttt{:}}
\newcommand{\ra}[2]{#1 \,:\, #2}
\newcommand{\grp}[1]{\{ #1 \}}
\newcommand{\Ne}{\underline}
\newcommand{\PI}{\blue{\Uppi}}
\newcommand{\la}[1]{\bsl #1\:}
\newcommand{\x}[1]{\V{x_{\mathrm{#1}}}}
\newcommand{\Ty}{\blue{\star}}

\newcommand{\rred}{\leadsto^\ast}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\title{Inputs $\langle$ Subjects $\rangle$ Outputs}
\author{Conor Mc Bride}
\maketitle

\section{An Example from 1971}

Let me give you a flavour of what is to come by reconstructing Martin-L\"of's 1971 type theory, in a bidirectional style. I chose this system because it small enough to learn from, but also because it is inconsistent, ruling out reliance on totality of normalisation.


\subsection{Syntax}

Let us establish our grammar of terms. I define two sorts of \emph{scoped} terms by mutual induction --- a scope is a sequence of sorted variables which grows on the right.
\newcommand{\syso}{\textsc}
\newcommand{\chk}[1]{\syso{chk}(#1)}
\newcommand{\syn}[1]{\syso{syn}(#1)}
\newcommand{\bva}[2]{\syso{#1}\,{\V{#2}}}
\newcommand{\PT}[3]{\PI\:#1\:\V{#2}.#3}
\newcommand{\LA}{\red{\uplambda}}
\newcommand{\LF}[2]{\LA\:\V{#1}.#2}

\[\begin{array}{rrl}
    \chk\gamma & ::= & \Ne{\syn\gamma} \\
               &   | & \Ty \\
               &   | & \PT {\chk\gamma}x{\chk{\gamma,\bva{syn}x}}\\
               &   | & \LF x{\chk{\gamma,\bva{syn}x}}
                       \medskip\\
    \syn\gamma & ::= & \ra{\chk\gamma}{\chk\gamma}\\
               &   | & \syn\gamma\;\chk\gamma
\end{array}\]

I have specified this syntax in an informal `scoped Backus-Naur form' notation. Each sort yields a nonterminal symbol carrying a scope, e.g. $\chk\gamma$. The notation ${\x{}.}$ indicates a variable binding, where we name the variable in the grammar so that we can bring it into scope. As you can see, $\chk\gamma$ embeds $\syn\gamma$ and adds $\Ty$ (the type of types, dependent function types and Curry-style functions. Meanwhile, $\syn\gamma$ includes type annotated terms in $\chk\gamma$, and applications where the function is also in $\syn\gamma$.

Implicitly, the grammar for every sort extends to include all the variables in scope of that sort. In particular $\x{} \in \syn{\gamma,\bva{syn}x,\gamma'}$.
We can always $\alpha$-convert to distinguish the variable names in scopes. A variable name is but a human-friendly way to indicate a position in a scope: machines fare better without them.

Scoped syntax is functorial with respect to the category of \emph{thinnings} --- order-preserving embeddings on scopes --- exactly because scope lookup is covariant with respect to thinnings and scope extension is functorial on thinnings. I write $\theta : \gamma \le \delta$ to indicate that $\theta$ witnesses such an embedding from $\gamma$ to $\delta$. You can think of $\theta$ as a vector of bits whose length is that of $\delta$ and whose 1s show which of $\delta$'s entries were embedded from $\gamma$.

Meanwhile, we acquire a category whose objects are scopes and whose morphisms $\gamma \Rightarrow \delta$ are simultaneous \emph{substitutions} mapping every $\bva {sort}x \in \gamma$ to some term in $\syso{sort}(\delta)$. Again, scope extension acts functorially on substitutions, exactly because thinnings act on terms.


\subsection{Typing Version $\upalpha$}

For each of our syntactic sorts, let us have a judgment form whose purpose is to establish trust in terms of that sort. The grammar of \emph{formal judgments} is
\newcommand{\jud}[1]{\syso{jud}(#1)}
\[\begin{array}{rrl@{\qquad\qquad}l}
    \jud\gamma & ::= & \chk\gamma \ni \chk\gamma & \mbox{checking a prior type}\\
               &   | & \syn\gamma \in \chk\gamma & \mbox{synthesizing a posterior type}\\
               &   | & \chk\gamma = \chk\gamma & \mbox{type equality}\\
               &   | & \x{}\in\chk\gamma \vdash \jud{\gamma,\bva{syn}x}
                       & \mbox{local extension}
\end{array}\]
where entire \emph{contexts} do not appear in formal judgments, only local extensions of contexts, recording what is known about a variable when we move under its binder. Note that thinnings act perfectly well on judgments, so we can bring everything we already knew into the current scope whenever we make a local extension. A local extension thus acts as an additional axiom, extending the rules just as bound variables extend terms, obviating the need to write a `variable rule' at all.
What are the rules for the other term forms? For checking, we have
\[
  \Rule{e\in S\quad S=T}{T\ni \Ne e}\qquad
  \Axiom{\Ty\ni \Ty}\qquad
  \Rule{\Ty\ni S\quad \x{}\in S\vdash \Ty\ni T}{\Ty\ni \PT SxT}\qquad
  \Rule{\x{}\in S\vdash T\ni t}{\PT SxT \ni \LF xt}
\]
Meanwhile, for synthesis, we have
\[
  \Rule{\Ty\ni T \quad T \ni t}{\ra tT \in T} \qquad
  \Rule{f\in \PT SxT \quad S\ni s}{f\;s\in \{\x{}\mapsto \ra sS\}T}
\]
and, for the time being, we may take the equality judgment to be up to renaming of bound variables:
\[
  \Axiom{T = T}
\]

Now, these rules are syntax directed in the $t$ of $T \ni t$ and the $e$ of $e \in S$. They have no extraneous premises checking, e.g., that the domain of an abstraction is a type. How are we to understand why these rules are sensible? Each judgment form has a \emph{precondition} and a \emph{postcondition}.
\newcommand{\pre}[1]{\textsc{pre}(#1)}
\newcommand{\post}[1]{\textsc{post}(#1)}
\[\begin{array}{|r|c|l|}
    \hline
    \pre J & J & \post J \\
    \hline
    \Ty\ni T & T \ni t & \top \\
    \top & e\in S & \Ty\ni S \\
    \Ty\ni S \wedge \Ty\ni T & S = T & \top \\
    \Ty\ni S \wedge \x{}\in S\vdash\pre J& \x{}\in S\vdash J & \x{}\in S\vdash \post J \\
    \hline
\end{array}  \]
Now, suppose we are working in a context of hypothetical judgments for each of which the precondition implies the postcondition (here, effectively that the types of the known variables are well formed). For any derivation of judgment $J$ in such a context, we should like to know that $\pre J$ is enough to ensure the pre- and postconditions for every judgment in the whole derivation. In other words, if we start well, we stay well. This property will hold for actual derivations of actual judgments if we can establish a suitable property of the rules from which derivations are composed. Let us find our way to that property.

To establish a judgment as the conclusion of a rule, one must demand that the premises hold. To demand a premise one must guarantee its precondition, but once the premise has been derived, one may rely on it and on its postcondition. Each judgment thus gives us an operator on propositions
\[
  \check{J}\;P = \pre J \wedge ((J \wedge \post J) \Rightarrow P)
\]
which explains what use it is to demand $J$ in the cause of establishing $P$. Of course, if one has a sequence of premises, $J_1\;\ldots\;J_n$, one can form the composition
\[
  (\circ_i\check{J_i})\;P = \check{J_1}\:(\ldots (\check{J_n}\:P)\ldots)
\]
which amounts to the assertion that the precondition for each premisefollows from the postconditions of the prior premises, and that $P$ follows from all of their postconditions put together. Now, for a rule
\[
  \Rule{J_1\;\;\ldots\;\;J_n}{J}
\]
we must show why
\[
  \pre J \;\Rightarrow\;  (\circ_i\check{J_i})\;(\post J)
\]
i.e., that if we assume it is reasonable to enquire after the rule's conclusion in the first place, then it is also reasonable to enquire after the premises and, upon their successful derivation, to deliver the conclusion's postcondition. For an axiom, with zero premises, this reduces to $\pre J \Rightarrow \post J$, so our assumption about the context amounts to treating hypothetical judgments as local axioms.

Let us visit each of our rules in turn.
\begin{itemize}
\item $\Rule{e\in S\quad S=T}{T\ni \Ne e}\quad$
  We are given $\Ty\ni T$. The first premise has nothing to check, and it gives us $\Ty\ni S$. So we have established the precondition for checking $S=T$.
\item $\Axiom{\Ty\ni \Ty}\quad$
  The postcondition for this rule is derived by this rule!
\item $\Rule{\Ty\ni S\quad x\in S\vdash \Ty\ni T}{\Ty\ni \PT SxT}\quad$ We are given $\Ty\ni\Ty$, which we know anyway, and that is what we must deliver to invoke the first premise. Once we know $\Ty\ni S$, we may extend the context with $\x{}\in S$. Again, $\Ty\ni\Ty$ for the second premise.
\item $\Rule{x\in S\vdash T\ni t}{\PT SxT \ni \LF xt}\quad$
  We know $\Ty\ni \PT SxT$, so by inversion,
  $\Ty\ni S$ and $\x{}\in S\vdash \Ty\ni T$. Correspondingly, we may extend the context with $\x{}\in S$ and then check $T\ni t$.
\item $\Rule{\Ty\ni T \quad T \ni t}{\ra tT \in T}\quad$
  For the first premise, we need $\Ty\ni\Ty$. For the second premise we need the first premise. The conclusion postcondition is again the first premise.
\item $\Rule{f\in \PT SxT \quad S\ni s}{f\;s\in \{\x{}\mapsto \ra sS\}T}\quad$
  The first premise promises us $\Ty\ni \PT SxT$, so by inversion,
  $\Ty\ni S$ and $\x{}\in S\vdash \Ty\ni T$. We may thus invoke the second premise. Now, for the conclusion postcondition, we may deduce $\ra sS\in S$ from $\Ty\ni S$ and $S\ni s$. Substituting this derivation for all uses of the hypothetical $\x{}\in S$, we obtain $\Ty\ni \{\x{}\mapsto \ra sS\}T$.
\end{itemize}

In that last case, I made use of the fact that substitution extends from terms to derivations. How do I know? I have been very careful to ensure that none of my rules ever says anything about \emph{free} variables. Substitutions preserve everything but free variables, so the only parts of derivations where they have noticeable impact are where we appeal to hypothetical judgments. Stability under substitution is exactly that if a substitution preserves all the hypothetical judgments, then it preserves all derivations. Note that an ill typed substitution may falsify the preconditions for a derivation to be meaningful, but it will still preserve the derivation.

So, we get out as much sense as we put in! However, this system does no computation, so we had better add it and see what sort of mess it makes.

  
\subsection{Computation}

\newcommand{\ma}[2]{\{#1\mapsto #2\}}

Let us define some \emph{contraction schemes}, from which we shall extract a notion of untyped conversion. Of course, there are more sophisticated ways to account for computation, but let us begin with the basics.
\[
  (\upsilon)\quad \Ne{\ra tT} \;\leadsto\; t
  \quad\quad\quad\quad 
  (\beta)\quad(\ra{\LF {\x{}}t}{\PT {S}{\x{}}T})\:s \;\leadsto\; \{x\mapsto\ra sS\}(\ra tT)
\]
The idea here is that function types drive function applications. The $\upsilon$ rule deletes the type annotation from a term which is not being applied. The $\beta$ rule uses the type information in the redex to annotate the reduct and also to annotate the argument so that, wherever the variable is substituted, further computation may be enabled.

We might think of a contraction scheme as saying that any substitution instance of the left-hand side contracts to the corresponding substitution instance of the right-hand side, but where do these substitutions come from? They come from \emph{matchings}: a matching is a formula (such as we write in these rules) with each schematic variable annotated by a term that it maps to. E.g.,
\[
  \Ne{\ra{\ma t{\LF x{\Ne{\x{}}}}}{\ma T{\PT\Ty\_\Ty}}}
\]
is a matching for $\upsilon$-contraction. Matchings support three erasures:
\begin{itemize}
\item if you replace $\ma vt$ by $v$, you get a formula which is the \emph{pattern} of the matching;
\item if you replace $\ma vt$ by $t$, you get a term which is the \emph{instance} of the matching;
\item if you erase everything outside the $\ma vt$s, you get the \emph{substitution} of the matching.
\end{itemize}
Moreover, we call any subterm of any $t$ in a matching annotation $\ma vt$ a \emph{residual} of the matching, and we can be sure that such residuals are copied intact into the matching substitution.
So, a \emph{redex} is a matching instance of the left-hand side of a contraction scheme.
We say the contraction schemes are \emph{orthogonal} if no term is a redex for two of them, and whenever a redex has a proper subterm which is also a redex, the latter is a residual of the former's matching. E.g., $\upsilon$ and $\beta$ are clearly orthogonal, because embedding, $\Ne{-}$, occurs only and outermost in $\upsilon$ while application, $-\:-$, occurs only and outermost in $\beta$.

Orthogonal contraction schemes give rise to confluent reduction systems, essentially by Takahashi's proof. Once we have contraction schemes, we can systematically derive an inductive definition of \emph{parallel reduction}. We begin with structural rules for variables and all syntactic productions, thus ensuring that the relation is at least reflexive and closed under all syntactic contexts.
\newcommand{\PRR}{\triangleright}
\newcommand{\prr}[2]{#1\:\PRR\,#2}
\newcommand{\PRS}{\PRR^{\!\ast}}
\newcommand{\prs}[2]{#1\:\PRS\,#2}
\[\begin{array}{c}
    \Axiom{\prr{\x{}}{\x{}}} \\
    \Rule{\prr e{e'}}{\prr{\Ne e}{\Ne{e'}}}\quad\quad
    \Axiom{\prr\Ty\Ty}\quad\quad
    \Rule{\prr S{S'} \quad \prr T{T'}}{\prr{\PT SxT}{\PT{S'}x{T'}}}\quad\quad
    \Rule{\prr t{t'}}{\prr{\LF xt}{\LF x{t'}}} \\
    \Rule{\prr t{t'}\quad \prr T{T'}}{\prr{\ra tT}{\ra{t'}{T'}}}\quad\quad
    \Rule{\prr f{f'} \quad \prr s{s'}}{\prr{f\;s}{f'\;s'}}
\end{array}\]
Then, we add rules generated from each contraction schemes by renaming all the schematic variables in the reduct and giving premises which allow the schematic variables in the premises to reduce to their renamed variants.
\[
  \Rule{\prr t{t'}}{\prr{\Ne{\ra tT}}{t'}} \quad\quad
  \Rule{\prr t{t'}\quad \prr S{S'} \quad \prr T{T'}\quad\prr s{s'}}
    {\prr{(\ra{\LF {\x{}}t}{\PT {S}{\x{}}t})\:s}{\{x\mapsto\ra{s'}{S'}\}(\ra{t'}{T'})}}
  \]

\newcommand{\dev}[1]{\mathbf{dev}(#1)}
We can also define the \emph{development}, $\dev{t}$, of a term $t$, which aggressively contracts redexes from the outside in.
\[\begin{array}{lcl}
    \dev{\Ne{\ra tT}} &=& \dev t \\
    \dev{(\ra{\LF {\x{}}t}{\PT SxT})\:s} &=&
                                                    \{x\mapsto\ra{\dev{s}}{\dev{S}}\}(\ra{\dev t}{\dev T}) \\
    \multicolumn{3}{l}{\hspace*{-0.3in}\textit{and otherwise,}}\\
    \dev{\x{}} &=& \x{}\\
    \dev{\Ne e} &=& \Ne{\dev{e}} \\
    \dev\Ty &=& \Ty\\
    \dev{\PT SxT} &=& \PT{\dev S}x{\dev T}\\
    \dev{\LF xt} &=& \LF x{\dev t}\\
    \dev{\ra tT} &=& \ra{\dev t}{\dev T}\\
    \dev{f\;s} &=& \dev f\; \dev s
\end{array}  \]

By construction, $\prr t{\dev t}$. Moreover, whenever we develop a redex, we develop all of its residuals, too. By orthogonality, that means we develop all the redexes present in the input, and as a consequence, whenever $\prr st$, we also have $\prr t{\dev s}$. We acquire the \emph{diamond} property for $\PRR$
\[
  \forall s,p,q.\; \prr sp \wedge \prr sq \;\Rightarrow\; \exists r.\; \prr pr \wedge \prr qr
\]
by taking $r = \dev s$. This extends to the same for $\PRS$, the reflexive-transitive closure of $\PRR$, by tiling a parallelogram with diamonds. We further obtain that the equivalence closure of $\PRR$ amounts to having a common reduct.

With this machinery in place, we may add computation to our theory in the form of two new rules.
We may compute a type before checking it or after synthesizing it.
\[
  \Rule{\prr T{T'} \quad T'\ni t}{T\ni t}\qquad \Rule{e\in S\quad \prr S{S'}}{e\in S'}
\]
Note that, with these rules in place, we may construct derivations with multiple computation steps on either side of the $S = T$ check, so that we effectively allow type conversion at the change of direction. Everywhere we insist on a particular type form, $\Ty$ or $\PT SxT$, we allow only reduction, but we also have that whenever a type is convertible to a canonical form, it can reach a convertible canonical form by reduction alone.

However, if we want to argue that preconditions ensure postconditions, we shall have to account for the impact of computation, which is our next task.


\subsection{Subject Reduction}

Judgments establish trust, so it would be unfortunate, to say the least, if computation were to damage that trust. Of course, we can speak only to forward computation: goodness knows monstrous arguments a backward $\beta$-step might conjure for a vacuous abstraction. It is fortunate, then, that our rules require only forward computation. The tricky part of proving that forward computation preserves judgments is the movement of subterms between the judgments' places: application copies the argument into the type, function type formation copies the domain into the context, and so on. If we are to proceed by mutual induction on derivations, we shall need to be sufficiently flexible in the statement of our goals, if our induction hypotheses are to be fit for purpose.

Like a rabbit from a hat, I produce the following claims:
\begin{itemize}
\item Suppose that under some $\x{i} : S_i$ we have $T\ni t$, and that $\prs{S_i}{S'_i}$, $\prs T{T'}$ and $\prr t{t'}$.\\Then under $\x{i} : S'_i$ we have $T'\ni t'$.
\item Suppose that under some $\x{i} : S_i$ we have $e\in S$, and that $\prs{S_i}{S'_i}$, $\prr e{e'}$.\\Then there exists some $S'$ such that $\prs S{S'}$ and under $\x{i} : S'_i$ we have $e'\in S'$.
\end{itemize}
That is, if we know a judgement and we allow the devil to compute contexts and checked types forwards arbitrarily, but the terms only by \emph{one} parallel reduction step, then we may recover our judgment by sufficiently computing synthesized types. If we let the devil compute the context, we shall certainly need to control the computation of synthesized types, if we are to have any chance of catching up with appeals to hypothetical judgments: when the devil computes $\prs{S_i}{S'_i}$, we must be able to deploy a copycat strategy to recover $\x{i}\in S_i$ as $\x{i}\in S'_i$. Let us see how the game plays out for the rest of the rules: we obtain a case for each term former taking a \emph{structural} parallel reduction step, together with cases for actual contraction.
\begin{itemize}
\item $\Rule{e\in S\quad S=T}{T\ni \Ne e}\quad \prs{T}{T'}\quad \prr e{e'}\quad$ We must show $T'\ni \Ne{e'}$.\\
  By induction hypothesis, $e'\in S'$ for some $\prs S{S'}$. As $S = T$, confluence gives us a common reduct, $U$ for $S'$ and $T'$. The precomputation rule allows us to derive $T'\ni\Ne{e'}$ from $U\ni\Ne{e'}$, which follows by change of direction from $e'\in U$, derived by postcomputation from $e'\in S'$.
\item $\Axiom{\Ty\ni \Ty}\quad \prs\Ty\Ty\quad \prr\Ty\Ty\quad$ We must show $\Ty\ni\Ty$, which holds.
\item $\Rule{\Ty\ni S\quad x\in S\vdash \Ty\ni T}{\Ty\ni \PT SxT}\quad \prs\Ty\Ty\quad \prr{\PT SxT}{\PT{S'}x{T'}}\quad$ We must show $\Ty\ni \PT{S'}x{T'}$.\\
  We had $\Ty\ni S$ and $\x{}\in S\vdash \Ty\ni T$ in the typing derivation, and $\prr S{S'}$ and $\prr T{T'}$ from reduction. One induction hypothesis yields $\Ty\ni S'$; the other yields
  $\x{}\in S'\vdash \Ty\ni T'$ as we may compute in the context. We may thus use the same rule to deduce the goal.
\item $\Rule{x\in S\vdash T\ni t}{\PT SxT \ni \LF xt}\quad \prs{\PT SxT}{\PT{S'}x{T'}}\quad \prr{\LF xt}{\LF x{t'}}\quad$ We must show $\PT{S'}x{T'}\ni \LF x{t'}$.\\
  Allowing for precomputation, we must have had $\x{}\in S\vdash T''\ni t$ for some $\prs{T}{T''}$ from the typing derivation. From reduction, we must have $\prs S{S'}$ and $\prs T{T'}$. Taking $T'''$ as the common reduct of $T'$ and $T''$, we may derive $\x{}\in S'\vdash T'''\ni t$ by induction hypothesis and obtain the goal by precomputation and abstraction.
\item $\Rule{\Ty\ni T \quad T \ni t}{\ra tT \in T}\quad \prr{\ra tT}{\ra{t'}{T'}}\quad$
  We must show $\ra{t'}{T'} \in R$ for some $\prs TR$.\\
  We must have had $\prr t{t'}$ and $\prr T{T'}$. By induction hypothesis, $\Ty\ni T'$ and, computing the type, $T'\ni t'$. We thus choose $R = T'$ and reapply the same rule.
\item $\Rule{f\in \PT SxT \quad S\ni s}{f\;s\in \{\x{}\mapsto \ra sS\}T}\quad \prr{f\:s}{f'\:s'}\quad$ We must show $f'\:s'\in R$ for some $\prs{\{\x{}\mapsto \ra sS\}T}R$.\\
  By induction, we obtain $f\in \PT {S'}x{T'}$ for some $\prs S{S'}$ and $\prs T{T'}$. Computing the type, our other hypothesis yields $S'\ni s'$. We may thus take
  $R = \{\x{}\mapsto \ra{s'}{S'}\}T'$ because computation is stable under substitution, and reapply.
\item $\Rule{\prr T{T'} \quad T'\ni t}{T\ni t}\quad \prs T{T''} \quad \prr t{t'}\quad$ We must show $T''\ni t'$.\\
  Confluence gives $T'''$ as common reduct of $T'$ and $T''$, so induction yield $T'''\ni t'$, then precomputation yields the goal.
\item $\Rule{e\in S\quad \prr S{S'}}{e\in S'}\quad \prr e{e'}\quad$ We must show $e'\in R$ for some $\prs{S'}R$.\\
  By induction, $e'\in S''$ for some $\prs S{S''}$. Confluence yields $S'''$ as common reduct of $S'$ and $S'''$. We take $R = S'''$ and
  derive goal from hypothesis by postcomputation.
\item $\Rule{\ra t{T_0}\in T_1\quad T_1=T}{T\ni \Ne{\ra t{T_0}}}\quad \prs T{T'}\quad \prr{\Ne{\ra t{T_0}}}{t'}\;\mbox{where}\;\prr t{t'}\quad$
  We must show $T'\ni t'$.\\
  We must have had $T_0\ni t$ and $\prs{T_0}T$. Hence $\prs{T_0}{T'}$ and the induction hypothesis yields the goal.
\item $\Rule{(\ra{\LF xt}{\PT{S'}x{T'}})\in \PT SxT \quad S\ni s}{(\ra{\LF xt}{\PT{S'}x{T'}})\;s\in \{\x{}\mapsto \ra sS\}T}$\\
  $\prr{(\ra{\LF xt}{\PT{S'}x{T'}})\:s}{\{x\mapsto\ra{s'}{S''}\}(\ra{t'}{T''})}\;\mbox{where}\;
  \prr s{s'}\;\;\prr{S'}{S''}\;\;\prr{T'}{T''}\;\;\prr t{T'}$\\
  We must show $\{x\mapsto\ra{s'}{S''}\}(\ra{t'}{T''})\in R$ for some $\prs{\{\x{}\mapsto \ra sS\}T}R$.\\
  We must have had $\prs{S'}S$ and $\prs{T'}T$. Further, we must have had $\x{}\in S_0\vdash T_0\ni t$ for some $\prs{S'}{S_0}$ and $\prs{T'}{T_0}$. Moreover, we must have had subderivations of
  $\Ty\ni S'$ and $x\in S'\vdash \Ty\ni T'$.
  By confluence, obtain common reducts
  $S_c$ and $T_c$. By induction, obtain $S_c\ni s'$ and $\x{}\in S_c\vdash T_c\ni t'$.
  Precomputation yields $S''\ni S'$ and $\x{}\in S_c\vdash T''\ni t'$.
  Induction yields $\Ty\ni S''$ and $x\in S_c\vdash \Ty\ni T''$ as $\prr{S'}{S''}$
  and $\prr{T'}{T''}$. We thus obtain
  $\ra{s'}{S''}\in S_c$ and $\x{}\in S_c \vdash \ra{t'}{T''}\in T_c$.
  Stability under substitution tells us that
  $R = \{x\mapsto \ra{s'}{S''}\}T_c$ yields the goal. We could also choose
  $R = \{x\mapsto \ra{s'}{S_c}\}T_c$.
\end{itemize}

So, my lucky rabbit did the trick! Whenever I needed an induction hypothesis, the term in question had computed only by parallel reduction. Whenever checked types precomputed or synthesized types postcomputed in competing ways, confluence always gave me a common reduct which I could then choose either in a checking hypothesis or a synthesis conclusion. It is almost as if the places in the judgments have some sort of \emph{orientation} which aligns with the rules for computation and the statements of subject reduction!

With subject reduction in place, it is straightforward to see that the preconditions and postconditions which demand validity of types are now stable with respect to the forward computations permitted by the rules. Repairing the na\"\i{}ve proof to cope with computation is safely left as an exercise.

Now, the real purpose of this paper is to demystify the alignment by which the above proof comes out. Let us look not at the rabbit but rather at the hat I pulled it from, for this trick is not magic, but millinery!


\section{Judging Judgments; Ruling Rules}

Bidirectional type systems come with a sense that checked types flow `inward' while synthesized types flow `outward', but it is not at all standard to bake this distinction into the very idea of what it is to be a judgment form. In our statement of subject reduction, we quantified universally over how the inputs computed and existentially over how the outputs computed, so perhaps the distinction is worth observing. More subtly, we have some places in judgment forms to which we attach conditions explaining what we rely on or guarantee for the things in those places, but others with no such condition; and in our example, that distinction coincided with the places which had $\PRS$ in the statement of subject reduction and those which were permitted only $\PRR$. Let us refine the notion of \emph{judgment} form to make this analysis more explicit, then explore the consequences of doing so.


\subsection{Modes for Judgment Forms}

We may think of a judgment as a structured interaction between a \emph{client} who \emph{proposes} and a server who \emph{avers}. The data in its places are transmitted either from client to server or vice versa, but moreover, some sort of \emph{guarantee} is made about the data by one to the other. We may classify judgment form places with a \emph{mode} according to the senders of their data and the makers of the guarantees about them:

\begin{tabular}{|c||c|c|}
  \hline
  \textbf{Mode} & \textbf{Sender} & \textbf{Guarantor} \\
  \hline \hline
  \textbf{Input} & client & client \\
  \textbf{Subject} & client & server \\
  \textbf{Output} & server & server \\
  \hline
\end{tabular}

We might imagine a fourth `whataboutery' mode in which the server sends data to the client in the hope of a guarantee, but it seems unlikely to play a role in typechecking interactions, at least for complete terms, so let us let slip that idea for the present. Likewise, one might imagine judgment forms with arbitrary interaction protocols, but we will get a long way with only those whose inputs precede their subjects which in turn precede the outputs.

Let us specify judgment forms uniformly for all scopes by writing grammar productions using syntactic sorts for nonterminal symbols, punctuation as we see fit, a `$\langle$' between inputs and subjects as far right as possible, and a `$\rangle$' between subjects and outputs as far left as possible after the `$\langle$'. Our example system has
\[\begin{array}{l}
    \syso{chk} \ni \langle \syso{chk} \rangle \\
    \langle \syso{syn} \rangle \in \syso{chk} \\
    \syso{chk} = \syso{chk} \langle\rangle 
  \end{array}\]


\subsection{Contracts and Contexts}

We may think of a judgment with subjects as a \emph{validator} for those subjects. The guarantees made by clients about inputs and servers about outputs are validity guarantees. Each judgment form must thus have a \emph{contract} of preconditions and postconditions: the preconditions are for each input a judgment in which that input stands as a subject; the postconditions are for each output a judgment in which that output stands as a subject.

We shall need at least those hypothetical judgments about a variable in which it stands as a subject, for how else are we to validate the uses of that variable? That is why, when we bind variables $\x{}$ in sort $\syso{syn}$, we take hypotheses of form $\x{}\in S$. We thus acquire that derivations respect well sorted simultaneous substitutions, provided their hypotheses similarly substituted are derivable: the latter derivations, suitably thinned, substitute into places where the hypothetical judgments were invoked, exactly extending substitutions from the syntax of terms to the derivations of judgments. It is also reasonable to consider hypothetical judgments in which the variable of concern stands as an input, e.g., equipping a variable already declared with a \emph{definition}. What matters is to specify the allowable instantiations of the variable compatibly with this grafting of derivations.

Hypothetical judgments must keep their contracts, too, so it is a duty for any rule which has a premise in a locally extended context to ensure that the corresponding contract is met. In our example, this means that whenever we say $\x{}\in S\vdash \ldots$ we must be able to explain why $\Ty\ni S$, because that is what the type synthesis postcondition promises.


\subsection{Patterns versus Expressions}

In inference rules as we know them, we write formulae which employ \emph{schematic variables} to stand for the concrete objects for which the rules may be used to construct derivations. A rule is valid for any syntactically correct instantiation of its schematic variables. That is, the schematic variables are bound \emph{implicitly} by universal quantifiers under which the rule amounts to the implication of its conclusion from the conjunction of its premises. When deploying rules to make derivations, we are obliged to instantiate those implicitly quantified variables with whatever insight and inspiration we can muster. For example, we might give a rule for constructing dependent pairs as follows:\\
\parbox{4.5in}{
\[
  \Rule{s:S \quad t:\{x\mapsto s\}T}
  {s,t : \blue{\Upsigma}\:S\:\V{x}.T}
    \]}\hfill \mbox{(danger!)}
Given $s$ and $t$, one constructs derivations by dreaming up a suitable $S$ and a suitable $T$ depending on $\x{}$, but how is this dreaming to be done? If we are \emph{checking} types, it is reasonable to dismantle the pair type to extract $S$ and $\V{x}.T$. However, if we are \emph{synthesizing}, we may readily obtain $S$ from the first premise, but the second premise gives us but one substitution instance of $T$, from which there is no general way to infer the dependency pattern. Inverting substitution is more magic than you have the right to expect!

Our modes make it clear who sends and who receives the data in a typing rule:
\begin{quote}
  \emph{A rule is the server for its conclusion and the client for its premises.}
\end{quote}
Hence, each rule receives the inputs and subjects of its conclusion, sends the inputs and subjects of its premises, receives the outputs of its premises, and sends the outputs of its conclusion. These communications can happen, in principle, in any causally sustainable order, but it is often an adequate simplification to demand that they happen \emph{clockwise}, starting from wherever in the conclusion has the $\rangle$ in its judgment form.

We may now make a distinction between the \emph{patterns} by which rules may \emph{analyse} the data they receive and the \emph{expressions} with which they \emph{construct} the data they send. We receive, in return, two benefits.

Firstly, implicit universal quantification of the schematic variables gives way to the explicit binding of each schematic variables in exactly one pattern, making it clear who is respnsible for delivering that information, and just as clear what is \emph{in scope} at the time. The use sites of schematic variables should be in expressions only, and then only in places where the variables in scope for them are either captured or substituted. Indeed, it is clear by inspection which variables are not in scope at use sites and must thus be substituted, hence we may write substitutions without explicit naming, e.g.
\[\Rule{f\in\PT SxT\quad S\ni s}
  {f\:s\in\{\ra sS\}T}
\]
is perfectly meaningful, because the binding site of $T$ is in the first premise with $\x{}$ in scope, but what is missing from scope in the conclusion's output is exactly such an $\x{}$, so there is no doubt as to what is being substituted by $\ra sS$.

Secondly, we become free to gain reassurance at the expense of power by reducing the expressivity of the pattern language to avoid magic that we are better off without. At the very least, we should disallow general substitutions in patterns, exactly because we cannot invert them. In our application example, the substitution is safely in an expression position; in our dependent pair example, if we read $:$ as $\in$, we see that the second premise's substitution occurs in a pattern position and is thus unwise. Later, we should also consider how further restricting the pattern language in some places might assist in establishing metatheoretical properties, e.g., by ensuring that we never demand a type matches a pattern that computation can destroy. As in `Theorems for Free', the less we can look, the more we are promised. When ignorance is bliss, 'tis folly to be wise.


\subsection{Citizens or Subjects?}

So far, it might seem that inputs and subjects differ only morally: who makes what guarantee? However, the distinction runs deeper. The schematic variables brought into scope by matching on a conclusion input or a premise output are \emph{citizens}, in (meta-)scope for use in subsequent premise input or conclusion output expressions. By contrast, schematic variables from matching on a conclusion subject are not citizens, but merely subjects themselves, and they may not, ab initio, be used in premise inputs or conclusion outputs, because subjects are unvalidated.

What promotes a subject to a citizen is its validation, i.e., its use as the subject of a premise. Indeed, every premise subject must be a subject schematic variable, and every subject schematic variable must be a premise subject exactly once. This linear discipline requires us to design judgment forms which characterize what it is to validate a thing. Moreover, the stipulation that premise subjects come from conclusion subjects prevents \emph{re}validation of data from inputs or outputs which should already be covered by a \emph{contract}. There is no `papiere bitte' for citizens!

This design choice marks a shift from standard practice in type theory. It is common to insist that $\Gamma\vdash t:T$ must imply $\Gamma\:\textsc{valid}$ and $\Gamma\vdash T\:\textsc{type}$, and thence to require additional premises e.g., revalidating the context for each atomic term. Whilst commendably diligent, this conventional choice has an unfortunate consequence for proofs by induction on derivations: it removes our ability to work with goals which give citizens more freedom than subjects. If a conclusion input can become a subject premise, then we are obliged to deduce an induction conclusion about a thing with a citizen's freedom to misbehave than our subject induction hypothesis can address. Concretely, in our earlier proof of subject reduction, the subjects were allowed only $\PRR$, so that their subterms do only $\PRR$, but the citizens were permitted arbitrary $\PRS$ computation. By ensuring that our rules do not revalidate citizens, we avoided any risk of seeking a $\PRS$ conclusion from a mere $\PRR$ hypothesis. If you ask whether $\Gamma\vdash t:T$ without first ensuring that $\Gamma\:\textsc{valid}$ and $\Gamma\vdash T\:\textsc{type}$, you are the author of your own misfortune.


\section{Generic Metatheory}

Virtue may be its own reward, but I like to get paid. Let us formulate a framework which rewards us with metatheorems if we play by its rules.

\subsection{Terms and Expressions}

Fix a set $\syso{sort}$ of syntactic sorts, and another set $\syso{atom}$ of terminal symbols.
Let us give a way to associate with each sort a description of a syntax with binding.

\newcommand{\dx}{\mathop{\,\red{\times}\,}}
\newcommand{\db}{\mathop{\red{.}\:}}

\begin{defn}[Syntax Descriptions]
  The set $\syso{desc}$ of \textbf{syntax descriptions} with respect to $\syso{sort}$ is given inductively as follows:
  \[\begin{array}{rrll}
      \syso{desc} & \ni & \red{1} & \mbox{for the empty tuple} \\
                  & | & \syso{desc} \dx \syso{desc} & \mbox{for pairing} \\
                  & | & \syso{sort} & \mbox{for a subterm of a given sort} \\
                  & | & \syso{sort}\db\syso{desc} & \mbox{for binding a variable of a given sort} \\
    \end{array}\]

  A \textbf{syntax} is then given by a function in $\syso{sort} \to \syso{list}(\syso{atom} \times \syso{desc})$, mapping each sort to
  a choice of atomic tags accompanied a description of their associated payload.
\end{defn}

\begin{eg}[Bidirectional 1971 Martin-L\"of Type Theory]
  Our example type theory takes $\syso{sort} = \{\syso{chk},\syso{syn}\}$, and you will see some of the atoms, shortly. Its syntax is
  \[\begin{array}{lcr@{\,}ll}
      \syso{chk} & \mapsto & [ & (\mathtt{type},     & \red{1}) \\
                 &         & , & (\mathtt{pi},       & \syso{chk} \dx (\syso{syn}\mathop{\red{.}\:}\syso{chk})) \\
                 &         & , & (\mathtt{lambda},   & \syso{syn}\db\syso{chk}) \\
                 &         & , & (\mathtt{embed},    & \syso{syn}) \\
                 &         & ] \\
      \syso{syn} & \mapsto & [ & (\mathtt{check},    & \syso{chk} \dx \syso{chk}) \\
                 &         & , & (\mathtt{apply},    & \syso{syn} \dx \syso{chk}) \\
                 &         & ]
  \end{array}\]
\end{eg}

\newcommand{\sem}[1]{\llbracket #1 \rrbracket}
\newcommand{\st}{\mathrel{\mid}}
\begin{defn}[Scopes and Scoped Terms]
  The category \textbf{Sco} of scopes has as objects $\gamma,\delta : \syso{list}(\syso{sort})$ with
  morphisms being thinnings, $\theta,\phi : \gamma \le \delta$. Terms are given as a
  family of functors $(\textbf{Sco}\to\textbf{Set})^{\syso{sort}}$. Given such a family, $R$, we may interpret
  some $D:\syso{desc}$ as a functor $\llbracket D \rrbracket(R,\cdot) : \textbf{Sco}\to\textbf{Set}$ as follows:
  \[\begin{array}{lcl}
      \sem{\red{1}}(R,\gamma) & = & 1 \\
      \sem{D\dx D'}(R,\gamma) & = & \sem{D}(R,\gamma) \times \sem{D'}(R,\gamma) \\
      \sem{s}(R,\gamma) & = & R(s,\gamma) \\
      \sem{s\db D}(R,\gamma) & = & \sem{D}(R, (\gamma, s))
    \end{array}\]
  Given a syntax $F$, we may then construct terms as a least fixpoint:
  \[
    \textbf{Term}(s, \gamma) = \{\#\theta\st\theta\in[s]\le \gamma\} \cup \{(a,t) \st (a,D)\in F(s), t\in\sem{D}(\textbf{Term},\gamma)\}
  \]
  Note that a thinning $\phi:\gamma\le\delta$ acts on a term $t$ as $t\phi$, where $(\#\theta)\phi = \#(\theta;\phi)$. On other terms,
  $\phi$ acts structurally; to go under a binder, note that $\phi,1:\gamma,s\le\delta,s$.
\end{defn}




\end{document}