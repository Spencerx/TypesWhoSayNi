\documentclass{jfp1}
\bibliographystyle{jfp}
\usepackage{pig}
\ColourEpigram
\usepackage{upgreek}
\usepackage{textcomp}
\usepackage{stmaryrd}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{dogma}[theorem]{Dogma}
\newtheorem{example}[theorem]{Example}

\newcommand{\bsl}{\texttt{\symbol{92}}}
\newcommand{\fsl}{\texttt{/}}
\newcommand{\A}{\texttt}
\newcommand{\V}{\mathit}
\newcommand{\ab}{\,\texttt{->}\;}

\newcommand{\emp}{\varepsilon}
\newcommand{\G}[1]{\lfloor #1 \rfloor}
\newcommand{\Pa}[1]{\texttt{(}#1\texttt{)}}
\newcommand{\Bk}[1]{\texttt{[}#1\texttt{]}}
\newcommand{\Bc}[1]{\texttt{\string{}#1\texttt{\string}}}
\newcommand{\D}{\texttt{.}}

\newcommand{\SC}[1]{\langle\mathit{#1}\rangle}
\newcommand{\GS}[2]{\langle#1\textrm{-}\mathit{#2}\rangle}

\newcommand{\dt}{\texttt{.}}
\newcommand{\cn}[2]{\Pa{#1 \dt #2}}

\newcommand{\hb}{\texttt{:}}
\newcommand{\ra}[2]{\Pa{#1 \hb #2}}
\newcommand{\grp}[1]{\{ #1 \}}
\newcommand{\Ne}{\underline}
\newcommand{\Set}{\textbf{Set}}
\newcommand{\op}[1]{#1^{\mbox{\tiny op}}}

\newcommand{\ths}{}

\begin{document}
\title[Journal of Functional Programming]{The types who say `ni'}
\author[C. McBride]{CONOR MCBRIDE\\
  University of Strathclyde\\
  \email{conor.mcbride@strath.ac.uk}}
\maketitle

\section{Introduction}

This paper is about my religion. It introduces a discipline for
constructing and validating bidirectional dependent type systems, where the
usual type \emph{synthesis} judgement, $t : T$, is partly replaced
(specifically for introduction forms) by the \emph{checking}
judgement, $T \ni t$, while elimination forms continue to have
their types extracted, $e \in S$. Bidirectional typing (or `local
type inference') has been with us for some
time~\cite{DBLP:journals/toplas/PierceT00}. It has been something of
a secret weapon for implementing dependent type
systems~\cite{DBLP:journals/corr/abs-1202-4905}: it is
overdue to put the metatheory on a solid footing. The
literature offers diverse conventions for labelling colons with
arrows, but I exploit the asymmetry of $\ni$ and $\in$ ({\tt \bsl ni}
and {\tt \bsl in} in \LaTeX, respectively) to ensure the
left-to-right progress of \emph{time}. The types who say `ni' come
before, and they know the terms they will accept.

I illustrate the approach with a nontrivial running example --- a bidirectional reconstruction
of Per Martin-L\"of's small and beautiful, but notoriously
inconsistent dependent type theory from
1971~\cite{martinloef:atheoryoftypes}. Crucially, the fact that the
system is not strongly normalizing is exploited to demonstrate
concretely that the methodology relies in no way on strong
normalization, which is perhaps peculiar given that bidirectional type
systems are often (but not here) given only for terms in
$\beta$-normal form~\cite{DBLP:journals/toplas/PierceT00}.

From the outset, it would seem prudent to manage expectations. I take
the view that types are not inherent in things, but rather imposed on
them by human design in arbitrary ways. Meaning is made, not
found. A practical consequence of this viewpoint is that we may
fix a generic syntax, simple and flexible, before giving any thought
to the design of types and the meaningful forms of computation they
justify. Every self-respecting religion pinches the parts it likes
from other religions, so I will choose \textsc{Lisp} s-expressions~\cite{MCCARTHY60}
as the generic syntax, but tighten up the treatment of variable
binding with methods of de Bruijn~\cite{deBruijn:dummies},
distinguishing variables from atoms and naming the former only in the
interests of informal civility. One should not expect things
expressible in this syntax to make sense: rather we should design
ways to make sense of some of them. We get out what we put
in, so let us seek understanding of how to direct our freedom towards virtuous
outcomes.

Oft sought properties of type systems, such as `stability under
substitution' and `subject reduction', are not to be had for the
proving, but rather by construction in accordance with good guidance.
The prospectus of this paper is to develop a metametatheory in which
to ensure the good metatheoretic properties of a whole class of theories.


\section{Unpacking the problem}

By way of motivating an alternative approach, let us briefly examine the current
situation. What are the problematic consequences of type synthesis?
What are the obstacles to adopting a mixture of type checking and type
synthesis? What makes subject reduction hard to prove? The ways in
which we address these questions give us keys to some answers.


\subsection{Which types must we write?}

In order to ensure that types can be synthesized, we will need to
write type annotations in some places. We work in a setting where
types involve computation, so it is clear we should have to solve an
ambiguous, let alone undecidable class of unification problems to omit
all the type information in the Milner manner.  We cannot sustain a
type system on the manifestly false promise that functions are in
general invertible. Our programs need strategically placed type
information to supply components we cannot hope to construct in any
other reliably effective way. The examples of dependent functions
and pairs allow us to contrast fantasy with reality.
\[\begin{array}{rc@{\qquad}c}
    & \mbox{\bf fantasy} & \mbox{\bf reality} \\
    \mbox{\bf functions} 
  & \Rule{x\!:\!S \vdash t\;:\;T}
         {\lambda x.\,t\;:\;(x\!:\!S)\to T}
  & \Rule{\textsc{type}\;S\quad x\!:\!S \vdash t\;:\;T}
         {\lambda x\!:\!S.\,t\;:\;(x\!:\!S)\to T}
                   \\
    \mbox{\bf pairs}
    & \Rule{s\;:\;S\quad t\;:\;T[s/x]}
           {(s,t)\;:\;(x\!:\!S)\times T}
  & \Rule{s\;:\;S\quad x\!:\!S\vdash \textsc{type}\;T\quad t\;:\;T[s/x]}
           {(s,t)_{x.\,T}\;:\;(x\!:\!S)\times T}\\
\end{array} \]
In the case of functions, the domain of an abstraction must come from
somewhere, and it must be in place and checked to be a type before it
is reasonable to extend the context and synthesize the type of the
body. However, once the body's type has been found, with respect to a
generic variable, there is no choice about how to abstract that
variable to yield a function type. In practice, one can place a
metavariable in the context as $x$'s type and hope to collect
constraints on it from the way $x$ is used, but one cannot expect
that those constraints will yield a clear solution every time.

The situation is, if anything, worse in the case of pairs. While the
type, $S$, of the first component, $s$, is clearly obtainable by
synthesis, the type, $T[s/x]$, of the second component, $t$, yields
but one instance of the pattern of dependency expressed by the pair
type, from which we must abstract some $T[x]$. Substitution is not
uniquely invertible. More concretely, the pair $(3, [0,1,2])$ of a
number and a length-indexed list can be given any pair type where the
length is computed by a function on the natural numbers which maps $3$
to $3$: there are a great many such functions. There is no choice but
to give this function explicitly. Moreover, once we can compute types
from data, there is no reason to believe that the second component is
even a length-indexed list at all, when the first is any number other
than 3.

We can construct the real rules from the fantasy rules, determining
which annotations are mandated by magical misapprehensions. That is to
say, it is not good enough to write schematic variables in typing rules
without ensuring a clear source for their instantiation. We might profit
from a finer analysis of scope and information flow in typing rules, giving
each schematic variable one binding site and zero or more use sites.
This paper will deliver one such analysis in due course. However, we can
already see that the necessary annotations will arise from the specifics
of the types in question, rather than in a uniform way that lends itself
to a generic methodology of metatheory.

But it gets worse. If we transform a dependent \emph{telescope},
\[x_0\!:\!S_0,x_1\!:\!S_1[x_0],\ldots,x_n\!:\!S_n[x_0,\ldots,x_{n-1}]\]
into a right-nested pair type, the corresponding values will be
festooned with redundant but differently instantiated copies of the
telescope's tails.
\[\begin{array}{l}
    (s_0,(s_1,\ldots (s_{n-1},s_n)_{x_{n-1}.S_n[s_0,\ldots,s_{n-1}]}\\
    \qquad
    \ldots)_{x_1.(x_2\!:\:S_2[s_0,x_1])\times\ldots S_n[s_0,x_1,\ldots,x_{n-1}]}\\
    \quad)_{x_0.(x_1\!:\!S_1[x_0])\times\ldots S_n[x_0,x_1,\ldots,x_{n-1}]}
\end{array}\]
The insistence that every subterm carry all the information necessary
for the synthesis of its type, regardless of where it sits and how much
we might already know of our \emph{requirements} for it, leads to a
corresponding blow up. The core languages of both the Glasgow Haskell
Compiler and the Coq proof assistant are presently afflicted: Garillot's
thesis documents a situation where an apparently sensible approach to
packaging mathematical structures is prevented in practice by an
exponential growth in redundant type information. This must stop!


\subsection{Can we turn things around?}

The irony of the type annotation problem, above, is that the `fantasy' rules
make perfect sense when seen as type \emph{checking} rules. A function type
tells you the domain type which goes in the context and the range type which
checks the body of an abstraction. A pair type tells you the general scheme
of dependency which must be instantiated when checking components. Might we
not propagate our requirements through the structure of terms, rather than
requiring each individual subterm to be self-explanatory?

Such an approach was pioneered by Pierce and Turner under the name
`Local Type Inference', and has since been explored by many others,
notably by Dunfield in the otherwise troublesome setting of
intersection types. It is indeed much easier to see that you get what
you want if you know what you want in advance. The usual situation is
that one checks types for introduction forms and synthesizes types for
elimination forms. Everything synthesizable is also checkable, as this
situation gives \emph{two} candidates for the type of the term in
question whose consistency can then be tested. However, there is no
hope to synthesize types for terms which are merely checkable, as the
number of candidate types is \emph{zero}. The latter bites when we seek to
express a $\beta$-redex --- the elimination of an introduction form:
\[
  (\lambda x.\,t)\:s
\]
Even if we are given the type of this expression, we cannot hope
to compute the type at which to check the abstraction, inferring the
general pattern of dependency from one instance of it.

The standard remedy is to restrict the language to $\beta$-normal forms
and conceal all opportunities for computation behind \emph{definitions}. We may give a definition
\[\mathsf{f} : (x\!:\!S)\to T;\; \mathsf{f} = \lambda x.\,t
  \]
where the type annotation is no mere act of pious documentation but rather our
assurance that the types of all identifiers in scope are known.
It is then straightforward to synthesize a type for the application $\mathsf{f}\:s$ --- or
is it? That type would be $T[s/x]$ for some suitable notion of substitution, but the textual
replacement of $x$ by $s$ will not preserve $\beta$-normality: substitution can create redexes.

The spider we usually swallow to catch this fly is \emph{hereditary}
substitution, which contracts all the redexes introduced by the
replacement of variables, and all the further redexes induced by those
contractions, and so on, until we have restored $\beta$-normality. The
efficacy of this solution rests on hereditary substitution being well
defined, which amounts to showing that our calculus is
$\beta$-normalizing. For systems with weak function spaces, such as
the logical frameworks from whose literature the technique originates,
this is no harder than normalizing the simply typed
$\lambda$-calculus. In the general setting of dependent type theories, however,
we have not such an easy victory: for Martin-L\"of's inconsistent 1971 type theory,
hereditary substitution is \emph{not} well defined, but the system enjoys subject reduction, none the less.

In the business of establishing metatheoretic properties of type
systems, it is certainly preferable if basic hygiene properties sych
as subject reduction can be established more cheaply than by appeal to
as heavy a requirement as normalization. Indeed, we have only a chance
of showing that well typed terms are normalizing, so it approaches the
circular to rely on normalization in the definition of what it means
to be well typed in the first place.

Even in settings where hereditary substitution is not well defined, one
might consider presenting it relationally, refining the burden of proof
to individual cases. The application rule would become
\[
  \Rule{f\;:\;(x\!:\!S)\to T\quad s\;:\;S \quad T[s]\Downarrow T'}
       {f\:s\;:\;T'}
\]
yielding a derivation only where the substitution is successful. The trouble
here is that the relation $T[s]\Downarrow T'$ is manifestly not stable under
substitution --- instantiating free variables can cause inert terms to compute,
perhaps for ever.

The effective remedy is to ensure that computation has a small-step presentation
which \emph{is} stable under substitution. We must ensure that our syntax is capable of
expressing $\beta$-redexes, and to that end, let us introduce type
annotations which mediate between introduction and elimination forms, ensuring
that we have enough information to validate them. In what follows, we shall do
so \emph{uniformly}, rather than placing annotations differently for different
types. The purpose of a type annotation is exactly to characterize a redex, so
it will help to standardize the ways in which redexes arise.


\subsection{What makes subject reduction difficult to prove?}

We might very well hope to prove the following admissible
\[\Rule{\Gamma\vdash t\;:\;T\quad t\leadsto t'}
  {\Gamma\vdash t'\;:\;T}
  \qquad
  \Rule{\Gamma\vdash\textsc{type}\;T\quad T\leadsto T'}
       {\Gamma\vdash\textsc{type}\;T'}
\]
and we should be mortified were it not the case. However, this statement
does not follow by induction on the typing derivation for the subject, $t$. In dependent
typing derivation, components from the term being checked can be copied to
the right of the colon (e.g., in the application rule) and, when moving under
binders (e.g., when checking that a function type is well formed), into the context.
The above statement allows for no computation in the context or the type, so
our induction hypotheses may fail to cover the cases which arise. Consider
the case for
\[
  \textsc{type}\;(x\!:\!S)\to T\quad (x\!:\!S)\to T \leadsto (x\!:\!S')\to T
\]
where the derivation is by the rule
\[\Rule{\textsc{type}\;S\quad x\!:\!S\vdash\textsc{type}\;T}
  {\textsc{type}\;(x\!:\!S)\to T}
  \]
We will have an induction hypothesis concerning reduction of $T$ after the derivation of
\[
  \Gamma,x\!:\!S\vdash \textsc{type}\;T
\]
but the computation in the type means that we now need to show
\[
  \Gamma,x\!:\!S'\vdash \textsc{type}\;T
\]
with a new context about which we know too little.

We shall certainly need a more general formulation of subject reduction
in which things other than the subject --- contexts and types --- may also compute, but this exposes us
to a further risk in cases where the typing rules move information
from context and type back into the subject position. E.g., the conversion rule
is sometimes formulated as
\[
  \Rule{t\;:\;S\quad S\cong T\quad \textsc{type}\;T}
  {t\;:\;T}
\]
where $T$ is $\beta$-convertibility: as reverse $\beta$-steps are most certainly
not guaranteed to preserve types, we must confirm that $T$ makes sense.
If our formulation of subject reduction allows too much computation in the type $T$,
our induction hypothesis for $\textsc{type}\;T$ may be too weak to show that we
still have a meaningful type.

In summary, the formulation of subject reduction statements is extremely
sensitive to how much computation is permitted in which places, and the literature
of metatheory for dependent type systems shows considerable delicate craft.
What design principles might we follow to be sure of a robust proof strategy?
Read on!

\section{What is to be done?}

The prospectus I offer is a \emph{general} proof of subject reduction for a
large class of dependent type theories, resting only on conditions which can
be checked mechanically. That is, for the theories in this class, subject
reduction is had for the asking.

In order to obtain this result I shall need to develop disciplines for
specifying type theories which, by design, avoid pitfalls like those
outlined above.  In some cases, these disciplines will merely make
explicit what is, in any case, standard practice. In other cases, I
deviate from the approach usually found in the presentation of type
synthesis systems to exploit particular characteristics of the
bidirectional setup.

Central to the project is a careful analysis of the roles each position
plays in the judgement forms and the flow of information through typing rules.
The key idea is that a rule is a \emph{server} for derivations of its conclusion
and a \emph{client} for derivations of its premise. A judgement form is thus a
tiny little session type, specifying the protocol for these client-server
interactions. We may thus formulate a clearer policy of who is promising
what to whom and check whether rules are compliant --- we do not write down
any old nonsense.

This tighter grip on information flow will manifest itself in a separation
of the kinds of formula we may write in a rule into \emph{patterns}, which contain
the binding sites of the schematic variables, and \emph{expressions}, which may
contain substitutions on schematic variables. An immediate consequence is
that rules can never demand the magical inversion of substitions. A more subtle
consequence is that the typing assumptions we encounter can always be inverted
mechanically to determine what is known about each schematic variable. A careful
policing of the scope of schematic variables, particularly
those which occur in the subjects of judgements, will enable us to formulate
the statement of subject reduction in a way that guarantees the effectiveness
of induction on typing derivations. Likewise, a tight Trappist discipline on
free variables in rules will ensure that any expressible system of rules is stable under
substitution.

I propose to work in a generic syntax, adequate to express canonical
constructions which allow the binding of variables, with a uniform
treatment of elimination and thus exactly one way to construct a
redex, exposing the type at which reduction can happen. The patterns
and expressions which may appear in rules will be specified with
respect to this syntax. Redexes, too, will be characterized in terms
of patterns: for any canonical type, we shall be able to compute its
set of non-overlapping redexes which must be given reducts to ensure
progress, yielding a rewriting system which is confluent by
construction. That each reduct has the same type as its redex
will be the key condition for subject reduction --- unsurprisingly
necessary, but remarkably sufficient for any protocol-compliant
system of rules.

In what follows, I shall adopt a mathematical style, identifying and
numbering specific Definitions, Lemmas and Theorems, but introducing a
new keyword --- \emph{Dogma}. A dogma is a \emph{religious} truth,
rather than a \emph{Platonic} truth: it is made true by human
endeavour, rather than by nature. A dogma is a design choice --- a
prejudice, if you like. However, the prejudice should be judicial:
every dogma should be motivated by some anticipated benefit. Moreover,
every numbered Dogma will have a `Proof', its exegesis, in the form of a
forward pointer to the Definition that ensures the Dogma is
followed. Let us begin.

\begin{dogma}[generic syntax]
  One syntax is sufficient to encode the types and terms of all the
  systems that we shall seek to explain. It is not expected that
  being syntactically well formed is the same thing as being
  meaningful. It is enough to explain what it is to be a classifier,
  i.e., a \emph{type}, and what it is to be classified.
\end{dogma}
\begin{proof}
  The generic syntax is given in Definition \ref{def:syntax}. The
  means to manage classification --- the notion of typing judgement ---
  is given in Definition \ref{def:judgement}.
\end{proof}


\section{Sufficient syntax}

\newcommand{\la}[1]{\bsl #1\:}

Formally, I work with a generic de Bruijn-style nameless syntax, with syntactic
categories indexed by their scope. The embedding of one scope into another
is called a \emph{thinning}. This section defines the syntax and the actions
upon it of thinnings and substitutions.

\subsection{Scoped and separated syntactic classes}

For the benefit of human beings, a \emph{scope} is written as a list of identifiers $x_{n-1},\ldots x_0$, although the inhuman truth is that it is just the number $n$
which gives the length of the sequence. I refer to scopes by
metavariables $\gamma$ and $\delta$, with $\emp$ denoting the empty
scope. A \emph{variable} is an identifier $x_i$ selected from
a scope, serving as the human face of its index $i$. I specify grammars
relative to an explicit scope, $\gamma$, and write $x_\gamma$ to
mean `a variable from $\gamma$'.

Unlike variables, \emph{atoms} ($a$, $A$) really are named global
constants which play the role of tags --- their purpose is not
to stand for things but to be told apart. The symbol $\Pa{}$ is
considered an atom and pronounced `nil'.

\newcommand{\lib}{\textbf{lib}}
\newcommand{\ess}{\textbf{ess}}
\newcommand{\cons}{\textbf{cons}}
\newcommand{\comp}{\textbf{comp}}


\newcommand{\Tm}[3]{\mathbf{Term}\:#1\:#2\:#3}
\begin{definition}[constructions and computations, essential and liberal\label{def:syntax}]
The object-level syntax is specified,
written as a subscript, and
is divided into four distinct mutually defined grammatical classes,
each with standard metavariable conventions,
arranged as four quadrants thus:
\[\begin{array}{r|l|l}
 d \;\backslash\; l& \ess\mbox{ential} & \lib\mbox{eral} \\
\hline
    \cons{truction} & k_\gamma, K_\gamma & s_\gamma, t_\gamma, S_\gamma, T_\gamma \\
    & \;\begin{array}[t]{rl}
          ::= & a \\
          | & \cn{s_\gamma}{t_\gamma} \\
          | & \la xt_{\gamma,x}
        \end{array}
    & \;\begin{array}[t]{rl}
          ::= & k_\gamma \\
          | & \Ne{n_\gamma}
        \end{array}
    \\
    \hline
    \comp\mbox{utation} & n_\gamma, N_\gamma & e_\gamma, f_\gamma, E_\gamma, F_\gamma \\
    & \;\begin{array}[t]{rl}
          ::= & x_\gamma \\
          | & e_\gamma\:s_\gamma 
        \end{array}
    & \;\begin{array}[t]{rl}
          ::= & n_\gamma \\
          | & \ra{t_\gamma}{T_\gamma}
        \end{array}
    \\
  \end{array}\]
Let us write $\Tm ld\gamma$ for the set of terms in the quadrant given
by \emph{liberality} $l$
and \emph{direction} $d$ with scope $\gamma$.
\end{definition}

\newcommand{\PI}[3]{\Pa{\Uppi\:#1\;\la{#2}{#3}}} 
\newcommand{\SG}[3]{\Pa{\Upsigma\:#1\;\la{#2}{#3}}} 
The meaningfulness of \emph{constructions} will always be
\emph{checked}, relative to some prior expectation. The
\emph{essential} constructions give us the raw materials for
canonical values, and they will always be invariant under computation.
I use teletype parentheses, $\Pa\ldots$ as `\textsc{Lisp} brackets' to leave
$(\ldots)$ free for their usual mathematical purpose of resolving ambiguity.
Let us further adopt the \textsc{Lisp} convention that
$\dt\Pa{\mathit{stuff}}$ may be abbreviated as $\mathit{stuff}$, which is
conveniently prejudicial to right-nested, nil-terminated lists. Our
constructions will often be such lists, with an atom at the head.
For example, dependent function types in $\gamma$ will be constructions of form
\[\PI {S}x{T}\quad
  \mbox{which abbreviates}\quad
  \cn\Uppi{\cn S{\cn{\la x T}{\Pa{}}}}
\]
where $\Uppi$ is a particular atom, $S$ is a $\gamma$-construction and
$T$ is a $\gamma,x$-construction. It is not my habit to notate
explicitly the potential dependency of $T$ on $x$: the abstraction
makes that much clear. Formally, working de Bruijn style, there is no reason
to name the variable bound by an abstraction, but humans seem to appreciate it.

The \emph{liberal} constructions extend the canonical constructions
with \emph{thunks}, $\Ne n$ --- essential computations which have not yet achieved canonical
form, either because they have not yet reduced, or because a variable
is impeding reduction.

Meanwhile, the \emph{computations} will always admit a type synthesis
process. The \emph{essential} computations comprise variables (whose
type will be assigned by a context) and eliminations, overloading the
application syntax --- the \emph{target} computation $e$ to the left is being
eliminated, and its synthesizable
type will tell us how to check that the construction $s$ to its right
is a valid \emph{eliminator}. These two give us the traditional forms
of \emph{neutral} term.

The \emph{liberal} computations extend the neutral computations with
\emph{radicals}, in the chemical sense, being canonical forms with a
type annotation which gives the information needed to check both the
canonical form it annotates and the eliminator it is `reacting' with.
Radicals are permitted only in eliminations, and these are
the only eliminations which compute. That is\ldots

\begin{dogma}[typed computation]
  There is no computation
step which proceeds uninformed by the type at which computation is
happening.
\end{dogma}
\begin{proof}
  Definition \ref{def:reduction} will ensure that reduction is the
  no more than the contextual closure of whatever may happen to
  eliminated radicals.
\end{proof}

Thunks and radicals form the connection between constructions and
computations, but you will notice that the syntax carefully precludes
the thunking of a radical --- a computation which is eliminated no
further needs no type annotation. We may extend thunking to liberal
computations as a `smart constructor' which deletes the annotations
of radicals.
\[
  \Ne{\ra tT} = t
\]
Observe also that we may now lift
\emph{all} the term formers to act on liberal components, yielding
liberal results, for everything apart from thunks admits
liberal substructures. In particular, it becomes reasonable to
substitute a liberal computation for a variable, in either a
construction or a computation, yielding a liberal construction or
computation, respectively. I write $t\fsl e$ for such a substitution
in a construction, and ensure that it is always clear from context
which variable in $t$ is being substituted --- the forward slash
of substitution answers the backslash of abstraction.

\newcommand{\U}{\ensuremath{\star}}

\begin{example}[Martin-L\"of's 1971 theory: syntax and $\beta$-reduction]
  Let $\Uppi$ and $\U$ be atoms. The canonical types are given as
  $\U$ and $\PI SxT$, with the former being the type of all types
  and the latter being the type of abstractions, $\la xt$. The
  $\beta$-rule is
\[
  \ra{\la xt}{\PI SxT}\:s \leadsto_\beta \ra{t\fsl\ra sS}{T\fsl\ra sS}
\]
\end{example}

Note that the $\beta$-rule substitutes a radical
for the bound variable, creating potential redexes wherever that
variable is eliminated. Moreover, the whole reduct will be radical. The type
annotations thus mark the active computation sites and are discarded
whenever computation concludes, in general.

\begin{dogma}[$\beta$-rules]
  All $\beta$-rules have form
  \[\ra tT\:s \leadsto_\beta \ra{t'}{T'}
  \]
  Further, each of $t$, $T$ and $s$ may demand only essential
  construction structure.
\end{dogma}
\begin{proof}
  This is the substance of Definition \ref{def:reduction}.
\end{proof}

\newcommand{\car}{\texttt{car}} 
\newcommand{\cdr}{\texttt{cdr}} 
Were we to add pair types, with atoms $\car$ and $\cdr$ as the
eliminators, we could have:
\[\ra{\cn st}{\SG SxT}\:\car \leadsto_\beta \ra sS \qquad
  \ra{\cn st}{\SG SxT}\:\cdr \leadsto_\beta \ra t{T\fsl\ra sS}
\]

By adopting this uniform presentation of computation, we have already
taken a major step towards establishing \emph{confluence} of
$\beta$-reduction: if one $\beta$-redex contains another, the inner
redex will always be copied zero or more times whenever the outer redex
contracts. Critical pairs can arise only if the same term is a redex
for two different $\beta$-rules.

The $\beta$-normal forms are characterized by replacing
$\ra{t_\gamma}{T_\gamma}$ in the grammar by $\ra{n_\gamma}{T_\gamma}$, ensuring that
all radicals are inert because of some free variable. We must resist
the clear temptation to elide type annotations on neutral terms
during reduction as the property of being neutral is not stable under
substitution.

\begin{remark}[run-time type erasure]
  The fact that computation is formally conducted in the presence of
  types is motivated by clarity of metatheory, rather than by
  pragmatic considerations of efficient closed-term execution.
  That said, one can achieve a sensible notion of type erasure by
  ensuring that the target term and eliminator of an eliminated
  radical determine the reduction rule applicable, and that type
  information therein is used only to determine type information in
  the reduct. By inspection, this is the case for our examples.
\end{remark}




\subsection{Thinnings and substitutions}

\newcommand{\thin}{\sqsubseteq}
\newcommand{\ith}{\mathbf{1}}
\newcommand{\eth}{\mathbf{0}}
\newcommand{\sel}[2]{#1!#2}

To keep a tight grip in our formal account of \emph{scope}, let us
develop the apparatus which explains when one scope embeds in another.

\begin{definition}[thinning]
A thinning is an order preserving embedding between scopes
\[\theta : \gamma \thin \delta
\]
I typically use $\theta$ and $\phi$ as metavariables for thinnings.
The thinnings are generated by
\[
  \Axiom{\emp:\emp\thin\emp} \qquad
  \Rule{\theta : \gamma \thin \delta}
       {\theta,0 : \gamma \thin \delta,x} \qquad
  \Rule{\theta : \gamma \thin \delta}
       {\theta,1 : \gamma,x \thin \delta,x} \qquad
\]
\end{definition}

That is, thinnings arise in the manner of Pascal's triangle: there are
$\left(\begin{array}{@{}c@{}}m \\ n\end{array}\right)$ thinnings from
$n$ to $m$, which is not surprising, as they correspond to
selections. A thinning is, in effect, given as a bit vector the length
of its target scope with its source scope being the `population
count', i.e., number of 1s, in the vector.

\begin{definition}[identity thinning]
The identity thinning $\ith_\gamma : \gamma \thin \gamma$ is given by
\[
  \ith_\emp = \emp \qquad \ith_{\gamma,x} = \ith_\gamma,1
\]
\end{definition}
Informally, we may just write $\ith$. The empty thinning, $\eth$ is generated analogously, repeating
0 to the appropriate length.

\begin{definition}[composition of thinnings]
  If $\theta:\gamma_0\thin \gamma_1$ and $\phi:\gamma_1\thin\gamma_2$, then $\theta\phi : \gamma_0\thin\gamma_2$
  defined as follows:
  \[
    \emp\emp = \emp \qquad
    \theta(\phi,0) = \theta\phi,0 \qquad
    (\theta,b)(\phi,1) = \theta\phi,b
  \]
\end{definition}
Note that I adopt the \emph{diagrammatic} convention for composition,
written just as juxtaposition. When we see $\theta\phi$, we should
remember that they meet in the middle, with $\theta$'s source on
the left and $\phi$'s target on the right.

You will not be surprised to learn that thinnings have categorical
structure, to be established formally in section \ref{sec:thincat}.
For the moment, though, let us establish the minimum machinery
necessary to proceed with our analysis of type systems.

The formal truth is that a `variable' $x_\gamma$ is a thinning
in some $x \thin \gamma$, i.e., from the singleton scope to $\gamma$. Noting that $\gamma$ occurs
positively in the grammar, we obtain an action of thinnings on our syntax.

\begin{definition}[action of a thinning]
  If $\theta:\gamma\thin \delta$, it has a covariant, quadrant-preserving action,
  $\cdot\theta:\Tm ld\gamma\to\Tm ld\delta$,
  which reduces to composition at variables.
  \[
    \begin{array}{rcl@{\qquad}rcl}
      a\theta & = & a & \Ne n\theta & = & \Ne{n\theta} \\
      \cn st\theta & = & \cn{s\theta}{t\theta} && \\
      (\la x t)\theta & = & \la x t(\theta,1) && \medskip\\
      (e\:s)\theta & = & e\theta\:s\theta & \ra tT\theta & = & \ra{t\theta}{T\theta} \\
    \end{array}
    \]
  \end{definition}

I write the action postfix as a reminder that the scope of $t\theta$
is the target scope of $\theta$, i.e., that the action is indeed covariant.

By contrast, thinnings act \emph{contravariantly} on vectors of any kind.
Viewed left-to-right, thinnings represent the \emph{injection} of one scope
into a larger scope. Viewed right-to-left, thinnings represent the
\emph{selection} of a subset.

\begin{definition}[selection]
  If $X$ is a set, let $X^\delta$ be the set of vectors with one
  component in $X$ for each variable in $\delta$. If
  $\theta:\gamma\thin\delta$ and $\vec x:X^\gamma$,
  then let $\sel\theta{\vec x} : X^\gamma$ be defined thus:
  \[
    \sel\emp\emp=\emp \qquad
    \sel{(\theta,0)}{(\vec x,x)} = \sel\theta{\vec x} \qquad
    \sel{(\theta,1)}{(\vec x,x)} = (\sel\theta{\vec x}),x
  \]
\end{definition}

I write the $\sel\theta\cdot$ action prefix as a reminder that the
length of $\sel\theta{\vec x}$ is given by the source scope
of $\theta$. In due course, we shall have that selection is
functorial, and natural in the type of elements.

We shall have need of left and right injections.
\newcommand{\thinl}[2]{#1 \triangleleft #2}
\newcommand{\thinr}[2]{#1 \triangleright #2}
\begin{definition}[injections to a concatenation]
  We take
  \[
    \thinl\gamma\delta = \ith_\gamma,\eth_\delta :
    \gamma\thin\gamma,\delta \qquad
    \thinr\gamma\delta = \eth_\gamma,\ith_\delta :
    \delta\thin\gamma,\delta
    \]
\end{definition}

Seen as selections, the above allow us to extract prefixes and
suffices of vectors.
\begin{lemma}[chopstick]
  Let $\vec x : X^{\gamma,\delta}$. It is the case that
\[
  \sel{(\thinl\gamma\delta)}{\vec x},
  \sel{(\thinr\gamma\delta)}{\vec x}
  = \vec x
\]
\end{lemma}
\begin{proof}
  This follows by induction on $\delta$. Strictly speaking, it relies
  also on the fact that selecting with the identity thinning acts as
  the identity, which again follows by induction on scope.
\end{proof}

Next, let us have \emph{substitutions}, which act simultaneously on all variables
in a scope. Let us take $\sigma$ and $\rho$ as metavariables for substitutions.

\newcommand{\su}{\Rightarrow}
\begin{definition}[substitution]
  Let $\gamma\su\delta$ be the set of substitutions from $\gamma$ variables to $\delta$ terms,
  i.e., vectors \[\sigma : (\Tm\lib\comp\delta)^\gamma\]
  Whenever $\theta : \gamma'\thin\gamma$, we have $\sel\theta\sigma$
  for the substitution in $\gamma'\su\delta$ given by selecting from
  $\sigma$ only the images corresponding to variables chosen by
  $\theta$. In particular, $\sel x\sigma$ is the image of some variable
  (i.e., singleton thinning)
  $x$. 
  We may write $\emp$ for the trivial
  substitution from the empty scope. If $\sigma : \gamma \su \delta$
  and $e : \Tm\lib\comp\delta$, then we have $\sigma,e : \gamma,x$.
\end{definition}

Note that as variables are computations, so must be their images, but that we shall permit
those images to be liberal, allowing in particular the replacement of variables by radicals.
We shall establish that substitutions form a category in due
course. Let us first see how they operate. Before we can give their action on terms, we say how they
pass under binders.

\newcommand{\wk}{\uparrow}
\newcommand{\wka}{\hat}
\begin{definition}[weakening]
  The thinning $\ith,0 : \gamma\thin\gamma,x$ which adds one new local variable is written
  $\wk$, and we denote its action on a thing by $\wka\cdot$. In particular, the weakening
  of a thinning, $\wka\theta$ is given by $\theta\wk$.
\end{definition}

\begin{definition}[action of a thinning on a substitution]
  If $\sigma:\gamma\su\delta$ and $\theta:\delta\thin\delta'$, we
  write $\sigma\theta:\gamma\su\delta'$ for the pointwise action of
  $\theta$ on the computations in $\sigma$. In particular,
  \[
    \wka\sigma = \sigma\wk : \gamma\su\delta,x
    \]
\end{definition}

We now have all we need to make substitutions act on our syntax.

\begin{definition}[action of a substitution]
  If $\sigma:\gamma\su \delta$, it has a liberalising action,
  $\cdot\sigma:\Tm ld\gamma\to\Tm\lib d\delta$, which is projection
  at variables.
  \[
    \begin{array}{rlc@{\qquad}rlc}
      a\sigma & = & a & \Ne n\sigma & = & \Ne{n\sigma} \\
      \cn st\sigma & = & \cn{s\sigma}{t\sigma} && \\
      (\la x t)\sigma & = & \la x t(\wka\sigma,x) && \medskip\\
      x\sigma & = & \sel x\sigma    & \ra tT\sigma & = & \ra{t\sigma}{T\sigma} \\
      (e\:s)\sigma & = & e\sigma\:s\sigma
    \end{array}
  \]
  Note that if $\sel x\sigma = \ra tT$, then $\Ne x\sigma = \Ne{\ra tT} =
  t$, as the smart $\Ne\cdot$ strips the type annotation.
\end{definition}

Substitutions also have categorical structure, as we shall see in
section \ref{sec:sbstcat}. For now, let us establish the operations.

\newcommand{\isu}{\iota}
\begin{definition}[identity substitution]
  The identity substitution $\isu_\gamma : \gamma\su\gamma$ is given by
  \[\isu_\emp = \emp \qquad \isu_{\gamma,x} = \wka\isu_\gamma,x
  \]
  A straightforward induction shows that $\sel x\isu = x$.
\end{definition}

\begin{definition}[composition of substitutions]
  If $\rho : \gamma_0\su\gamma_1$ and $\sigma : \gamma_1\su\gamma_2$, their
  composition, $\rho\sigma$ makes $\sigma$ act \emph{pointwise} on terms in $\rho$.
  \[\rho\sigma : \gamma_o\su\gamma_2\qquad \sel x{(\rho\sigma)} =
    (\sel x\rho)\sigma
  \]
\end{definition}

Lastly, let us see how to combine substitutions from separate scopes.

\begin{definition}[tensor of substitutions]
  Let $\sigma_i : \gamma_i\su\delta_i$ for $i=0,1$. Define
  \[
    \sigma_0\otimes\sigma_1 =
    \sigma_0(\thinl{\delta_0}{\delta_1}),\sigma_1(\thinr{\delta_0}{\delta_1})
    : \gamma_0,\gamma_1 \to \delta_0,\delta_1
  \]
\end{definition}

Note that when we push a substitution under a binder, we compute
$\wka\sigma,x$, which is exactly $\sigma\otimes\isu_x$.

We have established the \emph{definitions} of thinnings and
substitutions, with their actions on terms. Let us acknowledge the
debt outstanding to establish their \emph{structure}, but resist the
temptation to dive into detail now. Let us instead zoom out and
develop more of the ideas behind bidirectional type systems.



\section{Judging the judgements}

Typing \emph{judgements} make statements about the terms of our
theories. The acceptable \emph{derivations} of judgements are defined
inductively, by means of \emph{rules}, each of which has zero or
more \emph{premises} and one \emph{conclusion} ---
\emph{formal judgements} which give rise to actual judgements by
instantiation of their \emph{schematic variables}.

We should be clear about the distinction between the actual judgements
derived in the process of checking the actual types of actual terms,
and the formal judgements which we write when we specify the typing
rules. We should hope to gain useful results about the derivability
of actual judgements by taking care with the language we use for
formal judgements. This section's primary concern is with actual
judgements, but we should hope to learn some lessons informing
the treatment of formal judgements in the next section.


\subsection{The judgement forms}

The judgement forms are specified as sequences of \emph{punctuation}
and \emph{places}. An \emph{actual} judgement has terms in its places.
A formal judgement has \emph{formulae} which stand for sets of terms into the
\emph{places}. Each place in a judgement form is assigned a \emph{mode}.

\begin{definition}[mode]
  There are three modes which may be assigned to a place in a judgement:
  \begin{description}
  \item[input] an input is a term supplied by a rule client ---
    this term is already in some sense trusted;
  \item[subject] a subject is a term supplied by a rule client ---
    `trust' in this term remains to be established by appeal to the rule;
  \item[output] an output is a term supplied by a rule server ---
    this term is guaranteed to be `trustworthy'.
  \end{description}
\end{definition}

What does it mean to be `trusted'? For each input place in a judgement
form, we must specify a judgement where that input now stands as a
subject --- such a judgement is called a \emph{precondition} and
represents a proof obligation to be discharged by a rule client. For
each output place in a judgement form, we must specify a judgement
where that output now stands as a subject --- such a judgement is
called a \emph{postcondition} and represents a proof obligation to be
discharged by a rule server. When we have a little more apparatus in
place, we shall be able to compute precisely the proof obligation which ensures
that a rule justifies the preconditions for its premises and the postcondition
for its conclusion, given the preconditions for its conclusion and the postconditions
for its premises.

Without further ado, let me specify the judgement forms I
propose. Actual judgements have some scope, $\gamma$, and I shall be
careful to give scopes to their places, determining the scopes of the
terms we may put into them. I shall draw a box around the
subjects, of which there will be at most one, and ensure that inputs
stand left of the box, with outputs to the right.  I write
preconditions in braces to the left of the judgement form, with
postconditions in braces to the right.

\newcommand{\bx}[1]{\framebox{\ensuremath{#1}}}
\newcommand{\ty}[1]{\textsc{type}\;#1}
\newcommand{\univ}[1]{\textsc{univ}\;#1}
\newcommand{\chk}[2]{#1 \ni #2}
\newcommand{\syn}[2]{#1 \in #2}
\newcommand{\cxe}[2]{#1\!:\!#2\vdash}
\newcommand{\cxl}[2]{#1\;:\;#2}
\newcommand{\Pre}[1]{\textbf{Pre}\:#1}
\newcommand{\Post}[1]{\textbf{Post}\:#1}
\begin{definition}[$\gamma$-judgement\label{def:judgement}]
  The judgements, $J_\gamma$, in scope $\gamma$ are given inductively as follows:
  \[\begin{array}{r@{\quad}c@{\quad}l@{\quad}l}
      \Pre{J_\gamma} & J_\gamma& \Post{J_\gamma} & \mbox{purpose} \\
      \hline
      \{\} &\ty{\bx{T_\gamma}}& \{\}  & \mbox{type construction}
                                        \smallskip\\
      \{\ty{T_\gamma}\} &\univ{T_\gamma}\;\bx{}& \{\}  & \mbox{universe}
                                        \smallskip\\
      \{\ty{T_\gamma}\} & \chk{T_\gamma}{\bx{t_\gamma}} &\{\}  & \mbox{type checking}
                                                                 \smallskip\\
      \{\}  & \syn{\bx{e_\gamma}}{S_\gamma} & \{\ty{S_\gamma}\} & \mbox{type synthesis}
                                                                  \smallskip\\
      \{\}  & \cxl{\bx{x_\gamma}}{S_\gamma} & \{\ty{S_\gamma}\} & \mbox{type lookup}
                                                             \smallskip\\
      \{\ty{S_\gamma},\ty{T_\gamma}\} & S_\gamma = T_\gamma\;\bx{} & \{\}  & \mbox{type equality}
                                                                             \smallskip\\
      \{\ty{S_\gamma},\cxe x{S_\gamma} \Pre{J'_{\gamma,x}}\} & \cxe x{S_\gamma} J'_{\gamma,x}
           & \{\cxe x{S_\gamma}\Post{J'_{\gamma,x}}\} & \mbox{context extension} 
    \end{array}\]
  We may extend the postfix action of thinnings and substitutions to
  judgements, writing $J\theta$ and $J\sigma$, respectively.
\end{definition}

We may check that a construction is a valid type, which means that it
is reasonable to give as an input to type checking or an output from
type synthesis. We may check that something already known to be a type is in fact
a \emph{universe}, or type of types. We may check the types of constructions or synthesize
the type of computations. We may look up the type of a variable. We may check two known types
for equality. We may form a judgement which assigns a valid type to a fresh variable and
then demands a judgement in the scope extended with that variable.

A judgement with a subject may be called a \emph{validation} --- its
purpose is exactly to establish trust in its subject. Type construction, type checking, type synthesis and type lookup are validations. A judgement with no subject may be called a
\emph{test} --- its purpose is to refine our knowledge of things which are already trusted.
The universe and type equality judgements are tests. A context extension is either a validation
or a test according to the status of its inner judgement.

If you are familiar with presentations of type theories, you will immediately notice the
omission of explicit \emph{contexts}. This is not mere laziness on my part, but is done
with a purpose that I shall shortly reveal. Typing rules conclude one $\gamma$-judgement
from zero or more $\gamma$-judgement premises, where all of these $\gamma$-judgements
implicitly share the same $\gamma$-context. Let us say that such rules are \emph{locally presentable}.
Locally presentable rules may extend the context, assigning a valid type to the new most local
variable, but they are not explicit about the global context in which they apply.


\subsection{Contexts: hello, and goodbye}

Free variables exist, so we must have contexts to explain them.

\begin{definition}[$\gamma$-context, actions on contexts, global
  judgements, context validity]
  A $\gamma$-context, $\Gamma$ is a vector, mapping variables in $\gamma$ to
  liberal constructions over $\gamma$:
  \[
    \Gamma : (\Tm\lib\cons\gamma)^\gamma
    \]
  Informally, we write it as a sequence of type assignments,
  $x_1:S_1,\ldots,x_n:S_n$. We may write $\sel\theta\Gamma$ for selections
  from $\Gamma$, thence $\sel x\Gamma$ for projections from
  $\Gamma$. Meanwhile, thinnings and substitutions act pointwise as
  $\Gamma\theta$ and $\Gamma\sigma$ respectively. In particular, $\wka\Gamma = \Gamma\wk$.
  Given such a $\Gamma$, we may write $\Gamma\vdash J_\gamma$ to assert
  the \emph{global judgement} that a particular $\gamma$-judgement holds in $\Gamma$.
  A rule expressed in terms of global judgements is \emph{globally
    presented}.
  Context \emph{validity} is defined inductively, as follows:
  \begin{itemize}
  \item $\emp$ is a valid $\emp$-context.
  \item If $\Gamma$ is a valid $\gamma$-context and $\Gamma\vdash\ty
    S$, then $\wka\Gamma,x\!:\!\wka S$ is a valid $\gamma,x$-context.
  \end{itemize}
\end{definition}

At least two things may strike you as peculiar.
\begin{itemize}
\item The property that the types of later variables may depend only on
  earlier variables is not encoded in the \emph{syntax} of contexts,
  but only in their \emph{validity}. The consequence of this
  nonstandard choice is that the selective action of thinning makes
  syntactic sense, and in particular, the projection $x\Gamma$ from a
  $\gamma$-context yields a term in $\gamma$, which is exactly what we
  need when we \emph{use} the context.
\item Context validity is not presented as a
judgement. This, also, is purposeful: it prevents the
\emph{re}validation of the context by typing rules. I thus depart from
standard practice of taking the validity of the empty context as the
only axiom, where the rules which concern atomic or variable subjects
take context validity as a premise. In the client-server analysis of typing
rules, we may consider context validity to be entirely the responsibility
of the client: garbage in, garbage out! The precondition for the context
extension judgement form does exactly the work required to ensure that the
extended context is valid if the original context is.
\end{itemize}

There are but two globally presented rules:
\begin{definition}[context extension, type lookup\label{def:cxrules}]
  \[\textsc{extend}\;
    \Rule{\wka\Gamma,\cxe x{\wka S} J}
    {\Gamma\vdash \cxe xS J}
    \qquad
    \textsc{lookup}\;\Axiom{\Gamma\vdash x\in \sel x\Gamma}
  \]
\end{definition}
The client invoking the \textsc{extend} rule must ensure that $\Gamma$ is valid and that
$\Gamma\vdash\ty S$, which is sufficient to ensure the validity of the context
in the premise. Meanwhile, the client invoking the $\textsc{lookup}$
rule must again ensure that $\Gamma$ is valid, which will allow the
server to guarantee that $\Gamma\vdash \ty{\sel x\Gamma}$, provided we can
ensure that type construction is stable under thinning.

We shall develop the technical apparatus to achieve stability with respect to actions
on free variables shortly, but for now, let me give the high-level idea. If you never talk
about free variables, the truths you tell will be preserved by
operations which act only on free variables.

\begin{dogma}[locally presented rules]
  Only context extension and type lookup are globally presented. Rules for all other
  judgement forms are locally presented.
\end{dogma}
\begin{proof}
  This is borne out in Definitions \ref{def:var}, \ref{def:cod},
  \ref{def:prepost}, and \ref{def:tycon} through \ref{def:univ}.
\end{proof}

Of course, given that variables do occur free in computations, we shall need one rule
to synthesize their types. The purpose of the type lookup judgement is to permit
its local presentation.

\begin{definition}[variable type synthesis\label{def:var}]
  \[\textsc{var}\;
      \Rule{\cxl xS}
         {\syn xS}
    \]
\end{definition}

However, apart from this rule, access to the global context is absolutely forbidden.

\begin{dogma}[\label{dogma:free}no context access]
  The \textsc{var} rule is the only rule which may
  use the type lookup judgement.
  All other rules must be locally presented, referring only to
  schematic variables and term variables which
  are bound locally within judgements, either by abstraction or by context extension.
\end{dogma}
\begin{proof}
  This will be a consequence of the formulae permitted in rules by
  Definitions \ref{def:prem}, \ref{def:prems}, and \ref{def:tycon} through \ref{def:univ}.
\end{proof}

When we examine more formally the formulae which may appear in typing
rules, let us do so in accordance with this dogma.

Intuitively, it is clear that variable lookup is stable under
thinning. Inserting more free variables does not preclude access to
those we already know about. There is no shadowing in a de Bruijn system.
We shall similarly achieve stability under substitution by replacing
uses of the \textsc{var} rule with other synthesis derivations whose types
match, thinned as necessary. In order to ensure that these intuitions
can be realised, let us stop talking about free variables before we say
something we have cause to regret.


\subsection{Change of direction, type equality and reduction}

The configurable parts of a type theory in this setting are the rules
which police constructions, whether they represent types, values or
eliminators. Let us treat the remaining syntactic constructs once and
for all.

\begin{definition}[change of direction\label{def:cod}]
  The following rules shall exist in all systems.
  \[
    \textsc{thunk}\;
    \Rule{\syn{n}{S}\quad S=T}
    {\chk T{[n]}}
    \qquad
    \textsc{universe}\;
    \Rule{\syn{n}{S}\quad\univ S}
         {\ty [n]}
    \qquad
    \textsc{radical}\;
    \Rule{\ty T\quad \chk Tt}
    {\syn{(t:T)}T}
  \]
\end{definition}

For the purposes of this paper, type equality will be given by the axiom
\[
  \textsc{reflexivity}\;\Axiom{T = T}
\]
which is the only nonlinear rule I shall permit here, but it serves to make
an insistence on linearity in schematic variables --- at least, those which
stand for types --- unproblematic. The \textsc{thunk} rule insists that when
we have a synthesized type and a type to check, they must agree precisely
(or, for humans, they must agree up to the renaming of bound variables).
Meanwhile, we may have computations in a type only if the type synthesized
for that computation is recognizably a type of types. Lastly, while a radical
offers us a candidate for the type, $T$, to synthesize, we must check both that it
really is a type, and, now that $T$ is known to be a type, that $T$ accepts the
term, $t$.

It is worth remarking that more generous notions of type equality are worthy
of consideration. Moreover, as the \textsc{thunk} rule is clearly \emph{directed},
we could relax the requirement to the demand that $S$ be a \emph{subtype} of $T$.
That might be one way to arrange for a Russell-style cumulative hierarchy of
universes. Let us leave those possibilities for the future.

We have not yet treated the possibility of
\emph{reduction} in types, without which our notion of type equality
is overly restrictive. Calculemus!

\begin{definition}{type reduction, pre-checking and post-synthesis\label{def:prepost}}
\[
  \textsc{pre}\;
  \Rule{T\leadsto T' \quad \chk{T'}t}
  {\chk Tt}
  \qquad
  \textsc{post}\;
  \Rule{\syn eS\quad S\leadsto S'}
  {\syn eS}
\]
\end{definition}

The above rules allow us to compute a type to a canonical form before
checking a construction, or after the synthesis of a target type, so
that we can check an elimination. Moreover, either side of the
\textsc{thunk} rule, which insists on exact type equality, we may
compute synthesized and checked types to a common reduct.

Reduction is generated by the contextual closure of the $\beta$-rules.
It is not a judgement, but merely a scope-respecting binary relation
on liberal constructions and computations, needing no context. Even
so, it should be given a precondition-postcondition specification,
making clear which judgements are preserved by reduction: that will be
the \emph{subject reduction} property, and we shall attend to it in
due course. In the meantime, let us establish what is common to all
the reduction systems under this setup.

\begin{definition}[contextual closure of reduction\label{def:reduction}]
  \[
    \Rule{\ra tT\:s\leadsto_\beta \ra{t'}{T'}}
         {\ra tT\:s\leadsto \ra{t'}{T'}}
\]
\[
  \Rule{s\leadsto s'}
  {\cn st\leadsto\cn{s'}t} \qquad
  \Rule{t\leadsto t'}
  {\cn st\leadsto\cn s{t'}} \qquad
  \Rule{t\leadsto t'}
  {\la xt\leadsto\la xt'} \qquad
  \Rule{n\leadsto e}
  {\Ne n \leadsto \Ne e}
\]
\[\Rule{e\leadsto e'}
  {e\:s\leadsto e'\:s} \qquad
  \Rule{s\leadsto s'}
  {e\:s\leadsto e\:s'} \qquad
  \Rule{t\leadsto t'}
  {\ra tT \leadsto \ra{t'}T} \qquad
  \Rule{T\leadsto T'}
  {\ra tT \leadsto \ra t{T'}} \qquad
\]
We may clearly extend reduction pointwise to contexts,
$\Gamma\leadsto\Gamma'$.
Let $\leadsto^\ast$ be the reflexive transitive
closure of $\leadsto$. Let $\leadsto^?$ be the union of $\leadsto$
with equality, i.e., the relation that characterizes taking at most
one step.
\end{definition}

The fact that these rules offer no base case should not cause alarm:
it is for individual systems to identify specific radical eliminations
$\ra tT\:s$ as $\beta$-redexes. More particularly, if every well typed radical
elimination is a redex in at least one way, we shall guarantee
\emph{progress}, and if every well typed radical elimination is a
redex in at most one way, we shall guarantee \emph{confluence}.
Subject reduction amounts to showing that the assumptions which must
hold for a redex to be well typed are sufficient to show that the
redex has the same type. If we are systematic about how to frame
these requirements in terms of the typing rules, we shall have these
properties by construction.

We now have all the \emph{generic} rules for bidirectional type
systems, in the sense of this paper.
Before we turn to the machinery which allows us to specify individual type
theories by classifying the constructions which occur in them, let us establish
a concrete running example from which to generalise.


\subsection{A bidirectional reconstruction of Martin-L\"of's 1971 type theory\label{sec:ml71}}

I take Martin-L\"of's 1971 type theory~\cite{martinloef:atheoryoftypes} as
the exemplary system for two reasons. Firstly, it is small:

\newcommand{\trg}{\bullet}
\begin{definition}[bidirectional type-in-type]
\[\begin{array}{c}
  \Axiom{\ty\U}\qquad
  \Rule{\ty S\quad \cxe xS \ty T}
    {\ty \PI SxT}\qquad\qquad
  \Axiom{\univ \U}
  \\
  \Rule{\ty T}
  {\chk\U T}
  \qquad
  \Rule{\cxe xS \chk Tt}
  {\chk {\PI SxT}{\la x t}}
  \qquad\qquad
  \Rule{e\in\PI SxT\quad \chk Ss}
       {e\:s\in T\fsl\ra sS}
\end{array}\]
\[
  \Axiom{\ra{\la xt}{\PI SxT}\:s \leadsto_\beta \ra{t\fsl\ra sS}{T\fsl\ra sS}}
\]
\end{definition}

Secondly, it is inconsistent: it is famously possible to construct a term
which does not $\beta$-normalize. We shall not be able to appeal to normalization
in our efforts to establish any of its other metatheoretic properties, such as
subject reduction.

In addition to $\Pa$, we have two atoms, $\U$ and $\Uppi$, which tag our canonical
types --- the universe and function spaces, respectively. We further assert that
$\U$ may stand as a type of types, which allows types to be computations for which
the type $\U$ may be synthesized. The inconsistency arises from the rule which
makes any type, including $\U$ itself, checkably an inhabitant of $\U$. A
function type $\PI SxT$ tells us how to check an abstraction, which we need not
tag with a constructor. When we can synthesize a function type for the target
of an elimination, that tells us to interpret the entire eliminator as the function's
argument and check that it inhabits the function's domain.

Let us look more closely at the rules for checking and synthesizing types. We can see that
rules for judgements $\ty T$ and $\chk Tt$ trade only in essential constructions,
with premises checking components of the conclusion's subject. Meanwhile, the synthesis
rule makes no syntactic analysis of its target, requiring only a particular construction
for its synthesized type, in order to check the construction of the eliminator
and deliver the synthesized type of the whole.

The typing rules do not talk about free variables. The only schematic variable
which stands for a computation is the $e$ in the application rule, and it is used in a
very particular pattern.

\begin{dogma}[targets of eliminations]
  We are free to write synthesis rules only for eliminations. These rules must
  name the target but not analyse its syntactic structure. The first premise
  of an elimination rule must synthesize the type of the target.
\end{dogma}
\begin{proof}
  This will be the substance of Definition \ref{def:elimrule}.
\end{proof}

That is, the only real choices we make, even in the synthesis rules, are how to
analyse constructions. It is in the light of that observation that we
should ask `What are the redexes?'. The only elimination rule demands
the type $\PI sxT$ for its target, $e$. Na\"\i{}vely ignoring pre- and post-computation
for the moment, the only way $e$ can be a radical is if $\PI sxT$ is a
type, and if the canonical term thus annotated is checked at that
type, which means it must be $\la xt$. Working backwards from the type
formation, we learn what must be true about $S$ and $T$; working
backwards from type checking, we learn what must be true of $t$;
working backwards from the elimination rule, we learn what must be
true of $s$, and what must be the type component of the radical
reduct. We must have
\[
  \ty S \qquad \cxe xS\ty T\qquad \cxe xS T\ni t \qquad S\ni s
\]
and we must deliver exactly one $\beta$-rule, of the form
\[
  \ra{\la xt}{\PI SxT}\:s \leadsto_\beta \ra{\;\;?\;\;}{T\fsl\ra sS}
\]
Our assumptions certainly justify
\[
  \ra sS \in S \qquad \cxe xS \ra tT \in T
\]
so that if we can ensure stability under substitution, we shall have
\[
   \ra{t\fsl\ra sS}{T\fsl\ra sS} \in T\fsl\ra sS
\]
which rather suggests that we take $?=t\fsl\ra sS$. Showing that
$\ty T\fsl\ra sS$ is necessary, even to discharge the postcondition
for the elimination rule. The obligation that is peculiar to the
$\beta$-rule is to complete the radical by showing $T\fsl\ra sS\ni
t\fsl\ra sS$.

It is instructive to consider what would happen, were we to extend
our theory with dependent pairs. We should add
\[\begin{array}{c}
  \Rule{\ty S\quad \cxe xS \ty T}
  {\ty \SG SxT}\qquad\qquad
  \Rule{S\ni s\quad T\fsl\ra sS\ni t}
  {\SG SxT\ni\cn st}\\
  \Rule{e\in \SG SxT}
       {e\:\car\in S}\qquad
  \Rule{e\in \SG SxT}
       {e\:\cdr\in T\fsl e\:\car}\qquad
\end{array}  \]
Take note that the type of the second projection, $e\:\cdr$, depends
on the value of the first, $e\:\car$. That is enough to show that
we shall need schematic variables for at least one computation --- the
target of an elimination --- even though our other schematic variables
all stand for constructions. That the type of a function application
does not depend on the function is a lucky break which we should not
take for granted!

Now, for the $\beta$-rules, we may play the same game of `working
backwards' to find the assumptions underpinning the existence of
redexes. What is that game, exactly? We are \emph{unifying} the
type checked in an introduction rule with the target's type in
an \emph{elimination} rule, in order to obtain \emph{all} possible
radical eliminations. In this case, we find
\[\begin{array}{lcl}
  \ra{\cn st}{\SG SxT}\:\car&\leadsto_\beta& \ra{\;\;?_\car\;\;}S \\
  \ra{\cn st}{\SG SxT}\:\cdr&\leadsto_\beta& \ra{\;\;?_\cdr\;\;}{T\fsl\ra{\cn st}{\SG SxT}\:\car}
\end{array}\]
How do we know we have found them \emph{all}?
\begin{dogma}[unification]
We must ensure that
the language for writing types checked in introduction rules and
for target types analysed in elimination rules has not only decidable
unification, but also \emph{most general} unifiers.
\end{dogma}
\begin{proof}
  \textbf{To be discharged.}
\end{proof}

Returning to the problem of finding our reducts, we know
\[
  \ty S \qquad \cxe xS\ty T \qquad S\ni s\qquad T\fsl\ra sS\ni t
  \]
so we may certainly take $?_\car = s$. But may we take $?_\cdr = t$,
as one might hope? In order to do so, we must immediately precompute with
the $\beta$-rule for $e\:car$, yielding
\[
  \Rule{T\fsl\ra{\cn st}{\SG SxT}\:\car\leadsto_\beta T\fsl\ra sS\quad T\fsl\ra sS\ni t}
       {T\fsl\ra{\cn st}{\SG SxT}\:\car\ni t}
  \]

That is to say, the justification of some $\beta$-rules depends on the
deployment of others. We now have a balance to strike, if we want a
machine to check our systems of rules: on the one hand, reduction may
be nonterminating; on the other, some reduction is necessary. How much
reduction is judicious? Let us ponder that anon.


\section{Ruling the rules}

We have so far been thinking about the judgement forms for our type
systems and the actual derivations we can make of actual judgements
about actual terms. In doing so, we have identified some informal
disciplines to which we should like the typing rules to adhere. It is
time to make these disciplines precise and enforce them when we write
the formal judgements that comprise the premises and conclusion of
each of the typing rules under our control.


\subsection{How rules talk about constructions: patterns and expressions}

The formulae which occur in typing rules are not terms of the underlying
type theory. They contain \emph{schematic variables} and stand for the sets
of terms generated by instantiating those schematic variables. If we are
to achieve a generic approach to metatheory, we shall have to be precise
about what constitutes these formulae, and how the schematic variables are
scoped.

The modes of the places in the judgement forms determine whether a given
construction is being analysed by the rule or synthesized by it. Which formulae
are appropriate depends on which of these two activities is happening, so
we shall have \emph{two} syntaxes of formulae in typing rules --- \emph{patterns}
for analysis and \emph{expressions} for synthesis. Patterns contain the
binding sites for schematic variables; expressions the use sites.

\begin{dogma}[\label{dogma:mode}formula syntax by mode]
  The following table summarises the permitted formulae for each mode in premises
  and conclusions.
  \[\mbox{\begin{tabular}{r@{~ ~}@{~ ~}c@{~ ~}c@{~ ~}c}
                    & \textbf{input} & \textbf{subject} & \textbf{output} \\
            \textbf{premise}  & expression & schematic variable & pattern \\
     \textbf{conclusion} & pattern & pattern & expression \\
          \end{tabular} }\]
  Moreover, the type given in a context extension for a premise is an expression.
\end{dogma}
\begin{proof}
  The characterization of premises is given in Definitions
  \ref{def:prem} and \ref{def:prems}. The characterization of
  conclusions is given in Definitions \ref{def:tycon} through \ref{def:univ}.
\end{proof}

Informally, we have already learned quite a lot about the difference
between patterns and expressions. For example, we know that
substitution in schematic variables, such as our commonly occurring
$T\fsl\ra sS$, makes sense only in \emph{expressions} because we
can compute substitutions but not invert them. You may readily check
that the substitutions used in our rules so far occur only in
expression positions. Moreover, we have learned that the language of
patterns must be sufficently weak as to sustain a computable notion of
most general unifier: we can have no such hope for expressions.

The scoping of schematic variables in rules will be governed by the
following dogma.

\begin{dogma}[clockwise flow\label{dogma:clockwise}]
  Scoping of schematic variables in typing rules flows clockwise
  around each typing rule, from the inputs and subjects of the
  conclusion, left-to-right through the premises, then into the
  outputs of the conclusion. The schematic variables in the outputs
  of a premise are brought into scope after that premise.
\end{dogma}
\begin{proof}
  This will hold by construction as a consequence of Definitions
  \ref{def:prem}, \ref{def:prems}, and  \ref{def:tycon} through \ref{def:univ}.
\end{proof}

The effect of this dogma is to ensure a viable strategy for
implementing a type checking algorithm. It is clear how we will know
each of the things referred to by schematic variables --- by pattern
matching --- in time to make use of them in expressions.

However, there is a further refinement of schematic scoping which has
a powerful and subtle impact.

\begin{dogma}[validation of subject components\label{dogma:subject}]
  Each schematic variable in a conclusion's subject will be a premise
  subject exactly once, and no schematic variable may be the
  subject of a premise unless it is bound by the subject of the
  conclusion. No subject schematic variable will be used in an
  expression until after it has been validated as the subject of
  a premise.
\end{dogma}
\begin{proof}
  This will hold by construction as a consequence of Definitions
  \ref{def:prem} and \ref{def:prems}.
\end{proof}

This dogma captures the idea that the schematic variables in the
conclusion inputs and the premise outputs are already sufficiently
trusted that there is never any need to revalidate them. The schematic
variables in conclusion subjects, however, must be validated before it
is safe to use them. We thus achieve a significant decrease in the
complexity of subject reduction proofs, because the status of a
schematic variable tells us how much computation is safe. Trusted
things --- inputs and outputs --- may reduce arbitrarily; untrusted
things --- subjects --- may take at most one step. A thing which may
have reduced arbitrarily never appears in a subject position where
computation is restricted, so the induction hypothesis for the
conclusion will always apply: if the conclusion subject takes at most one step,
then each of the premise subjects --- its components --- takes at most
one step.

We may now \emph{state} a sensible formulation of \emph{subject
  reduction}. Indeed, we may \emph{compute} it from the judgement
forms. Suppose an `old' judgement holds; compute its context and inputs
arbitrarily and its subjects by at most one step; there exist new
outputs computable somehow from the old outputs such that the new
judgment holds.

\begin{theorem}[subject reduction]
  The following are admissible.
  \[\begin{array}{c}
    \Rule{\Gamma\vdash \ty T\quad \Gamma\leadsto^\ast\Gamma'\quad
      T\leadsto^?T'}
      {\Gamma'\vdash \ty T'}
    \\
    \Rule{\Gamma\vdash \chk Tt\quad \Gamma\leadsto^\ast\Gamma' \quad
      T\leadsto^\ast T'\quad t\leadsto^?t'}
      {\Gamma'\vdash \chk{T'}{t'}}
    \qquad
    \Rule{\Gamma\vdash \syn eS\quad  \Gamma\leadsto^\ast\Gamma' \quad
      e\leadsto^?e'}
      {\exists S'.\;\Gamma'\vdash\syn{e'}{S'} \wedge S\leadsto^\ast
      S'}
   \end{array}\]
\end{theorem}

The proof of this theorem is, of course, deferred.

Let us now design the language of patterns, then consider how to
manage the business of binding schematic variables for use in
expressions.


\subsection{There are some questions it is better not to ask}

It is the job of a formal pattern to ask questions about an actual
term. `What is a sensible question?' is a sensible meta-question.
For example, `Are you the free variable $x$?' is a \emph{dangerous}
question, because the answer is `Yes.' for the free variable $x$, but
perhaps not for some substitution instance of $x$. As Dogma
\ref{dogma:free} indicates, our rules should not ask such questions if
we want typing derivations to be stable under substitution.

We shall have some language of patterns ($p,q$) which come equipped with a
notion of $p$-environment ($\pi,\chi$) mapping schematic variables in
$p$ to actual terms, such that any actual term `matching' the pattern
$p$ will given by the action, $\pi p$, for some $p$-environment $\pi$. The grammar of
possible $p$s will be some restriction of the grammar of actual terms.
We had better ensure that environments can be acted on by thinnings,
$\pi\theta$, and by substitutions, $\pi\sigma$. Moreover, we must
guarantee that
\[
  (\pi p)\theta = (\pi\theta)p\qquad
  (\pi p)\sigma = (\pi\sigma)p
\]

The latter should remind us to ask whether there are any syntactic
constructs, other than free variables, which are \emph{not} preserved
by substitution, and in our setup, we should remember that
\[
  \Ne x\fsl\ra tT = t
\]
from which we deduce that `Are you a thunk?' is also a dangerous
question for a pattern to ask.

\begin{remark}[the $\upsilon$ alternative]
  In previous work, I have proposed to permit terms of form $\Ne{\ra
    tT}$ and hence to allow structural substitution for thunks, at the
  cost of adopting the additional reduction rule,
  \[
    \Ne{\ra tT} \leadsto_\upsilon t
  \]
  where $\upsilon$ is the grunt of satisfaction at the complete
  elimination of a cut. However, such a rearrangement does not
  affect the sensible notion of pattern, because of the following
  consideration.
\end{remark}

\begin{dogma}[stability of pattern matching under reduction]
  Consider pattern $p$. If $t=\pi p$ for some $p$-environment, $\pi$,
  and $t\leadsto t'$, then there must exist some $\pi'$ such that
  $t'=\pi' p$ and $\pi\leadsto\pi'$, where the latter is the
  extension of reduction to environments which allows reduction in exactly
  one term therein.
\end{dogma}
\begin{proof}
  Given Definition \ref{def:reduction}, we shall shortly ensure this property
  via Definition \ref{def:pat}.
\end{proof}

Why should we adopt such a dogma? Given that we may precompute types
before checking them by pattern matching and postcompute synthesized
types before analysing them, again by pattern matching, it would be
awkward, to say the least, if positive matches were transient. It is
one thing to say that some computation must be done before a match is
achieved, but quite another to say that some computation must be
avoided to ensure that a match is sustained --- `blink and you'll
miss it'. There are two properties which are reliably stabilised
by computation:
\begin{itemize}
\item being an \emph{essential} construction (or `canonical form')
\item \emph{independence} of a variable
\end{itemize}

Independence of free variables is not stable under substitution,
but independence of \emph{bound} variables is unproblematic, as they
are unaffected by substitution. Accordingly, we have found a class
of syntactic questions which a pattern may reasonably ask. Let us
define their grammar.


\subsection{Patterns}

\newcommand{\Pat}[1]{\textbf{Pat}\:#1}
\begin{definition}[$\gamma$-pattern\label{def:pat}]
The grammar of patterns, $\Pat\gamma$, for a given scope is as follows:
\[\begin{array}{rrl@{\quad}l}
    p_\gamma, q_\gamma & ::= & a \\
         &   | & (p_\gamma.q_\gamma) \\
         &   | & \la x q_{\gamma,x} \\
         &   | & \theta & \mbox{where}\;\exists\delta.\,\theta :
                          \delta\thin\gamma \\
         &   | & \bot
  \end{array}\]
where $\bot$ is the pattern which \emph{never} matches, and we identify
\[\bot = \cn\bot q = \cn p\bot = \la x\bot
  \]
\end{definition}

\newcommand{\huth}[1]{\langle #1 \rangle}
That is, patterns contain only the essential constructions, together with
placeholders bearing a thinning whose purpose is to specify which of
the variables in $\gamma$ may occur in the place. They go where constructions
go. The same Lisp convention for avoiding dots in right-nested tuples applies.
Formally, the pattern used for the function type in our example theory is
\[
  \PI \ith {}\ith
\]
with two placeholders, the second of which allows dependency on the
bound variable.

It is straightforward to define the action of a thinning on a pattern: if
$\theta : \gamma\thin\gamma'$ then $\cdot\theta :
\Pat\gamma\to\Pat{\gamma'}$, going under binders in the same way as
for terms, and acting on placeholders by composition.
The effect of this is to grow the notional scope of a pattern while
forbidding any of the inserted variables to occur in its places.

Informally, of course, we write distinct names in the places: these are the
binding sites for the schematic variables. We may omit the identity thinning,
hence
\[
  \PI SxT
\]
If we wish a different thinning, we may attach angle brackets to the name,
listing the permitted variables. We could also have written:
\[
  \PI{S\huth{}}x{T\!\huth{x}}
\]
Meanwhile, the pattern $\PI{S\huth{}}x{T\!\huth{}}$ matches only function types which
happen to be non-dependent.

\newcommand{\pr}{\preceq}
The $\bot$ pattern may seem a trifle peculiar: it would certainly be
rather pointless to use it in a rule, as that would ensure its
inapplicability. My motivation for introducing it is primarily
algebraic. Patterns have a sensible notion of \emph{refinement}:
informally, $p\pr p'$ if everything matching $p$ also matches $p'$. The top of this
ordering is the identity thinning $\ith$, and $\bot$ is its bottom.

\begin{definition}[pattern refinement]
  Pattern refinement is given inductively as follows.
  \[
    \Axiom{a\pr a}\qquad
    \Rule{p\pr p'\quad q\pr q'}
    {\cn pq \quad \cn{p'}{q'}}\qquad
    \Rule{q\pr q'}
    {\la xq \pr \la xq'} \qquad
    \Axiom{p\theta\pr \theta}\qquad
    \Axiom{\bot\pr p}
    \]
\end{definition}

In due course, we shall establish that refinement makes patterns a
category with \emph{pullbacks}, giving us the notion of most general
unifier that we need.

Now, to make patterns talk about terms, we need the environments, $\pi$, that
explain what `matching' means.

\newcommand{\Ev}[1]{#1}
\begin{definition}[matching $p$-environment]
  A pattern $p : \Pat\gamma$ induces a set of $p$-environments, whose
  name we may abbreviate to $\Ev p$, defined inductively as follows.
  \[
    \Axiom{a : a}\qquad
    \Rule{\pi : p\quad \chi : q}
    {\cn\pi\chi : \cn pq}\qquad
    \Rule{\chi : q}
    {\la x\chi : \la x q}\qquad
    \Rule{t : \Tm\lib\cons\delta\quad \theta:\delta\thin\gamma}
         {t : \theta}
       \]
  Such a $p$-environment, $\pi$, acts on $p$ to give
  \[\pi p : \Tm\lib\cons\gamma \]
  where, in particular, the action $t\theta$ is the term $t\theta$.
  A term \emph{matches} $p$ if it can be expressed as $\pi p$ for some $\pi$.
\end{definition}

Speaking formally, what then, is a \emph{schematic variable}? As
patterns constitute the binding sites of schematic variables, we may
consider latter to be given by \emph{paths} to placeholders.

\newcommand{\from}{\leftarrow}
\newcommand{\here}{\bullet}
\begin{definition}[schematic $p$-variable, projection from environment]
  If $p : \Pat\gamma$, let $\delta\from p$ be the set of schematic
  $p$-variables, $\xi$, with scope $\delta$, defined inductively as follows.
  \[\Rule{\theta : \delta\thin\gamma}
    {\here : \delta\from \theta}\quad
    \Rule{\xi : \delta\from p}
    {\cn\xi{}:\delta\from \cn pq} \qquad
    \Rule{\xi : \delta\from q}
    {\cn{}\xi:\delta\from \cn pq} \qquad
    \Rule{\xi : \delta\from q}
    {\la{}\xi:\delta\from \la xq}
  \]
  Moreover, given such a $\xi$ and some $\pi : p$, we obtain
  $\sel\xi\pi : \Tm\lib\cons\delta$ by following the path.
\end{definition}

Informally, of course, we shall continue to use \emph{names} for
schematic variables. As you can see where each name stands in the
pattern which binds it, you can tell what its formal path must be.

\paragraph{Computation patterns}~
So far, we have considered patterns for \emph{constructions}. However,
we have seen in our examples, particularly the elimination rule for
$\cdr$, that we have need of schematic variables which stand for
\emph{computations}. We have learned also that no finer analysis of
computations is stable. Accordingly, schematic variables are the only
sensible patterns for computations, naming the whole of them. We may
thus keep track of them by reusing the notion of scope from the
underlying term syntax. Let us, however, keep a clear separation
between the scope of schematic variables in an expression and the
scope of its term variables.


\subsection{Expressions}

Let us now define the notion of \emph{expression} which appears in
formal judgements, with a precise notion of \emph{schematic scope}.

\newcommand{\ssc}[2]{(#1|#2)}
\begin{definition}[schematic scope]
  A schematic scope is a pair $\ssc\delta p$, where $\delta$ is an
  ordinary scope, giving the schematic variables which stand for
  computations, and $p : \Pat\emp$, determining the schematic
  variables which stand for constructions.
\end{definition}

\newcommand{\Ex}[4]{\textbf{Expr}\:#1\:#2\:#3\:#4}
\begin{definition}[expression]
  The syntax of expressions $\Ex{\ssc\delta p}ld\gamma$, with
  schematic scope $\ssc\delta p$ and scope $\gamma$ extends that
  of terms, as follows:
\[\begin{array}{r|l|l}
 d \;\backslash\; l& \ess\mbox{ential} & \lib\mbox{eral} \\
\hline
    \cons{truction} & k_\gamma, K_\gamma & s_\gamma, t_\gamma, S_\gamma, T_\gamma \\
    & \;\begin{array}[t]{rl}
          ::= & a \\
          | & \cn{s_\gamma}{t_\gamma} \\
          | & \la xt_{\gamma,x}
        \end{array}
    & \;\begin{array}[t]{rl}
          ::= & k_\gamma \\
          | & \Ne{n_\gamma} \\
          | & \xi\fsl\sigma \quad\mbox{where}\;
              \xi : \gamma'\from p, \sigma : (\Ex{\ssc\delta p}\lib\comp\gamma)^{\gamma'}
        \end{array}
    \\
    \hline
    \comp\mbox{utation} & n_\gamma, N_\gamma & e_\gamma, f_\gamma, E_\gamma, F_\gamma \\
    & \;\begin{array}[t]{rl}
          ::= & x_\gamma \\
          | & e_\gamma\:s_\gamma \\
          | & x_\delta
        \end{array}
    & \;\begin{array}[t]{rl}
          ::= & n_\gamma \\
          | & \ra{t_\gamma}{T_\gamma}
        \end{array}
    \\
  \end{array}\]
  In particular, $\Tm ld\gamma$ is now seen to be $\Ex{(\emp,\Pa{})}ld\gamma$.
\end{definition}

That is, the essential computations are extended with schematic
computation variables from $\delta$, and the liberal constructions are
extended with \emph{instantiations} of schematic $p$-variables by a
substitution of the term variables upon which they may depend, hence
our commonplace $T\fsl\ra sS$. By informal convention, we may abbreviate
$\xi\fsl\isu$ to $\xi$, which means that if some $T$ is bound in a
pattern $\la x T$, we may write $T$ as an expression in any scope
containing $x$, as we have already seen in the rule for checking a
function:
\[  \Rule{\cxe xS \chk Tt}
  {\chk {\PI SxT}{\la x t}}
  \]
The binding site of $T$ is in the rule's conclusion: the checked type
$T$ in the premise is really abbreviating the expression $T\fsl x$, with
the $x$ in the premise's context extension effectively
capturing the bound $x$ in $T$.


\subsection{Premises}

Now that we know what an expression is, we can say what a
\emph{premise} is. The key point is that premises are defined with
respect to a schematic scope of \emph{trusted} schematic variables
and a pattern of \emph{untrusted} schematic variables. A type
construction or type checking premise will select one of the untrusted
schematic variables as its subject and establish trust in it.
Each such premise must grow the trusted schematic scope and remove the
subject from the untrusted pattern, until everything is trusted.

\begin{definition}[pattern variable removal, placelessness]
  If $q : \Pat\gamma$ and $\xi : \delta\from q$, then $q-\xi$ is given
  by replacing the placeholder in $q$ pointed to by $\xi$ with the
  atom $\Pa{}$. Likewise, if $\chi : \Ev q$, we may take $\chi-\xi$ to
  be the environment in $\Ev{q-\xi}$ given by replacing the term in
  $\chi$ pointed to by $\xi$ with the atom $\Pa{}$. A pattern with no
  placeholders is said to be \emph{placeless}.
\end{definition}

\newcommand{\Prem}[5]{\textbf{Prem}\:#1\:#2\:#3\:#4\:#5}
\begin{definition}[formal premise\label{def:prem}]
  If $\ssc\delta p$ is the schematic scope of trusted schematic
  variables, $q : \Pat\emp$ is the pattern of untrusted schematic variables,
  and $\gamma$ is the scope of term variables, then
  $\Prem{\ssc\delta p}q\gamma{p'}{q'}$ is the set of valid premises
  establishing \emph{fresh} trust in $p':\Pat\gamma$ and leaving $q' : \Pat\emp$
  untrusted, defined inductively as follows:
  \[\begin{array}{c}
     \Rule{\xi : \delta\from q\quad \theta:\delta\thin\gamma}
     {\ty{\xi\theta} : \Prem{\ssc\delta p}q\gamma{\theta}{(q-\xi)}}
    \\
     \Rule{T : \Ex{\ssc\delta p}\lib\cons\gamma \quad
       \xi : \delta\from q \quad \theta:\delta\thin\gamma}
     {\chk{T}{\xi\theta} : \Prem{\ssc\delta p}q\gamma{\theta}{(q-\xi)}}
  \\
  \Rule{S,T : \Ex{\ssc\delta p}\lib\cons\gamma}
    {S=T : \Prem{\ssc\delta p}q\gamma{\Pa{}}q}
    \qquad
    \Rule{T : \Ex{\ssc\delta p}\lib\cons\gamma}
    {\univ T : \Prem{\ssc\delta p}q\gamma{\Pa{}}q}
  \\
  \Rule{S : \Ex{\ssc\delta p}\lib\cons\gamma \quad
    J : \Prem{\ssc\delta p}q(\gamma,x){p'}{q'}}
  {\cxe xSJ : \Prem{\ssc\delta p}q\gamma{(\la xp')}{q'}}
  \end{array}\]
\end{definition}

Before we walk through this definition, let us see how it plugs into
the definition of a formal premise sequence, managing scope correctly.

\newcommand{\Prems}[3]{\textbf{Prems}\:#1\:#2\:#3}
\begin{definition}[formal premise sequence\label{def:prems}]
  If $\ssc\delta p$ is the schematic scope of trusted schematic
  variables, $q : \Pat\emp$ is the pattern of untrusted schematic variables, then
  $\Prems{\ssc\delta p}q{p'}$ is the set of valid premise sequences
  establishing \emph{overall} trust in $p':\Pat\emp$, defined inductively.
  \[
    \Rule{q\;\mbox{placeless}}
    {\emp : \Prems{\ssc\delta p}q{p}}
    \qquad
    \Rule{J : \Prem{\ssc\delta {p_0}}{q_0}\emp{p'}{q_1}\quad
          \vec J : \Prems{\ssc\delta {\cn{p_0}{p'}}}{q_1}{p_2}}
         {J\;\vec J : \Prems{\ssc\delta {p_0}}{q_0}{p_2}}
    \]
\end{definition}

There is quite a lot to take in, here, so we shall proceed slowly and carefully.

Firstly, note how the untrusted $q$s thread left-to-right. The empty
sequence demands that $q$ be placeless, ensuring that we stop only
once trust has been established in everything. The only place where
$q$ changes is in a \emph{validation} premise: a subject
$\xi : \delta\from q$ is selected, and the untrusted pattern $q-\xi$
is sent onwards and rightwards.

Secondly, notice how the trusted $p$s thread left-to-right. The
definition of \emph{premise} gives us a $p'$ which has
freshly become trustworthy because of that premise. Correspondingly,
in the step case of sequences, the trusted pattern for the tail is
$\cn{p_0}{p'}$, where $p_0$ is what was already trusted and $p'$
is what the head premise has validated. Observe that validation
premises give us a new trusted schematic variable --- the validated
subject --- while a test gives us nothing new. Furthermore, whenever
a premise contains an expression, it depends only on trusted
schematic variables.

Thirdly, let us consider what happens with the scope $\gamma$ of term
variables. A premise sequence consists of $\emp$-premises: we cannot
talk about free variables, reifying Dogma \ref{dogma:free}. A premise can,
however, locally extend the scope by means of context extension, with
a suitable expression giving the type of the bound variable. If we
eventually arrive at a validation, its subject $\xi\theta$ uses
$\theta$ to select which of those bound variables are the variables
$\xi$ depends on. That is, we insist $\xi$ is validated in full
generality, not just for a particular substitution
instance. Informally, of course, we never need to write this thinning,
because we choose the names in the context extension to capture the
names free at the subject's binding site.

Fourthly, you are entitled to some concern that there are no type
\emph{synthesis} premises. That is because elimination rules need
premises only to validate components of the \emph{eliminator}, which
are all constructions. The validation of the \emph{target},
synthesizing its type, can be treated uniformly, as we shall shortly
see.


\subsection{What are the rules?}

Let us now work through the judgement forms, characterizing the rules
which we may write for each.


\begin{definition}[formal type construction\label{def:tycon}]
A \emph{type construction} rule takes
the form
\[\Rule{\vec J}
       {\ty q}
\]
where $q:\Pat\emp$ and $\vec J : \Prems{\ssc\emp{\Pa{}}}q{p'}$ for some
$p'$.
\end{definition}
That is, we initially trust \emph{nothing}, but we learn to trust
the things in $q$ as we work our way through $\vec J$.

\begin{definition}[formal type checking]
A \emph{type checking} rule takes the form
\[\Rule{\vec J}
       {\chk pq}
\]
where $p,q:\Pat\emp$ and $\vec J : \Prems{\ssc\emp p}q{p'}$ for some
$p'$.
\end{definition}
That is, we initially trust the components of the \emph{type},
and we learn to trust the components of the term.

\begin{definition}[formal elimination rule\label{def:elimrule}]
The \emph{type synthesis} rule for an
elimination takes the form
\[\Rule{\syn ep \quad \vec J}
       {\syn{e\:q}S}
\]
where $p,q:\Pat\emp$, $\vec J : \Prems{\ssc ep}q{p'}$ for some $p'$, and
$S : \Ex{\ssc e{p'}}\lib\cons\emp$.
\end{definition}
That is, we first synthesize the
type of the target, $e$ and match it with a pattern, $p$, whose
components we trust because of the type synthesis
postcondition. Meanwhile, we have matched the eliminator with a
pattern, $q$. We may now check the eliminator with a premise sequence
trusting $p$ and also the target, $e$ --- our one schematic
computation variable. By the time we have validated the eliminator, we
now trust $p'$, so the synthesized type may refer to $e$ and to the
components of $p'$.

\begin{definition}[formal universe checking\label{def:univ}]
The rules for checking whether a known type is a
\emph{universe} take the form
\[\Rule{\vec J}
       {\univ p}
  \]
where $p:\Pat\emp$ and $\vec J : \Prems{\ssc\emp p}{\Pa{}}{p'}$ for
some $p'$.
\end{definition}
Here, we already trust the components of the type, so there
is nothing untrusted.

\begin{example}[Martin-L\"of's 1971 theory]
By inspection, the configured rules for our reconstruction of
Martin-L\"of's 1971 theory (see Section \ref{sec:ml71}) are admitted by the above definitions.
\end{example}


\subsection{How to apply a rule}

We have said how to construct the formal rules from \emph{formal}
judgements. Let us now say how to deploy those rules to derive
\emph{actual} judgements. In particular, even though the rules never
talk about free variables, the actual terms they talk about may very
well contain free variables.

How are we to allow patterns $p : \Pat\emp$ to refer to terms over
some $\gamma$? How are we to instantiate expressions with the
resulting environments? The first step is to define the \emph{opening}
of a pattern with respect to a scope.

\begin{definition}[pattern opening]
  If $p : \Pat\delta$, let $\gamma\otimes p : \Pat{(\gamma,\delta)}$
  given by replacing each $\theta$ in $p$ with $\ith_\gamma,\theta$.
\end{definition}

Correspondingly, if $\pi:\gamma\otimes p$, then the term in $\pi$
which instantiates some $\xi : \delta'\from p$ will have scope
$\gamma,\delta'$. That is to say, it may use free variables, along
with those of the locally bound variables selected by its placeholder.
A term $t:\Tm\lib\cons{(\gamma,\delta)}$ matches a pattern
$p:\Pat\delta$ if it can be expressed as $\pi(\gamma\otimes p)$ for
some environment $\pi$.

A schematic scope $\ssc\delta p$ can thus be instantiated in scope $\gamma$ by a pair
$\ssc\rho \pi$ where $\rho : \delta\su\gamma$ and $\pi : \gamma\otimes
p$.

\begin{definition}[instantiation]
  Let $\ssc\delta p$ be a schematic scope and $\gamma$ some target
  scope of free variables. Let  $\rho : \delta\su\gamma$ and $\pi :
  \gamma\otimes p$. We may define an action
  \[
    \cdot\ssc\rho\pi : \Ex{\ssc\delta p}ld{\gamma'} \to
      \Tm\lib d{\gamma,\gamma'}
  \]
  The action proceeds structurally, extending $\gamma'$ to go under a
  binder without modifying $\rho$ or $\pi$. We need only explain what
  happens at variables and schematic variables:
  \begin{itemize}
    \item A term variable $x$ in $\gamma'$ becomes
      $x(\thinr\gamma{\gamma'})$.
    \item A schematic computation variable $x$ in $\delta$ becomes
      $x\rho(\thinl\gamma{\gamma'}$.
    \item For $\xi\fsl\sigma$, with $\xi:\delta'\from p$ and
      $\sigma:(\Ex{\ssc\delta p}ld{\gamma'})^{\delta'}$, we first
      obtain
      \[
        \sel\xi\pi : \Tm\lib\cons\gamma,\delta' \qquad
        \sigma\ssc\rho\pi : \delta'\su\gamma,\gamma'
      \]
      We then substitute
      \[
        (\sel\xi\pi)(\isu(\thinl\gamma{\gamma'}),\sigma\ssc\rho\pi)
      \]
      which is a precise way of saying that we project $\xi$'s
      term from the $\pi$ and substitute its bound variables
      with the instantiation of $\sigma$, letting its free variables be.
  \end{itemize}
\end{definition}

To deploy this machinery, we need to follow the structure of
Definitions $\ref{def:prem}$ and $\ref{def:prems}$, repeating the same
juggling act for trusted and untrusted environments, $\pi$ and $\chi$,
respectively, as we did between the trusted and untrusted patterns,
$p$ and $q$. Fortunately, there is only one way to do it.

\newcommand{\prem}[4]{\textbf{prem}\:#1\:\ssc{#2}{#3}\:#4}
\newcommand{\prems}[4]{\textbf{prems}\:#1\:\ssc{#2}{#3}\:#4}
\begin{definition}[actual premise]
  If $J : \Prem{\ssc\delta p}q\gamma{p'}{q'}$, $\rho : \delta\su\gamma'$,
  $\pi : \gamma'\otimes
  p$, and $\chi : \gamma'\otimes q$, then define
  \[\prem J\rho\pi\chi = J'|\pi'|\chi'\;\;\mbox{where}\;\;
    J'\;\mbox{is a $\gamma',\gamma$-judgment,}\;
    \pi' : \gamma'\otimes p'\;\mbox{and}\; \chi':\gamma'\otimes q'
  \]
  whose computation proceeds as follows:
  \[\begin{array}{l@{\:}lcl@{\,|\,}l@{\,|\,}l}
      \prem{(\ty{\xi\theta})&}\rho\pi\chi & =
      & \ty{(\sel\xi\chi)(\ith_{\gamma'},\theta)}
      & \sel\xi\chi & \chi-\xi \\
      \prem{(\chk T{\xi\theta})&}\rho\pi\chi & =
      & \chk{T\ssc\rho\pi}{(\sel\xi\chi)(\ith_{\gamma'},\theta)}
      & \sel\xi\chi & \chi-\xi \\
      \prem{(S=T)&}\rho\pi\chi & =
      & S\ssc\rho\pi=T\ssc\rho\pi & \Pa{} & \chi \\
      \prem{(\univ T)&}\rho\pi\chi & =
      & \univ{T\ssc\rho\pi} & \Pa{} & \chi \\
      \prem{(\cxe xS J)&}\rho\pi\chi & =
      & \cxe x{S\ssc\rho\pi}J' & \la x\pi' & \chi'\\
      \multicolumn{6}{l}{\quad  \mbox{where} \;\;J'|\pi'|\chi' = \prem J\rho\pi\chi}
  \end{array}\]
\end{definition}

That is, the schematic variables are everywhere instantiated using the
trusted environment, and the validation premises move the validated
term from the untrusted environment to the freshly trusted
environment. We may now iterate across a formal premise sequence.

\begin{definition}[actual premise sequence]
  If $\vec J : \Prems{\ssc\delta p}q{p'}$,  $\rho : \delta\su\gamma'$,
  $\pi : \gamma'\otimes p$, and $\chi : \gamma'\otimes q$, then define
  \[\prems{\vec J}\rho\pi\chi = \vec{J'}|\pi'|\chi'\;\;\mbox{where}\;\;
    \vec{J'}\;\mbox{is a list of $\gamma',\gamma$-judgments,}\;\mbox{and}\;
    \pi' : \gamma'\otimes p'
  \]
  whose computation proceeds as follows.
  \[\begin{array}{l@{\:}lcl@{\,|\,}l}
      \prems{\emp&}\rho\pi\chi & = & \emp & \pi \\
      \prems{J\;\vec J&}\rho{\pi_0}{\chi_0} & = & J'\;\vec{J'} & \pi_2
      \\
      \multicolumn{5}{l}{\quad \mbox{where}\quad
      J'|\pi'|\chi_1 = \prem J\rho{\pi_0}{\chi_0}\quad
      \vec{J'}|\pi_2 = \prems{\vec J}\rho{\cn{\pi_0}{\pi'}}{\chi_1}
      }
  \end{array}\]
\end{definition}

We can now say which actual derivations our formal rules are willing
to yield.

\begin{definition}[actual rules\label{def:actualrules}]
  \[\begin{array}{ccl}
      \textbf{formal} & \textbf{actual} & \medskip \\
      \Rule{\vec J}
      {\ty q}
      & \Rule{\vec{J'}}
        {\ty{\chi q}}
      & \vec{J'}|\pi' = \prems{\vec J}\emp{\Pa{}}\chi \\
      \Rule{\vec J}
      {\chk pq}
      & \Rule{\vec{J'}}
        {\chk{\pi p}{\chi q}}
                                        & \vec{J'}|\pi' = \prems{\vec J}\emp\pi\chi \\
      \Rule{\syn ep \quad \vec J}
      {\syn{e\:q}S}
                      & \Rule{\syn e{\pi p}\quad \vec{J'}}
                        {\syn{e\:(\chi q)}{S\ssc e{\pi'}}}
                                        & \vec{J'}|\pi' = \prems{\vec J}e\pi\chi \\
      \Rule{\vec J}
      {\univ p}
      & \Rule{\vec{J'}}
        {\univ {\pi p}}
      & \vec{J'}|\pi' = \prems{\vec J}\emp\pi{\Pa{}} \\
  \end{array} \]
\end{definition}

Only one component is now missing. Let us fill it in, forthwith!

\begin{definition}[formal and actual $\beta$-reduction rules\label{def:beta}]
  A formal $\beta$-rule is given by a quintuple comprising three
  patterns and two expressions
  \[
    \ra{p_0}{p_1}\:p_2 \leadsto_\beta \ra tT
  \]
  where $p_i:\Pat\emp$ and $t,T:\Ex{\ssc\emp\cn{\cn{p_0}{p_1}}{p_2}}\lib\cons\emp$.
  The actual $\beta$-rule thus represented is
  \[
    \ra{\pi_0p_0}{\pi_1p_1}\:(\pi_2p_2) \leadsto_\beta
    \ra{t\ssc\emp{\cn{\cn{\pi_0}{\pi_1}}{\pi_2}}}{T\ssc\emp{\cn{\cn{\pi_0}{\pi_1}}{\pi_2}}}
    \]
\end{definition}


\subsection{What is a bidirectional type theory?}

We are, at last, in a position to specify what is meant by `specifying
a bidirectional type theory', in the sense of this paper.

\begin{definition}[bidirectional type theory]
  A bidirectional type theory is presented by giving the sets of formal
  rules for type construction, type checking, elimination and universe
  checking, as specified in Definitions \ref{def:tycon} through
  \ref{def:univ}, together with a set of formal $\beta$-reduction rules as
  specified in Definition \ref{def:beta}. The rules of the type theory thus presented
  are as follows: \textsc{extend} and \textsc{lookup} (Definition
  \ref{def:cxrules});
  \textsc{var} (Definition \ref{def:var});
  \textsc{thunk}, \textsc{universe} and \textsc{radical} (Definition
  \ref{def:cod});
  \textsc{pre} and \textsc{post} (Definition \ref{def:prepost});
  the actual translations of the formal rules given in Definition
  \ref{def:actualrules}.
  Its reduction rules are given by the contextual closure rules in
  Definition \ref{def:reduction} together with the actual translation
  of the formal $\beta$-rules given by Definition \ref{def:beta}.
\end{definition}


\section{The monoidal category of thinnings acts functorially on syntax\label{sec:thincat}}

Thinnings form a well known category: the \emph{semi-simplicial} category, often notated $\Delta_+$.


\begin{lemma}[category of thinnings]
  We have the usual categorical laws:
  \[
    \ith\theta = \theta = \theta\ith \qquad
    (\theta\phi)\psi = \theta(\phi\psi)
    \]
\end{lemma}
\begin{proof}
  Functional induction on the (graph of) composition readily establishes these results.
\end{proof}

\begin{lemma}[monoidal structure of thinnings]
  Concatenation of bit vectors, $\theta,\phi$, induces monoidal
  structure on $\Delta_+$ with respect to the monoid of concatenation
  on scopes. That is, if $\theta:\gamma\thin\delta$ and
  $\phi:\gamma'\thin\delta'$, then $\theta\otimes\phi :
  \gamma,\gamma'\thin\delta,\delta'$
  is given by $\theta,\phi$, satisfying
  \[
    \ith\otimes\ith = \ith \qquad
    (\theta\theta')\otimes(\phi\phi') =
     (\theta\otimes\phi)(\theta'\otimes\phi')
   \]
   such that
   \[
     \ith\otimes\theta = \theta = \theta\otimes\ith\qquad
     (\theta_0\otimes\theta_1)\otimes\theta_2 = \theta_0\otimes(\theta_1\otimes\theta_2)
   \]
\end{lemma}
\begin{proof}
  These equations are readily shown by functional induction on
  concatenation of thinnings.
\end{proof}

\begin{lemma}[functoriality of scope extension]
  The actions $\cdot,x$ on scopes and $\cdot\otimes1$ on thinnings is functorial:
  \[
    \ith_\gamma,1 = \ith_{\gamma,x}\qquad \theta\phi,1 = (\theta,1)(\phi,1)
    \]
\end{lemma}
\begin{proof}
  This holds by definition.
\end{proof}

\begin{remark}[thinnings as an integer monoid]
  In another life, I teach undergraduates about computer hardware. Consequently, I recognize the identity thinning as the two's complement representation of $-1$.
  Effectively, we may regard the integers as the infinite right-to-left bit vectors which eventually stabilise as all 0 (for non-negative integers) or all 1 (for negative integers). Thinning composition induces a monoid on the integers whose neutral element is $-1$. The details are left to the curious reader.
\end{remark}

\begin{lemma}[natural selection]
  If $X$ is a set, let $X^\delta$ be the set of vectors with one
  component in $X$ for each variable in $\delta$. $X^\cdot$ extends
  to a functor from $\op{\Delta_+}$ to \Set.
  Specifically, if $\theta:\gamma\thin\delta$ and $\vec x:X^\gamma$,
  then $\sel\theta{\vec x} : X^\gamma$, defined thus:
  \[
    \sel\emp\emp=\emp \qquad
    \sel{(\theta,0)}{(\vec x,x)} = \sel\theta{\vec x} \qquad
    \sel{(\theta,1)}{(\vec x,x)} = (\sel\theta{\vec x}),x
  \]
  Moreover $\sel\theta\cdot$ is a natural transformation from
  $\cdot^\gamma$ to $\cdot^\delta$.
\end{lemma}
\begin{proof}
  We require
  \[
    \sel\ith{\vec x} = \vec x \qquad
    \sel{(\theta\phi)}{\vec x} = \sel\theta{(\sel\phi{\vec x})} \qquad
    \sel{\theta}{(\vec x f^\delta)} = (\sel\theta{\vec x})f^\gamma
  \]
  where postfix $\cdot f^\cdot$ is the pointwise mapping of $f:X\to Y$ across a vector.
  All are readily established by functional induction.
\end{proof}

Selection by our injections allows us to take prefixes and suffixes of
vectors. In particular, we can cut a vector in two, then stick it back
together.

\begin{lemma}[left-to-right concatenation]
  If $\vec x : X^{\gamma,\delta}$, then
  \[
    \sel{(\thinl\gamma\delta)}{\vec x}, \sel{(\thinr\gamma\delta)}{\vec x}
    = \vec x
  \]
\end{lemma}
\begin{proof}
  This follows by functional induction on concatenation of scopes.
\end{proof}


\begin{lemma}[functoriality of the thinning action]
 The thinning action extends $\Tm ld\cdot$ to a functor from $\Delta_+$ to $\Set$.
\end{lemma}
\begin{proof}
  We must show that $m\ith = m$ and that $m(\theta\phi) = (m\theta)\phi$ for any term $m$ in any quadrant.
  This is established straightforwardly by induction on $m$, relying on the functoriality of scope extension
  to pass under an abstraction.
\end{proof}

Before we leave the category of thinnings, let me establish two more properties which
will prove crucial to my treatment of \emph{patterns}.

\newcommand{\pb}[2]{#1 \sqcap #2}
\newcommand{\cp}[2]{#1 \sqcup #2}
\begin{lemma}[$\Delta_+$ has pullbacks]
  If $\theta_i : \gamma_i\thin\gamma$ for $i=0,1$, then there exists a
  scope $\delta$, a thinning $\theta : \delta\thin\gamma$, and
  thinnings $\phi_i : \delta\thin\gamma_i$ such that
  $\phi_i\theta_i = \theta$  for $i=0,1$ with the universal property that
  for any $\phi:\delta'\thin\gamma$ and $\phi'_i : \delta'\thin\gamma_i$ such that
  $\phi=\phi'_0\theta_0$, we have some $\theta' : \delta'\thin\delta$
  such that
  $\phi=\theta'\theta$ and
  $\phi'_i = \theta'\phi_i$ for $i=0,1$. We define
  $\pb{\theta_0}{\theta_1} = \phi_0 | \theta | \phi_1$,
  leaving $\delta$ implicit.
\end{lemma}
\begin{proof}
  We shall see that $\theta$ is the pointwise conjunction of the $\theta_i$,
  which needs must embed in both $\gamma_i$. More formally, we compute pullbacks thus:
  \[\begin{array}{c@{\;\;=\;\;}r@{|}c@{|}ll}
    \pb{\emp}{\emp} & \emp&\emp&\emp & \\
    \pb{(\theta_0,0)}{(\theta_1,0)} & \phi_0&\theta,0&\phi_1
      & \mbox{where}\;\phi_0|\theta|\phi_1 = \pb{\theta_0}{\theta_1} \\                                     
    \pb{(\theta_0,0)}{(\theta_1,1)} & \phi_0&\theta,0&\phi_1,0
      & \mbox{where}\;\phi_0|\theta|\phi_1 = \pb{\theta_0}{\theta_1} \\
    \pb{(\theta_0,1)}{(\theta_1,0)} & \phi_0,0&\theta,0&\phi_1
      & \mbox{where}\;\phi_0|\theta|\phi_1 = \pb{\theta_0}{\theta_1} \\
    \pb{(\theta_0,1)}{(\theta_1,1)} & \phi_0,1&\theta,1&\phi_1,1
      & \mbox{where}\;\phi_0|\theta|\phi_1 = \pb{\theta_0}{\theta_1} \\
    \end{array}\]
  The universal property holds by straightforward induction on the call graph
  of $\pb\cdot\cdot$.
\end{proof}

\begin{remark}[pullback by selection]
  If we view $\theta_i : \gamma_i\thin\gamma$ for $i=0,1$ as bit
  vectors of length $\gamma$, it
  becomes reasonable to compute $\sel{\theta_0}{\theta_1} :
  \delta\thin\gamma_0$
  and $\sel{\theta_1}{\theta_0} : \delta\thin\gamma_1$. These are,
  respectively, $\phi_0$ and $\phi_1$ whenever
  $\pb{\theta_0}{\theta_1} = \phi_0 | \theta | \phi_1$.
\end{remark}

\begin{lemma}[$\Delta_+/\gamma$ has coproducts]
  If $\theta_i : \gamma_i\thin\gamma$ for $i=0,1$, then there exists a
  scope $\delta$, a thinning $\theta : \delta\thin\gamma$, and
  thinnings $\phi_i : \gamma_i\thin\delta$ such that
  $\phi_i\theta = \theta_i$  for $i=0,1$ with the universal property that
  for any $\phi:\delta'\thin\gamma$ and $\phi'_i : \gamma_i\thin\delta'$ such that
  $\theta_i = \phi'_i\phi$, we have some $\theta' :
  \delta\thin\delta'$ such that
  $\theta=\theta'\phi$ and
  $\phi'_i = \theta'\phi_i$ for $i=0,1$. We define
  $\cp{\theta_0}{\theta_1} = \phi_0 | \theta | \phi_1$,
  leaving $\delta$ implicit.
\end{lemma}
\begin{proof}
  We shall see that $\theta$ is the pointwise disjunction of the $\theta_i$,
  which needs must both $\gamma_i$ embed in. More formally, we compute
  coproducts in the slice thus:
  \[\begin{array}{c@{\;\;=\;\;}r@{|}c@{|}ll}
    \cp{\emp}{\emp} & \emp&\emp&\emp & \\
    \cp{(\theta_0,0)}{(\theta_1,0)} & \phi_0&\theta,0&\phi_1
      & \mbox{where}\;\phi_0|\theta|\phi_1 = \cp{\theta_0}{\theta_1} \\                                     
    \cp{(\theta_0,0)}{(\theta_1,1)} & \phi_0,0&\theta,1&\phi_1,1
      & \mbox{where}\;\phi_0|\theta|\phi_1 = \cp{\theta_0}{\theta_1} \\
    \cp{(\theta_0,1)}{(\theta_1,0)} & \phi_0,1&\theta,1&\phi_1,0
      & \mbox{where}\;\phi_0|\theta|\phi_1 = \cp{\theta_0}{\theta_1} \\
    \cp{(\theta_0,1)}{(\theta_1,1)} & \phi_0,1&\theta,1&\phi_1,1
      & \mbox{where}\;\phi_0|\theta|\phi_1 = \cp{\theta_0}{\theta_1} \\
    \end{array}\]
  The universal property holds by straightforward induction on the call graph
  of $\cp\cdot\cdot$.
\end{proof}

A direct consequence of the latter is that we can compute the
\emph{support} of a term --- the smallest scope in which it is
represented.
\newcommand{\supp}[1]{\lfloor #1 \rfloor}
\begin{lemma}[support]
  For any $t:\Tm ld\gamma$, there exist $\theta:\delta\thin\gamma$ and
  $s:\Tm ld\delta$ such that $t = s\theta$, with the universal
  property that whenever $t=s'\theta'$, there is some $\phi$ such that
  $\theta=\phi\theta'$. We may define $\supp t = s|\theta$
\end{lemma}
\begin{proof}
  Let us consider four cases. Firstly, the support of an atom is
  trivial.
  \[
    \supp a = a|\eth
  \]
  Secondly, for pairing (or radicals, or
  eliminations), we compute the pointwise disjunction of the supports of the components.
  \[
    \supp{\cn st} = \cn{s'\phi_0}{t'\phi_1}|\theta\;\mbox{where}\quad
    \;s',\theta_0=\supp s\; t',\theta_t=\supp t\;
    =\supp\phi_0|\theta|\phi_1 = \cp{\theta_0}{\theta_1} \\
  \]
  Thirdly, variables have singleton support.
  \[
    \supp x = 1|x
  \]
  Fourthly, the support of an abstraction requires us to separate the
  bound variable from the free variables.
  \[
    \supp{\la xt} = \la xt'(\ith\otimes b)|\theta\;\mbox{where}\supp
    t=t'|\theta\otimes b
  \]
  We may check that, indeed,
  $(\la xt'(\ith\otimes b))\theta
  = \la x(t'(\ith\otimes b)(\theta\otimes 1))
  = \la x(t'(\theta\otimes b)) = \la xt$.
  The universal property of the support follows from that of $\cp\cdot\cdot$.
\end{proof}


\section{The monoidal category of substitutions acts functorially on syntax\label{sec:sbstcat}}


\begin{definition}[concatenation of substitutions]
  If $\sigma_i:\gamma_i\su\delta$ for $i=0,1$, then the vector
  concatentation is a substitution from the concatenated source scopes
  to the shared target scope.
  \[\sigma_0,\sigma_1 : \gamma_0,\gamma_1\su\delta
    \]
\end{definition}

Consequently, unlike with thinnings, concatenation is not quite enough
to induce monoidal structure, and has the wrong type to be the tensor.
We shall need to fix up the target scopes with some thinnings.

\begin{definition}[tensor of substitutions, weakening]
  If $\sigma_i : \gamma_i\su\delta_i$ for $i=0,1$, we may take
  \[
    \sigma_0\otimes\sigma_1 : \gamma_0,\gamma_1 \su \delta_0,\delta_1
    \quad
    \sigma_0\otimes\sigma_1 = \sigma_0(\thinl{\delta_0}{\delta_1}),\sigma_1(\thinr{\delta_0}{\delta_1})
  \]
  In particular, if $\sigma : \gamma\su\delta$, then its weakening,
  $\sigma\otimes1 : \gamma,x\su\delta,x$,
  is given by
  \[\sigma\otimes1 = \wka\sigma,x  \quad\mbox{where}\quad \wka\sigma = \sigma\wk
     \quad\mbox{where}\quad y(\sigma;\theta) = (y\sigma)\theta
  \]
  That is, $\sigma\theta$ denotes pointwise action of $\theta$ on terms in $\sigma$.
  We may define $\sigma,\gamma' : \gamma,\gamma' \su \delta,\gamma'$ by iterating this construct
  over $\gamma'$
\end{definition}


\begin{lemma}[action of compositions]
  All four compositions of thinnings and substitions act on terms as the sequence of the
  component actions.
  \[
    t(\theta\phi) = (t\theta)\phi \quad\;
    t(\sel\theta\sigma) = (t\theta)\sigma \quad\;
    t(\rho\phi) = (t\rho)\phi \quad\;
    t(\rho\sigma) = (t\rho)\sigma
  \]
  Accordingly, it is unambiguous to abbreviate $\sel\theta\sigma$ by $\theta\sigma$ for
  selection from a vector which happens to be a substitution.
\end{lemma}
\begin{proof}
  Structural induction on terms establishes this property. To pass under binders, we must
  establish that weakening distributes $(\cdot\cdot)\otimes1 = (\cdot\otimes1)(\cdot\otimes1)$, which follows from
  the fact that
  \[
    \wka t(\cdot\otimes1) = t\wka\cdot
  \]
  for both thinnings and substitutions.
\end{proof}

To complete the components for the category of substitutions, we must have the identity.

\begin{lemma}[thinning identity]
  If $\theta:\gamma\thin\delta$, then
  \[
    \theta\isu_\delta = \isu_\gamma\theta : \gamma\su\delta
    \]
\end{lemma}

\begin{lemma}[monoidal category of substitutions]
  Substitutions $\sigma : \gamma\su\delta$ are the arrows of a
  monoidal category with identity $\isu$,
  composition by juxtaposition
  and tensor $\otimes$. Moreover, scope extension, $\cdot,x$ is an endofunctor acting
  on arrows as $\cdot\otimes1$, and action on terms makes $\Tm\lib d\cdot$ a functor
  from substitutions to \Set.
\end{lemma}
\begin{proof}
  The category and functor laws collate the above results about identity and composition
  of thinnings and substitutions. The monoidal properties of $\otimes$
  follow by plumbing with thinnings.
\end{proof}

\begin{lemma}[monoidal functor from thinnings to substitutions]
  Composition $\cdot\isu = \isu\cdot$ is the action on arrows of an identity-on-objects
  monoidal functor from thinnings to substitutions.
\end{lemma}
\begin{proof}
  We already have that $\ith\isu = \isu$. We may readily see that
  \[(\theta\isu)(\phi\isu)=\isu(\theta(\phi\isu))=(\theta(\phi\isu))
    = (\theta\phi)\isu \]
  It takes a little more work to establish that when
  $\theta_i:\gamma_i\thin\delta_i$
  \[\begin{array}{lclcl}
    (\theta_0\isu)\otimes(\theta_1\isu) & = &
      (\isu\theta_0)\otimes(\isu\theta_1)\\
    &= &
        \isu\theta_0(\thinl{\delta_0}{\delta_1}),\isu\theta_1(\thinr{\delta_0}{\delta_1}) \\
    &= &
    \isu(\thinl{\delta_0}{\delta_1})(\theta_0\otimes\theta_1), 
    \isu(\thinr{\delta_0}{\delta_1})(\theta_0\otimes\theta_1) \\
    &= &
    ((\thinl{\delta_0}{\delta_1})\isu,(\thinr{\delta_0}{\delta_1})\isu)(\theta_0\otimes\theta_1) \\
    &= &
    \isu (\theta_0\otimes\theta_1)
    &=&(\theta_0\otimes\theta_1)\isu
    \end{array}\]
\end{proof}


\section{Orphaned material}

Such are the patterns for constructions. The only patterns we ever need for
computations take the form $\trg\:p$.

Every pattern has a \emph{domain}, which is a left-nested sequence of
scopes: $\delta_{n-1},\ldots,\delta_0$. Domains are to schematic variables
as scopes are to term variables, and I refer to them by the
metavariables $\omega$ and $\chi$. All of our machinery related to thinnings works
perfectly well for domains, embedding one sequence in another by inserting
zero or more new entries. Formally, then, we may select a schematic variable
from a domain by giving a singleton thinning, $\xi : \gamma \thin \omega$.

\newcommand{\dom}[1]{\textbf{dom}(#1)}
\newcommand{\src}[1]{\textbf{src}(#1)}
\begin{definition}[domain of a pattern]
  The \emph{domain} of a pattern is computed as follows:
  \[\begin{array}{lcl}
      \dom a & = & \emp \\
      \dom{(p.q)} & = & \dom{p},\dom{q} \\
      \dom{\la xq} & = & \dom{q} \\
      \dom{\theta} & = & \src{\theta}
    \end{array}\]
  where $\omega,\omega'$ is concatenation of domains and $\src{\theta} = \delta$ whenever
  $\theta : \delta\thin\gamma$.
  We may write $\dom{\vec p}$ for the concatenation of the domains of a sequence
  of patterns.
\end{definition}

Let us now say what are the expressions, relative to a domain of schematic variables and
a scope of term variables.

\newcommand{\Exp}[2]{\textbf{Exp}(#1,#2)}
\newcommand{\Expo}[2]{\textbf{Exp}^\trg(#1,#2)}
\begin{definition}[$\omega$-$\gamma$-expression]
  The expressions $\Exp\omega\gamma$ are liberal constructions in the full syntax of
  terms, augmented by the liberal constructions $\xi/\sigma$, where
  $\xi : \delta \thin \omega$ and $\rho : \delta\su\gamma$.
  Expressions in the synthesis rule for an elimination may also contain the
  essential computation, $\trg$, standing for the target. These we classify
  as $\Expo\omega\gamma$.
\end{definition}

Informally, just as we write names at the binding sites of schematic variables
in patterns, we refer to them by names instead of some $\xi$. We may omit the identity
substitution $/\isu$, hence $S$ not $S/\emp$ and $T$ not $T/x$ in, e.g., the example
rule for checking an abstraction.
The binding site of a schematic variable determines its scope, so we
need not say which variables are being substituted.

It is straightforward to extend the action of thinnings $\cdot\theta$ and substitutions
$\cdot\sigma$ from terms to expressions as we can compute the compositions $\rho;\theta$
and $\rho;\sigma$.

By inspection, our example theory conforms to Dogma \ref{dogma:mode}, using
patterns $\U$, $\PI SxT$, $\la t$ and $\trg\:s$ where only patterns are permitted,
and in expression positions, only schematic variables and $T/(s:S)$. We have
already excluded the mention of free term variables from our rules, but there
is still the matter of which schematic variables we may use where. Let us now
be entirely precise.


\subsection{What is the domain?}

The following dogma specifies the permitted usage of schematic variables in the
premises and conclusion of a configurable rule.

\begin{dogma}[clockwise flow]
  Let a rule take the form
  \[\Rule{J_1\quad\ldots J_n}
         {J'}
  \]
  Let $\vec p_i$ be the sequence of patterns in the outputs of $J_i$ and $\vec p'$ the sequence of
  patterns in the \emph{inputs} of $J'$. Let $q$ be the subject of $J'$ if it has one and $()$
  otherwise.
  We may compute sequences of domains
  $\omega_1,\ldots,\omega_{n+1}$ of \emph{trusted} schematic variables
  and $\chi_0,\ldots,\chi_1$ of \emph{untrusted} schematic variables. We shall require
  the expressions in the inputs of $J_i$ to have domain $\omega_i$ and the subject of $J_i$ to
  be chosen from $\chi_i$. The expressions in the outputs of $J'$ must have domain $\omega_{n+1}$.
  We require that $\chi_{n+1}=\emp$. The computation proceeds as follows:
  \begin{itemize}
  \item initially, $\omega_1 = \dom{\vec p'}$ and $\chi_1=\dom q$;
  \item if $J_i$ has no subject, then $\omega_{i+1}=\omega_i,\dom{\vec p_i}$ and $\chi_{i+1}=\chi_i$;
  \item if $J_i$ has context extensions yielding scope $\gamma$ and a subject, then that
    subject must be specified by a pair $\xi\theta$, such that $\xi : \delta\thin\chi_i$
    and $\theta : \delta\thin\gamma$; that being the case,
    $\omega_{i+1}=\omega_i,\dom{\vec p_i},\delta$
    and $\chi_{i+1} = \chi_i-\xi$, i.e., $\chi_i$ with $\xi$ removed.
  \end{itemize}
\end{dogma}

What is going on? The \emph{trusted} $\omega_i$ grow as we work our way
clockwise around the rule, starting with the schematic variables which come from the conclusion's
inputs: we trust them in the sense that we know the precondition which must hold for them.
The \emph{untrusted} $\chi_i$ come from the subject's conclusion
and shrink to nothing as we work our way along the premises. Each premise
grows the trusted domain by the schematic variables of its outputs (trusted because
of postconditions) and moves its subject, if it has one, out of the untrusted domain
and into the trusted: the premise states the trust which has been established.
The thinning in a premise subject ensures that the term variables in
scope for that schematic variable map to term variables locally bound in the
context: informally, we achieve this by artful name capture, as in the
premise $\cxe xS \chk Tt$ of the rule for $\la xt$.

Let me emphasize a key consequence of Dogma \ref{dogma:clockwise}:
subjects of premises come \emph{only} from the subject of a
conclusion; a rule establishes trust in \emph{every} part of its
conclusion's subject; trust, once established, is \emph{never}
revisited.

We may readily check that our example rules all conform to Dogma \ref{dogma:clockwise}.


\subsection{How to apply a rule}

\newcommand{\mto}{\rightharpoonup}

Dogma \ref{dogma:clockwise} makes it intuitive that rules are applied
to actual terms by matching patterns to compute environments and
instantiating expressions with environments. Let us make this intuition precise.

\newcommand{\Env}[1]{\textbf{Env}(#1)}
\begin{definition}[$\omega$-environment]
  A given domain $\omega$ determines a notion of environment, $\Env\omega$.
  We may refer to such environments by metavariable $\Omega$, and define them inductively as follows:
  \[
    \Axiom{\emp : \Env\emp} \qquad
    \Rule{\Omega : \Env\omega\quad t : \Tm\lib\cons\delta}
         {\Omega,t : \Env{\omega,\delta}}
  \]
\end{definition}
Again, we may use a comma to concatenate whole environments, as well as to extend them with a
single term.

Pattern matching rests on dependency checking, known colloquially as \emph{thickening},
as it is the partial inverse of thinning.
\begin{definition}[thickening]
  If $\theta : \delta\thin\gamma$ then $\theta\cdot : \Tm ld\gamma \mto \Tm ld\delta$,
  specified by the assertion
  \[\theta t = s \;\Leftrightarrow\; t = s\theta
  \]
  We may compute $\theta t$ by a failure-propagating structural recursion on $t$. It suffices
  to consider the cases for abstraction and variables, as all others proceed componentwise.
  Straightforwardly, $\theta\grp{\la x t} = \la x\grp{(\theta1)t}$. For variables, recall
  that the variable we write as $x$ is really a singleton thinning $\phi : x\thin\gamma$:
  let $(\theta',\phi') = \pb\theta\phi$ and return $\theta'$ if $\phi'=\ith$, as by
  the definition of a pullback, we have $\theta';\theta = \ith;\phi = \phi$.
\end{definition}

We may thus characterize pattern matching as a partial function yielding environments.

\newcommand{\match}[2]{\textbf{match}(#1,#2)}
\begin{definition}[pattern matching]
  \[\match\cdot\cdot : (p : \Pat{\gamma})\times\Tm\lib\cons\gamma
    \mto \Env{\dom p}\]
  \[\begin{array}{l@{\;=\;}l}
    \match aa & \emp \\
    \match{(p.q)}{(s.t)} & \match ps, \match qt \\
    \match{\la x q}{\la x t} & \match qt \\
    \match\theta t & \theta t
  \end{array}\]
\end{definition}

Remember, though, that the patterns in our locally presented rules scope only over the
\emph{locally} bound variables, by Dogma \ref{dogma:free}. We shall need to match
terms which contain the free variables that the rules are not allowed to talk about.

\begin{definition}[shifting of domains and patterns]
  Let $\gamma,\omega$ be the domain obtained by mapping every scope $\delta$ in $\omega$
  to the concatenation $\gamma,\delta$. Let $\gamma,p$ be the pattern obtained by
  mapping every placeholder $\theta$ in $p$ to the concatenation $\ith_\gamma,\theta$.
  For the latter, we obtain
    \[\gamma,\cdot : \Pat{\gamma'} \to \Pat{\gamma,\gamma'} \]
\end{definition}

\begin{lemma}[coherence of shifting]
  \[\dom{\gamma,p} = \gamma,\dom p
  \]
\end{lemma}
\begin{proof}
  Proceed by structural induction on $p$, using the fact that mapping commutes with
  concatenation.
\end{proof}

If $\gamma$ is the scope of free variables and $\gamma'$ the scope of variables
locally bound in a rule, we may take a pattern $p : \Pat{\gamma'}$ and
a term $t : \Tm\lib\cons{\gamma,\gamma'}$,
and compute $\match{(\gamma,p)}t$, obtaining, if we are lucky, an $\Env{\gamma,\dom p}$.

What happens to expressions? We should be able to instantiate an expression
$t : \Exp\omega\gamma$ with an environment $\Omega : \Env{\gamma,\omega}$, and obtain a
$t\Omega : \Tm{\textbf{liberal}}{\textbf{construction}}{\gamma}$. We shall need a little
room to manoeuvre before we begin.

\begin{lemma}[functoriality of environments]
  We may extend $\Env{\cdot,\omega}$ to functors from thinnings or substitutions
  to \Set.
\end{lemma}
\begin{proof}
  Suppose $\Omega : \Env{\gamma,\omega}$, $\theta : \gamma\thin\gamma'$ and
  $\sigma : \gamma\su\gamma'$.
  We define $\Omega\theta$ and $\Omega\sigma$ to act pointwise on the terms
  $t$ in $\Omega$ over $\gamma,\delta$ for each $\delta$ in $\omega$.
  The image of $t$ under $\cdot\theta$ is $t(\theta\delta)$, while for
  $\cdot\sigma$, we obtain $t(\sigma\delta)$. The functor laws hold because
  terms are also functors from thinnings and substitutions, and the iterated
  weakening $\cdot\delta$ is an endofunctor on both thinnings and substitutions.
\end{proof}

With this in place, we may now push environments under binders.

\begin{definition}[expression instantiation]
  \[\Rule{t : \Exp\omega\gamma\quad \Omega : \Env{\gamma,\omega}}
         {t\Omega :  \Tm{\textbf{liberal}}{\textbf{construction}}{\gamma}}
  \]
  The operation proceeds by structural recursion on $t$. We propagate $\Omega$
  componentwise, except when we encounter an abstraction or a schematic variable:
  \begin{itemize}
  \item $\grp{\la x t}\Omega = \la x \grp{t\hat\Omega}$
  \item $\grp{\xi/\rho}\Omega = t(\isu_\gamma,\rho)$ where
    $\xi : \delta\thin\omega$, thence
    $t : \Tm\lib\cons{(\gamma,\delta)}$ is the
    projection at $\xi$ from $\Omega$.
  \end{itemize}
\end{definition}

That is to say, instantiating a schematic variable requires substitution of the variables
abstracted by the corresponding term in the environment.



$\U\U\U$



\begin{definition}[thinning judgements]
  If $J$ is a $\gamma$-judgement and $\theta : \gamma\thin\delta$, then $J\theta$
  is the $\delta$-judgement given by the action of $\theta$ distributed structurally
  to all the places of $J$. I give only the case for context extension
  \[
    \grp{\cxe xS J}\theta = \cxe x{S\theta}{J\grp{\theta1}}
  \]
  as the rest just propagate $\theta$ unchanged.
\end{definition}

The only variables which appear in the rest of the rules are \emph{bound},
either by the abstraction construct of the syntax, or by the context extension
judgement form, and thus  It will take a little
more clarity about the structure of rules to achieve this outcome for substitution,
but let us deal with the case for thinnings now.

\begin{definition}[contextual thinning]
  If $\theta:\gamma\thin\delta$, $\Gamma$ is a $\gamma$-context and $\Delta$ is a $\delta$-context
  then $\Gamma\thin_\theta\Delta$ asserts that $\theta$ extends from scopes to contexts and holds as
  follows.
  \[
    \Axiom{\emp\thin_\emp\emp} \qquad
    \Rule{\Gamma\thin_\theta\Delta}
    {\Gamma,x\!:\!S\thin_{\theta1}\Delta,x\!:\!S\theta} \qquad
    \Rule{\Gamma\thin_\theta\Delta\quad \Delta\vdash \ty T}
      {\Gamma\thin_{\theta0}\Delta,y\!:\!T}
    \]
\end{definition}

\begin{lemma}[stability under thinning]
  The following is admissible
  \[\Rule{\Gamma\thin_\theta\Delta\quad \Gamma\vdash J}
         {\Delta\vdash J\theta}
       \]
  Moreover, if $\Gamma$ is valid and $\Gamma\thin_\theta\Delta$, then $\Delta$ is valid.
\end{lemma}
\begin{proof}
For type lookup judgements, we proceed by induction on the derivation of
$\Gamma\thin_\theta\Delta$ and inversion of $\Gamma\vdash\cxl xS$: we must show
$\Delta\vdash \cxl {x;\theta}{S\theta}$. There are three cases:
\begin{itemize}
\item $\Gamma,x\!:\!S\thin_{\theta1}\Delta,x\!:\!S\theta$ and
  $\Gamma,\cxe xS \cxl x{\wka S}$ by \textsc{top}. Invoke \textsc{top}.
\item $\Gamma,y\!:\!T\thin_{\theta1}\Delta,y\!:\!T\theta$ and
  $\Gamma,\cxe xS \cxl x{\wka S}$ by \textsc{pop}. Invoke \textsc{pop}.
\item $\Gamma\thin_{\theta0}\Delta,y\!:\!T$. Invoke \textsc{pop}.
\end{itemize}
In each case, we rely on the fact that
\[
  \wka{S}\grp{\theta1} = S\grp{\wk;\theta1} = S\grp{\theta;\wk} = \widehat{S\theta}
\]
For the rest of the judgement forms, we proceed by induction on derivations.
For $\Gamma\thin_\theta\Delta$ and $\Gamma\vdash\cxe xS J$, we need
merely invoke the induction hypothesis for $\Gamma,x\!:\!S\thin_{\theta1}\Delta,x\!:\!S\theta$
and $\Gamma,\cxe xS J$. For the \textsc{var} rule, we have already established the
conformity of type lookup. For all the remaining rules, as yet unspecified,
Dogma \ref{dogma:free} insists that
we have deduced some $\Gamma\vdash J\eth$ from some $\Gamma\vdash J_i\eth$. Inductively,
we obtain $\Delta\vdash J_i\eth\theta$, but $\eth;\theta = \eth$, so we may invoke
the very same rule to deduce $\Delta\vdash J_i\eth$, and again, $\eth = \eth;\theta$
so we have the induction conclusion.
\end{proof}

We are nearly ready to consider how, in general, to construct the
typing rules for particular theories, but we have three more rules to
give which must be present in any theory, characterising the
relationship between types, type checking and type synthesis.

These rules all conform to \ref{dogma:free}.

The seasoned type theorist will further notice the total absence from the rules
so far of any form of \emph{computation}. Our type systems are, thus far,
entirely \emph{inert}. That much I shall remedy after we have introduced the
means of creating a redex, but for that, we shall need to check canonical constructions
and synthesize types for computations. And in order to do that, we should think further
about which typing rules make sense by construction.


\section{Ruling on rules}

\emph{in which I introduce the formal notions of `pattern' and `expression', with a
  systematic treatment of the schematic variables in rules; I also give the permitted
  shapes of rules which check constructions and rules which synthesize types for eliminations;
  I give the scoping dogma for schematic variables in rules, in particular the dogma which insists that subjects in premises must come from the subjects of the conclusion; I demonstrate stability under substitution}



\section{Questions better left unasked}

\emph{in which I investigate further the consequences of the deliberately restrictive language of patterns; the structural rules for reduction ensure that pattern matching is stable with respect to reduction; patterns have simple unification, ensuring the appropriate invertibility of rules; on syntax-directedness and its consequences; I specify the sanity clause and prove its decidability}

\section{Mind diamonds!}

\emph{in which I show how a system of syntax-directed typing rules demands a set of $\beta$-rules
  which are non-overlapping by construction, resulting in the definability of a Takahashi-style notion of `development', and thence confluence --- parallel reduction has the diamond property by construction}

In our reconstruction of Martin-L\"of's 1971 theory, the demanded $\beta$-rule is
\[
  (\la x t : \PI SxT)\:s \leadsto (? : T/(s:S))
\]
For best results, take $? = t/(s:S)$.


\section{Subject reduction by construction}

\emph{in which I demand that the $\beta$-rules be well typed in the inert fragment of a type theory,
  and show that compliance with this demand is decidable;
  I then prove that confluence is sufficient to ensure that subject reduction holds in the presence of the following}
\[
  \textsc{pre}\;
  \Rule{T\leadsto T' \quad \chk{T'}t}
  {\chk Tt}
  \qquad
  \textsc{post}\;
  \Rule{\syn eS\quad S\leadsto S'}
  {\syn eS}
\]

In our reconstruction of Martin-L\"of's 1971 theory, inertly inverting
the typing rules for the left-hand side yields
\[
  \ty S\quad \cxe xS\ty T\quad \chk Ss\quad \cxe xS\chk Tt
\]
from which we may inertly deduce that
\[
  \chk{T/(s:S)}{t/(s:S)}
\]
and hence that Martin-L\"of's 1971 theory has the subject reduction property.


\section{Discussion}

\emph{in which I review related work and progress, then set out the prospectus for
the research programme opened by this paper}


% \begin{definition}[readable notation for variables and thinnings]
% Just as de Bruijn indices are not a suitable notation for human-readable variables, so bit vectors are not a suitable notation for human-readable thinnings.
% I write $\GS\gamma{var}$ for the grammar of variables in $\gamma$ and use names from $\gamma$ as the valid forms in that grammar. I write $\GS\gamma{thinning}$
% for the grammar of thinnings whose target scope is $\gamma$, where
%  \[\begin{array}{rrll}
%      \GS\gamma{thinning} & ::= & & \mbox{--- $\ith$, includes all the variables in $\gamma$}\\
%                    &   | & \GS\gamma{var} \GS\gamma{var}^\ast  & \mbox{--- include only the listed variables}\\
%                    &   | & - & \mbox{--- $\eth$, exclude all the variables in $\gamma$}\\
%                    &   | & - \GS\gamma{var} \GS\gamma{var}^\ast & \mbox{--- exclude only the listed variables} \\
%    \end{array}\]
%\end{definition}

%It is straightforward to compute the source scope of a $\GS\gamma{thinning}$, given that we know its target scope: by a slight abuse of notation, I write
%thinnings $\ths\theta$ in places where scopes $\gamma$ are expected, meaning the source scope that $\theta$ effectively selects from some larger target scope.



% \section{Binding Lisps}
% 
% Given the mission to develop type theories in broad generality, we shall need
% to be open minded about what constucts they might offer. Let us adopt a
% simple but generic way of representing them: tasteful notations for specific
% theories can be layered on top, but for purposes of metatheory, the less
% artful we are, the better. There are many tolerable choices we might make, but my
% childhood guides me to jump in Lisp puddles, and my
% training as a graduate student warns me to be cautious about variable binding.
% 
% \begin{definition}[binding Lisp]
%   A \textbf{binding Lisp} is some grammar, $\GS\gamma{lisp}$ with at least the following constructs:
%   \[\begin{array}{rrll}
%       \GS\gamma{lisp} & ::= & \SC{atom} & \mbox{--- a sequence of alphanumeric characters} \\
%                 &   | & \Pa{\GS\gamma{lisp}\dot\GS\gamma{lisp}} & \mbox{--- a cons-cell} \\\\
%                 &   | & x \ab \GS{\gamma,x}{lisp} & \mbox{--- an abstraction} \medskip\\
%   \end{array}\]
% \end{definition}
% 
% As ever, we allow syntactic sugar to avoid an excess of dotted pairs.
% 
% \begin{definition}[binding Lisp, relaxed notation]
%   \[\begin{array}{rrll}
%       \GS\gamma{lisp} & ::= & \SC{atom} & \mbox{--- a sequence of alphanumeric characters} \\
%                 &   | & \Pa{\GS\gamma{lisp}\dot\GS\gamma{lisp}} \\
%                 &   | & \Pa{\GS\gamma{constr}} & \mbox{--- a construction in parentheses} \\
%                 &   | & x \ab \GS{\gamma,x}{lisp} & \mbox{--- an abstraction} \medskip\\
%       \GS\gamma{constr} & ::= &  & \mbox{--- nil, the empty atom}\\
%                 &   | & \D \GS\gamma{lisp} & \mbox{--- a non-nil tail} \\
%                 &   | & \GS\gamma{lisp}\; \GS\gamma{constr} & \mbox{--- cons} \\
%   \end{array}\]
% \end{definition}
% 
% Note that
% \begin{enumerate}
% \item some atoms may look like variables;
% \item $\Pa{}$ is a way to write empty atom;
% \item $\D\cdot$ and $\Pa\cdot$ cancel, so that $\Pa{\D l}$ is the same $\GS\gamma{lisp}$ object as $l$. and $\D\Pa c$ is the same $\GS\gamma{constr}$ object as $c$;
% \item this basic syntax offers no way to \emph{use} a variable.
% \end{enumerate}
% 
% Let us remedy the latter posthaste!
% 
% \begin{definition}[terms]
%   The syntax of \textbf{terms} is a binding Lisp augmented with the following additional construct
%   \[\begin{array}{rrll}
%       \GS\gamma{term} & ::= & \cdots & \mbox{--- all the binding Lisp constructs} \\
%                 &   | & \Bk{\GS\gamma{elim}} & \mbox{--- an elimination in brackets} \medskip\\
%       \GS\gamma{elim} & ::= & \GS\gamma{var} & \mbox{--- variable usage} \\
%                 &   | & \ra{\GS\gamma{term}}{\GS\gamma{term}} & \mbox{--- a radical} \\
%                 &   | & \GS\gamma{elim}\; \GS\gamma{term} & \mbox{--- an action} \\
%     \end{array}\]
% where a $\GS\gamma{var}$ is one of the variables in $\gamma$, readily relieved of its name by determining its position in $\gamma$.
% \end{definition}
% 
% The binding Lisp constructs give us the means to express the canonical forms of a theory; the eliminations allow us to express computations. The latter amount to a \emph{head} with a possibly empty sequence, or \emph{spine}, of actions attached. A head may be a variable, as bound by an abstraction, or it may be a \emph{radical} --- a term (possibly canonical) activated for computation by annotation with its type.
% 
% We shall define our type theories by saying which terms are types and which terms are \emph{checked} by those types. By contrast, and bidirectionally, we shall always be able to \emph{synthesize} types for meaningful eliminations.
% 
% To aid comprehension, I shall write atoms in $\A{teletype}$ font and variables in $\V{italics}$. However, you can always spot a variable binding by the $\ab$ to its right and a variable usage by the $\texttt{[}$ to its left.
% 
% \begin{example}[polymorphic identity function]
%   We may write $\A{Type}$ for the type of types and $(\A{Pi}\;S\;\V x\ab T)$ for a dependent function type. The type of the polymorphic identity function may thus be written
%   \[
%     \Pa{\A{Pi}\;\A{Type}\;\V X \ab \Pa{\A{Pi}\;\Bk X\;\V x\ab \Bk X}}
%   \]
%   with the polymorphic identity function being just
%   \[\V X\ab \V x \ab \Bk x
%     \]
% \end{example}
% 
% The role of atoms is to be distinct from one another. The role of variables is to connect usage with abstraction: naming them variables is an informal concession to readability.
% 
% All binding Lisps admit thinning.
% 
% \begin{definition}[thinning action]
%   If $s$ is an object in some $\GS\gamma{lisp}$ and $\theta : \gamma \thin \delta$ is a thinning, then $s\theta$ is the object in the corresponding
%   $\GS\delta{lisp}$ given by
%   \[\begin{array}{lcl}
%       \A{a}\theta & = & \A{a} \\
%       \Pa c\theta & = & \Pa{c\theta} \\
%       (x\ab t)\theta & = & x\ab(t\theta1)
%     \end{array}
%     \qquad
%     \begin{array}{lcl}                     
%       \;\theta & = & \;\\
%       (\D t)\theta & = & \D(t\theta)\\
%       (t\;c)\theta & = & (t\theta)\;(c\theta)
%     \end{array}\]
%   This action has a partial inverse, written $\theta?t$ for some $\GS\delta{lisp}$ object t, giving the $\GS\gamma{lisp}$ object $s$ such
%   that $s\theta = t$ if it exists. 
%   It remains to specify the action of thinnings on the extra constructs of specific $\GS\gamma{lisp}$s. For $\GS\gamma{term}$, we have
%   \[\begin{array}{lcl}
%       \Bk{e}\theta & = & \Bk{e\theta} \medskip \\
%       x\theta & = & x \\
%       \ra tT\theta & = & \ra{t\theta}{T\theta} \\
%       (e\;t)\theta & = & (e\theta)\;(t\theta)
%     \end{array}
%   \]
%   where, in a de Bruijn representation $x\theta = x$ computes the representation for $x$ in the target scope from the representative of
%   $x$ in the source scope.
% \end{definition}
% 
% 
% 
% \section{Meta-syntax: patterns, thinnings and expressions}
% 
% For my purposes, it is not enough to be formal about the type theory which is the object of study. I intend to establish general results about classes of type theories. I must therefore be formal about the \emph{meta}-language in which I am formal about type theories. When we write typing rules or contraction schemes, we do not write terms of our type theory, but rather \emph{formulae} containing \emph{meta}variables, which describe terms in our type theory with particular structural properties. What are those formulae?
% 
% It is conventional to generate such formulae by the free extension of the object theory with metavariables, intending all the terms which are given by instantiating the metavariables. I fear I shall be more painstaking, distinguishing two classes of formula, the \emph{patterns} which contain the binding sites of metavariables, from the \emph{expressions} which contain their use sites. The benefit of this distinction will be to allow much tighter control of information flow through the rules of the theory.
% 
% \begin{definition}[pattern]
% The syntax of \emph{patterns} is a binding Lisp augmented with the following additional construct
%   \[\begin{array}{rrll}
%       \GS\gamma{pat} & ::= & \cdots & \mbox{--- all the binding Lisp constructs} \\
%                 &   | & \Bc{\SC{mvar}\; \GS\gamma{thinning}} & \mbox{--- a metavariable binding in braces} \medskip\\
%     \end{array}\]
% and thinnings acting by
% \[
% \Bc{m\;\phi}\theta = \Bc{m\;\phi;\theta}
% \]
% \end{definition}
% 
% A metavariable binding gives a name to a metavariable, for the sake of human readability: these names should be mutually distinct. In practice, metavariable bindings may effectively be distinguished by \emph{position} and need not have names at all. As a metavariable binding may occur inside zero or more object variable abstractions, we write a \emph{thinning} to indicate upon which of those object variables a term instantiating the metavariable may depend. The surface syntax of thinnings is conveniently redundant, allowing us to specify permitted dependencies either by inclusion or by exclusion.
% 
% Note that patterns allow us to analyse only
% \begin{enumerate}
% \item the canonical form structure induced by a binding Lisp;
% \item dependency on \emph{bound} variables.
% \end{enumerate}
% That is, the pattern language is designed carefully to ask only questions whose answers are invariant under substitution of free variables.
% 
% Crucially, from every pattern, we may compute its \emph{problem} --- the sequence of name/source scope pairs $m/\delta$ given by each
% $\Bc{m\;\theta}$ contained therein.
% 
% \begin{example}[function types]
%   The pattern that describes a dependent function type is
%   \[(\A{Pi}\;\Bc S\;\V x\ab\Bc T) \qquad \mbox{or, equivalently, the explicit} \qquad (\A{Pi}\;\{S\}\;\V x\ab\{T\:x\})
%   \] whose problem is
%   \[S/;\,T/x
%     \]
%   The pattern that captures the non-dependent special case is
%   \[(\A{Pi}\;\Bc S\;\V x\ab\Bc{T - x}) \qquad \mbox{or, equivalently, the implicit} \qquad (\A{Pi}\;\{S\}\;\V x\ab\{T -\})
%   \] whose problem is
%   \[S/;\,T/
%     \]
% \end{example}
% 
% One way to solve a problem $\Omega$ is to match a $\GS\gamma{term}$ to a $\GS{\emp}{pat}$, mapping each $m/\ths{\theta}$ to a $\GS{\gamma,\ths{\theta}}{term}$, yielding
% a sequence of definitions $m(\ths{\theta})=t$. I write $\emp$ for the empty solution.
% 
% \newcommand{\ma}{\mathbin{?}}
% \begin{definition}[pattern matching]
%   If $p$ is a $\GS\delta{pattern}$ with problem $\Omega$ and $t$ a $\GS{\gamma,\delta}{term}$, then $p\ma t$ is the
%   $\Omega \Rightarrow \gamma$ solution partially defined as follows:
% \[\begin{array}{l@{}c@{}lcl}
% \A{a}&\ma&\A{a} & = & \emp \\
% \Pa{p_a\D p_d}&\ma&\Pa{t_a\D t_d} & = & p_a\ma t_a;\; p_d\ma t_d\\
% (x\ab p) &\ma& (x\ab t) & = & p \ma t \\
% \Bc{m\;\theta} &\ma& t & = & m(\ths\theta)=1,\theta\ma t
%   \end{array}\]
% Note that in the abstraction case, it is not a requirement that the names match, rather, by $\alpha$-equivalence, the names do match.
% \end{definition}
% 
% 
% Now, let us turn to the expressions.
% 
% \begin{definition}[expression]
% The syntax of expressions is a binding Lisp augmented with the following additional constructs:
%   \[\begin{array}{rrll}
%       \GS\gamma{expr} & ::= & \cdots & \mbox{--- all the binding Lisp constructs} \\
%                 &   | & \Bk{\GS\gamma{image}} & \mbox{--- as `elim' is to `term'} \\
%                 &   | & \Bc{\SC{mvar}(/\GS\gamma{image})^\ast} & \mbox{--- a metavariable binding in braces} \medskip\\
%       \GS\gamma{image} & ::= & \GS\gamma{var} & \mbox{--- variable usage} \\
%                 &   | & \ra{\GS\gamma{expr}}{\GS\gamma{expr}} & \mbox{--- a radical} \\
%                 &   | & \GS\gamma{image}\; \GS\gamma{expr} & \mbox{--- an action} \\
%     \end{array}\]
% with thinning acting structurally.
% \end{definition}
% 
% Each metavariable must be instantiated with an image for each bound variable on which they depend, as determined by the pattern which binds the metavariable.
% Expressions are the language that rules use for synthesis, making use of information gained by pattern matching. E.g., once we have matched a function type with pattern
% \[(\A{Pi}\;\Bc S\;\V x\ab\Bc T)
% \]
% and an argument with pattern
% \[
% \Bc s
% \]
% we may deliver the type of the application with the expression
% \[
% \Bc{T/\ra{\Bc s}{\Bc{S}}}
% \]
% 
% 
% \section{Judgements and Rules}
% 
% A judgement form is a crude variety of `session type', characterising the data involved as
% \emph{inputs}, \emph{subjects} or \emph{outputs}. A subject is something to be validated by
% the judgement form: not all judgements require a subject, but for every sort of thing, there
% must be a judgement form for which it is the subject. An input must have a \emph{client promise},
% being a judgement with that input as its subject, explaining what sort of validation the
% client is expected to perform in advance of seeking to derive a judgement with that input. An
% output must have a corresponding \emph{server promise}, being a judgment with that output as its subject,
% explaining what sort of validation must be ensured by whoever claims to have derived the judgement
% 
% \newcommand{\type}[1]{\texttt{type}\;#1}
% \newcommand{\chek}[2]{#1\;\texttt{:>}\;#2}
% \newcommand{\synth}[2]{#1\;\texttt{<:}\;#2}
% \newcommand{\subty}[2]{#1\;\texttt{<=}\;#2}
% \newcommand{\redu}[2]{#1\;\texttt{\raisebox{0.035in}{\texttildelow}>}\;#2}
% 
% There are four judgement forms which are considered primitive. I list them in their ascii notation, with their associated
% promises given in braces:
% \begin{enumerate}
% \item type formation
% \[\type{T_{\mbox{subj}}}
% \]
% takes a putative type as its subject;
% \item type checking
% \[\{\type{T_{\mbox{in}}\}}\qquad
% \chek{T_{\mbox{in}}}{t_{\mbox{subj}}}
% \]
% takes a putative term as its subject and checks it in a given type;
% \item type synthesis
% \[\synth{e_{\mbox{subj}}}{T_{\mbox{out}}}\qquad
% \{\type{T_{\mbox{out}}}\}
% \]
% takes an elimination as its subject and tries to find a type for it;
% \item subtyping
% \[\{\type{S_{\mbox{in}}}\quad \type{T_{\mbox{in}}}\}\qquad
% \subty{S_{\mbox{in}}}{T_{\mbox{in}}}
% \]
% takes two given types and checks whether one is subsumed by the other.
% \end{enumerate}
% 
% 
% \section{Computation}
% 
% Making no presumption of strong normalization, I adopt a \emph{small-step} notion of reduction, generated by the contextual closure of contraction schemes. Whatever other constructs and contractions we may legitimize, we shall certainly have the following:
% 
% \begin{definition}[$\upsilon$-contraction]
% \[[\ra tT] \leadsto_\upsilon t\]
% \end{definition}
% 
% A spineless radical ceases to be a radical at all. It loses its type annotation and delivers its now deactivated result into the term which is its host.
% 
% 
% 
% 
\bibliography{TypesNi}

\end{document}