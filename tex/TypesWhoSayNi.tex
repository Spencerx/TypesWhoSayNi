\documentclass{jfp1}
\bibliographystyle{jfp}
\usepackage{pig}
\ColourEpigram
\usepackage{upgreek}
\usepackage{textcomp}
\usepackage{stmaryrd}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{dogma}[theorem]{Dogma}
\newtheorem{example}[theorem]{Example}

\newcommand{\A}{\texttt}
\newcommand{\V}{\mathit}
\newcommand{\ab}{\,\texttt{->}\;}

\newcommand{\emp}{\varepsilon}
\newcommand{\G}[1]{\lfloor #1 \rfloor}
\newcommand{\Pa}[1]{\texttt{(}#1\texttt{)}}
\newcommand{\Bk}[1]{\texttt{[}#1\texttt{]}}
\newcommand{\Bc}[1]{\texttt{\string{}#1\texttt{\string}}}
\newcommand{\D}{\texttt{.}}

\newcommand{\SC}[1]{\langle\mathit{#1}\rangle}
\newcommand{\GS}[2]{\langle#1\textrm{-}\mathit{#2}\rangle}

\newcommand{\hb}{\texttt{:}}
\newcommand{\ra}[2]{\Pa{#1 \hb #2}}

\newcommand{\ths}{}

\begin{document}
\title[Journal of Functional Programming]{The types who say $\backslash$ni}
\author[C. T. McBride]{CONOR T. MCBRIDE\\
  University of Strathclyde\\
  \email{conor.mcbride@strath.ac.uk}}
\maketitle

\section{Introduction}

This paper is about my religion. It introduces a discipline for
constructing and validating bidirectional type systems, illustrated
with a nontrivial running example --- a bidirectional reconstruction
of Per Martin-L\"of's small and beautiful, but notoriously
inconsistent dependent type theory from
1971~\cite{martinloef:atheoryoftypes}. Crucially, the fact that the
system is not strongly normalizing is exploited to demonstrate
concretely that the methodology relies in no way on strong
normalization, which is perhaps peculiar given that bidirectional type
systems are often (but not here) given only for terms in
$\beta$-normal form~\cite{DBLP:journals/toplas/PierceT00}.

From the outset, it would seem prudent to manage expectations. I take
the view that types are not inherent in things, but rather imposed on
them by human design in arbitrary ways. Meaning is made, not
found. A practical consequence of this viewpoint is that we may
fix a generic syntax, simple and flexible, before giving any thought
to the design of types and the meaningful forms of computation they
justify. Every self-respecting religion pinches the parts it likes
from other religions, so I will choose \textsc{Lisp} s-expressions~\cite{MCCARTHY60}
as the generic syntax, but tighten up the treatment of variable
binding with methods of de Bruijn~\cite{deBruijn:dummies},
distinguishing variables from atoms and naming them only in the
interests of informal civility. One should not expect things
expressible in this syntax to make sense: rather we should design
ways to make sense of some of them. We get out what we put
in, so let us seek understanding of how to direct our freedom towards virtuous
outcomes.

Oft sought properties of type systems, such as `stability under
substitution' and `type preservation', are not to be had for the
proving, but rather by construction in accordance with good guidance.
The prospectus of this paper is to develop a metametatheory in which
to ensure the good metatheoretic properties of a whole class of theories.


\section{A Generic Syntax for Bidirectional Type Systems}

\newcommand{\from}{\leftarrow}
\newcommand{\la}[1]{\backslash #1\:}

For the benefit of human beings, a \emph{scope} is written as a list of identifiers $x_{n-1},\ldots x_0$, although the inhuman truth is that it is just the number $n$
which gives the length of the sequence. I refer to scopes by
metavariables $\gamma$ and $\delta$, with $\emp$ denoting the empty
scope. A \emph{variable} is an identifier $x_i$ selected from
a scope, serving as the human face of its index $i$. I specify grammars
relative to an explicit scope, $\gamma$ and write $x_i\from\gamma$ to
mean `a variable from $\gamma$'.

Unlike variables, \emph{atoms} ($a$, $A$) really are named global
constants which play the role of constructors --- their purpose is not
to stand for things but to be told apart. The symbol $()$ is
considered an atom and pronounced `nil'.

The object-level syntax is specified,
written as a subscript, and
is divided into four distinct mutually defined grammatical classes,
each with standard metavariable conventions,
arranged as four quadrants thus:
\[\begin{array}{r|l|l}
& \mbox{\bf essential} & \mbox{\bf liberal} \\
\hline
    \mbox{\bf constructions} & k_\gamma, K_\gamma & s_\gamma, t_\gamma, S_\gamma, T_\gamma \\
    & \;\begin{array}[t]{rl}
          ::= & a \\
          | & (s_\gamma.t_\gamma) \\
          | & \la xt_{\gamma,x}
        \end{array}
    & \;\begin{array}[t]{rl}
          ::= & k_\gamma \\
          | & [n_\gamma]
        \end{array}
    \\
    \hline
    \mbox{\bf computations} & n_\gamma, N_\gamma & e_\gamma, f_\gamma, E_\gamma, F_\gamma \\
    & \;\begin{array}[t]{rl}
          ::= & x\from\gamma \\
          | & e_\gamma\:s_\gamma 
        \end{array}
    & \;\begin{array}[t]{rl}
          ::= & n_\gamma \\
          | & (k_\gamma:T_\gamma) 
        \end{array}
    \\
\end{array}\]

\newcommand{\PI}[3]{(\Uppi\:#1\;\la{#2}{#3})}
The meaningfulness of \emph{constructions} will always be
\emph{checked}, relative to some prior expectation. The
\emph{essential} constructions give us the raw materials for
canonical values, and they will always be invariant under computation.
Let us adopt the \textsc{Lisp} convention that matching parentheses
preceded by a dot may be deleted, along with the dot, which is
conveniently prejudicial to right-nested, nil-terminated lists. Our
constructions will often be such lists, with an atom at the head.
For example, dependent function types in $\gamma$ will have form
\[\PI {S_\gamma}x{T_{\gamma,x}}
\]
where $\Uppi$ is an atom.

The \emph{liberal} constructions extend the canonical constructions
with \emph{thunks} --- essential computations which have not yet achieved canonical
form, either because they have not yet reduced, or because a variable
is impeding reduction.

Meanwhile, the \emph{computations} will always admit a type synthesis
process. The \emph{essential} constructions comprise variables (whose
type will be assigned by a context) and eliminations, overloading the
application syntax --- the computation $e$ to the left is being
eliminated, and its synthesizable
type will tell us how to check that the construction $s$ to its right
is a valid eliminator. These two give us the traditional forms
of \emph{neutral} term.

The \emph{liberal} computations extend the neutral computations with
\emph{radicals}, in the chemical sense, being canonical forms with a
type annotation which gives the information needed to check both the
canonical form it annotates and the eliminator it is `reacting' with.
If we exclude radicals from this syntax, we obtain the \emph{normal
  forms}. Radicals are permitted only in eliminations, and these are
the only eliminations which compute. That is, there is no computation
step which proceeds uninformed by the type at which computation is
happening.

Thunks and radicals form the connection between constructions and
computations, but you will notice that they carefully exclude one
another --- there is syntax neither for a thunked radical nor a
radical thunk. We may extend these term formers to thunk liberal
computations and radicalise liberal constructions by defining
\[
  [(k:T)] = k \qquad ([n]:T) = n
\]
with the impact of deleting exactly the type annotations which inform
no computation, either because they mark a canonical form which is
not being eliminated or because they mark a neutral form whose
elimination is necessarily stuck. Observe that, as a consequence,
\[
  [(t:T)] = t
\]
whether $t$ is canonical or not. Observe also that we may now lift
\emph{all} the term formers to act on liberal components, yielding
liberal results, for everything apart from thunks and radicals admits
liberal substructures. In particular, it becomes reasonable to
substitute a liberal computation for a variable, in either a
construction or a computation, yielding a liberal construction or
computation, respectively. I write $t/e$ for such a substitution
in a construction, when it is clear from context which variable in
$t$ is being substituted.

With this apparatus in place, we may reformulate the traditional
$\beta$-rule for functions as a rewriting rule on liberal
computations, thus:
\[
  (\la xt : \PI SxT)\:s \leadsto (t/(s:S):T/(s:S))
\]
If it so happens that $s$ is canonical, then we will be substituting a radical
for the bound variable, creating potential redexes wherever that
variable is eliminated. If it so happens
that t is canonical, then the whole reduct will be radical. The type
annotations thus mark the active computation sites and are discarded
whenever computation stops.




\section{The Category of Scopes and Thinnings}

\newcommand{\thin}{\sqsubseteq}
\newcommand{\ith}{\mathbf{1}}
\newcommand{\eth}{\mathbf{0}}

\begin{definition}[thinning]
A thinning is an order preserving embedding between scopes
\[\theta : \gamma \thin \delta
  \]
The thinnings are generated by
\[
  \Axiom{\emp:\emp\thin\emp} \qquad
  \Rule{\theta : \gamma \thin \delta}
       {\theta0 : \gamma \thin \delta,x} \qquad
  \Rule{\theta : \gamma \thin \delta}
       {\theta1 : \gamma,x \thin \delta,x} \qquad
\]
\end{definition}

That is, thinnings arise in the manner of Pascal's triangle: there are $\left(\begin{array}{@{}c@{}}m \\ n\end{array}\right)$ thinnings from $n$ to $m$,
which is not surprising, as they correspond to selections.

A thinning is given as a bit vector the length of its target scope with its source scope being the `population count', i.e., number of 1s, in the vector.
Thinnings form a well known category: the \emph{semi-simplicial} category, often notated $\Delta_+$.

\begin{definition}[identity thinning]
The identity thinning $\ith_\gamma : \gamma \thin \gamma$ is given by
\[
  \ith_\emp = \emp \qquad \ith_{\gamma,x} = \ith_\gamma1
\]
\end{definition}
Informally, we may just write $\ith$. The empty thinning, $\eth$ is generated analogously, repeating 0 to the appropriate length.

\begin{definition}[composition of thinnings]
  If $\theta:\gamma\thin \gamma'$ and $\phi:\gamma'\thin\gamma''$, then $\theta;\phi : \gamma\thin\gamma''$
  defined as follows:
  \[
    \emp;\emp = \emp \qquad
    \theta;\phi0 = (\theta;\phi)0 \qquad
    \theta b;\phi1 = (\theta;\phi)b
  \]
\end{definition}

\begin{lemma}[category of thinnings]
  We have the usual categorical laws:
  \[
    \ith;\theta = \theta = \theta;\ith \qquad
    (\theta;\phi);\psi = \theta;(\phi;\psi)
    \]
\end{lemma}
\begin{proof}
  Functional induction on the (graph of) composition readily establishes these results.
\end{proof}

\begin{remark}[thinnings as an integer monoid]
  In another life, I teach undergraduates about computer hardware. Consequently, I recognize the identity thinning as the two's complement representation of $-1$.
  Effectively, we may regard the integers as the infinite right-to-left bit vectors which eventually stabilise as all 0 (for non-negative integers) or all 1 (for negative integers). Thinning composition induces a monoid on the integers whose neutral element is $-1$. The details are left to the curious reader.
\end{remark}

Meanwhile, thinnings form a monoidal category by concatenation $\gamma,\gamma'$ of scopes and $\theta,\theta'$ of thinnings.

\begin{definition}[readable notation for variables and thinnings]
Just as de Bruijn indices are not a suitable notation for human-readable variables, so bit vectors are not a suitable notation for human-readable thinnings.
I write $\GS\gamma{var}$ for the grammar of variables in $\gamma$ and use names from $\gamma$ as the valid forms in that grammar. I write $\GS\gamma{thinning}$
for the grammar of thinnings whose target scope is $\gamma$, where
  \[\begin{array}{rrll}
      \GS\gamma{thinning} & ::= & & \mbox{--- $\ith$, includes all the variables in $\gamma$}\\
                    &   | & \GS\gamma{var} \GS\gamma{var}^\ast  & \mbox{--- include only the listed variables}\\
                    &   | & - & \mbox{--- $\eth$, exclude all the variables in $\gamma$}\\
                    &   | & - \GS\gamma{var} \GS\gamma{var}^\ast & \mbox{--- exclude only the listed variables} \\
    \end{array}\]
\end{definition}

It is straightforward to compute the source scope of a $\GS\gamma{thinning}$, given that we know its target scope: by a slight abuse of notation, I write
thinnings $\ths\theta$ in places where scopes $\gamma$ are expected, meaning the source scope that $\theta$ effectively selects from some larger target scope.

It is my habit to index grammars by scopes whenever variable binding may be involved.


\section{Binding Lisps}

Given the mission to develop type theories in broad generality, we shall need
to be open minded about what constucts they might offer. Let us adopt a
simple but generic way of representing them: tasteful notations for specific
theories can be layered on top, but for purposes of metatheory, the less
artful we are, the better. There are many tolerable choices we might make, but my
childhood guides me to jump in Lisp puddles, and my
training as a graduate student warns me to be cautious about variable binding.

\begin{definition}[binding Lisp]
  A \textbf{binding Lisp} is some grammar, $\GS\gamma{lisp}$ with at least the following constructs:
  \[\begin{array}{rrll}
      \GS\gamma{lisp} & ::= & \SC{atom} & \mbox{--- a sequence of alphanumeric characters} \\
                &   | & \Pa{\GS\gamma{lisp}\dot\GS\gamma{lisp}} & \mbox{--- a cons-cell} \\\\
                &   | & x \ab \GS{\gamma,x}{lisp} & \mbox{--- an abstraction} \medskip\\
  \end{array}\]
\end{definition}

As ever, we allow syntactic sugar to avoid an excess of dotted pairs.

\begin{definition}[binding Lisp, relaxed notation]
  \[\begin{array}{rrll}
      \GS\gamma{lisp} & ::= & \SC{atom} & \mbox{--- a sequence of alphanumeric characters} \\
                &   | & \Pa{\GS\gamma{lisp}\dot\GS\gamma{lisp}} \\
                &   | & \Pa{\GS\gamma{constr}} & \mbox{--- a construction in parentheses} \\
                &   | & x \ab \GS{\gamma,x}{lisp} & \mbox{--- an abstraction} \medskip\\
      \GS\gamma{constr} & ::= &  & \mbox{--- nil, the empty atom}\\
                &   | & \D \GS\gamma{lisp} & \mbox{--- a non-nil tail} \\
                &   | & \GS\gamma{lisp}\; \GS\gamma{constr} & \mbox{--- cons} \\
  \end{array}\]
\end{definition}

Note that
\begin{enumerate}
\item some atoms may look like variables;
\item $\Pa{}$ is a way to write empty atom;
\item $\D\cdot$ and $\Pa\cdot$ cancel, so that $\Pa{\D l}$ is the same $\GS\gamma{lisp}$ object as $l$. and $\D\Pa c$ is the same $\GS\gamma{constr}$ object as $c$;
\item this basic syntax offers no way to \emph{use} a variable.
\end{enumerate}

Let us remedy the latter posthaste!

\begin{definition}[terms]
  The syntax of \textbf{terms} is a binding Lisp augmented with the following additional construct
  \[\begin{array}{rrll}
      \GS\gamma{term} & ::= & \cdots & \mbox{--- all the binding Lisp constructs} \\
                &   | & \Bk{\GS\gamma{elim}} & \mbox{--- an elimination in brackets} \medskip\\
      \GS\gamma{elim} & ::= & \GS\gamma{var} & \mbox{--- variable usage} \\
                &   | & \ra{\GS\gamma{term}}{\GS\gamma{term}} & \mbox{--- a radical} \\
                &   | & \GS\gamma{elim}\; \GS\gamma{term} & \mbox{--- an action} \\
    \end{array}\]
where a $\GS\gamma{var}$ is one of the variables in $\gamma$, readily relieved of its name by determining its position in $\gamma$.
\end{definition}

The binding Lisp constructs give us the means to express the canonical forms of a theory; the eliminations allow us to express computations. The latter amount to a \emph{head} with a possibly empty sequence, or \emph{spine}, of actions attached. A head may be a variable, as bound by an abstraction, or it may be a \emph{radical} --- a term (possibly canonical) activated for computation by annotation with its type.

We shall define our type theories by saying which terms are types and which terms are \emph{checked} by those types. By contrast, and bidirectionally, we shall always be able to \emph{synthesize} types for meaningful eliminations.

To aid comprehension, I shall write atoms in $\A{teletype}$ font and variables in $\V{italics}$. However, you can always spot a variable binding by the $\ab$ to its right and a variable usage by the $\texttt{[}$ to its left.

\begin{example}[polymorphic identity function]
  We may write $\A{Type}$ for the type of types and $(\A{Pi}\;S\;\V x\ab T)$ for a dependent function type. The type of the polymorphic identity function may thus be written
  \[
    \Pa{\A{Pi}\;\A{Type}\;\V X \ab \Pa{\A{Pi}\;\Bk X\;\V x\ab \Bk X}}
  \]
  with the polymorphic identity function being just
  \[\V X\ab \V x \ab \Bk x
    \]
\end{example}

The role of atoms is to be distinct from one another. The role of variables is to connect usage with abstraction: naming them variables is an informal concession to readability.

All binding Lisps admit thinning.

\begin{definition}[thinning action]
  If $s$ is an object in some $\GS\gamma{lisp}$ and $\theta : \gamma \thin \delta$ is a thinning, then $s\theta$ is the object in the corresponding
  $\GS\delta{lisp}$ given by
  \[\begin{array}{lcl}
      \A{a}\theta & = & \A{a} \\
      \Pa c\theta & = & \Pa{c\theta} \\
      (x\ab t)\theta & = & x\ab(t\theta1)
    \end{array}
    \qquad
    \begin{array}{lcl}                     
      \;\theta & = & \;\\
      (\D t)\theta & = & \D(t\theta)\\
      (t\;c)\theta & = & (t\theta)\;(c\theta)
    \end{array}\]
  This action has a partial inverse, written $\theta?t$ for some $\GS\delta{lisp}$ object t, giving the $\GS\gamma{lisp}$ object $s$ such
  that $s\theta = t$ if it exists. 
  It remains to specify the action of thinnings on the extra constructs of specific $\GS\gamma{lisp}$s. For $\GS\gamma{term}$, we have
  \[\begin{array}{lcl}
      \Bk{e}\theta & = & \Bk{e\theta} \medskip \\
      x\theta & = & x \\
      \ra tT\theta & = & \ra{t\theta}{T\theta} \\
      (e\;t)\theta & = & (e\theta)\;(t\theta)
    \end{array}
  \]
  where, in a de Bruijn representation $x\theta = x$ computes the representation for $x$ in the target scope from the representative of
  $x$ in the source scope.
\end{definition}



\section{Meta-syntax: patterns, thinnings and expressions}

For my purposes, it is not enough to be formal about the type theory which is the object of study. I intend to establish general results about classes of type theories. I must therefore be formal about the \emph{meta}-language in which I am formal about type theories. When we write typing rules or contraction schemes, we do not write terms of our type theory, but rather \emph{formulae} containing \emph{meta}variables, which describe terms in our type theory with particular structural properties. What are those formulae?

It is conventional to generate such formulae by the free extension of the object theory with metavariables, intending all the terms which are given by instantiating the metavariables. I fear I shall be more painstaking, distinguishing two classes of formula, the \emph{patterns} which contain the binding sites of metavariables, from the \emph{expressions} which contain their use sites. The benefit of this distinction will be to allow much tighter control of information flow through the rules of the theory.

\begin{definition}[pattern]
The syntax of \emph{patterns} is a binding Lisp augmented with the following additional construct
  \[\begin{array}{rrll}
      \GS\gamma{pat} & ::= & \cdots & \mbox{--- all the binding Lisp constructs} \\
                &   | & \Bc{\SC{mvar}\; \GS\gamma{thinning}} & \mbox{--- a metavariable binding in braces} \medskip\\
    \end{array}\]
and thinnings acting by
\[
\Bc{m\;\phi}\theta = \Bc{m\;\phi;\theta}
\]
\end{definition}

A metavariable binding gives a name to a metavariable, for the sake of human readability: these names should be mutually distinct. In practice, metavariable bindings may effectively be distinguished by \emph{position} and need not have names at all. As a metavariable binding may occur inside zero or more object variable abstractions, we write a \emph{thinning} to indicate upon which of those object variables a term instantiating the metavariable may depend. The surface syntax of thinnings is conveniently redundant, allowing us to specify permitted dependencies either by inclusion or by exclusion.

Note that patterns allow us to analyse only
\begin{enumerate}
\item the canonical form structure induced by a binding Lisp;
\item dependency on \emph{bound} variables.
\end{enumerate}
That is, the pattern language is designed carefully to ask only questions whose answers are invariant under substitution of free variables.

Crucially, from every pattern, we may compute its \emph{problem} --- the sequence of name/source scope pairs $m/\delta$ given by each
$\Bc{m\;\theta}$ contained therein.

\begin{example}[function types]
  The pattern that describes a dependent function type is
  \[(\A{Pi}\;\Bc S\;\V x\ab\Bc T) \qquad \mbox{or, equivalently, the explicit} \qquad (\A{Pi}\;\{S\}\;\V x\ab\{T\:x\})
  \] whose problem is
  \[S/;\,T/x
    \]
  The pattern that captures the non-dependent special case is
  \[(\A{Pi}\;\Bc S\;\V x\ab\Bc{T - x}) \qquad \mbox{or, equivalently, the implicit} \qquad (\A{Pi}\;\{S\}\;\V x\ab\{T -\})
  \] whose problem is
  \[S/;\,T/
    \]
\end{example}

One way to solve a problem $\Omega$ is to match a $\GS\gamma{term}$ to a $\GS{\emp}{pat}$, mapping each $m/\ths{\theta}$ to a $\GS{\gamma,\ths{\theta}}{term}$, yielding
a sequence of definitions $m(\ths{\theta})=t$. I write $\emp$ for the empty solution.

\newcommand{\ma}{\mathbin{?}}
\begin{definition}[pattern matching]
  If $p$ is a $\GS\delta{pattern}$ with problem $\Omega$ and $t$ a $\GS{\gamma,\delta}{term}$, then $p\ma t$ is the
  $\Omega \Rightarrow \gamma$ solution partially defined as follows:
\[\begin{array}{l@{}c@{}lcl}
\A{a}&\ma&\A{a} & = & \emp \\
\Pa{p_a\D p_d}&\ma&\Pa{t_a\D t_d} & = & p_a\ma t_a;\; p_d\ma t_d\\
(x\ab p) &\ma& (x\ab t) & = & p \ma t \\
\Bc{m\;\theta} &\ma& t & = & m(\ths\theta)=1,\theta\ma t
  \end{array}\]
Note that in the abstraction case, it is not a requirement that the names match, rather, by $\alpha$-equivalence, the names do match.
\end{definition}


Now, let us turn to the expressions.

\begin{definition}[expression]
The syntax of expressions is a binding Lisp augmented with the following additional constructs:
  \[\begin{array}{rrll}
      \GS\gamma{expr} & ::= & \cdots & \mbox{--- all the binding Lisp constructs} \\
                &   | & \Bk{\GS\gamma{image}} & \mbox{--- as `elim' is to `term'} \\
                &   | & \Bc{\SC{mvar}(/\GS\gamma{image})^\ast} & \mbox{--- a metavariable binding in braces} \medskip\\
      \GS\gamma{image} & ::= & \GS\gamma{var} & \mbox{--- variable usage} \\
                &   | & \ra{\GS\gamma{expr}}{\GS\gamma{expr}} & \mbox{--- a radical} \\
                &   | & \GS\gamma{image}\; \GS\gamma{expr} & \mbox{--- an action} \\
    \end{array}\]
with thinning acting structurally.
\end{definition}

Each metavariable must be instantiated with an image for each bound variable on which they depend, as determined by the pattern which binds the metavariable.
Expressions are the language that rules use for synthesis, making use of information gained by pattern matching. E.g., once we have matched a function type with pattern
\[(\A{Pi}\;\Bc S\;\V x\ab\Bc T)
\]
and an argument with pattern
\[
\Bc s
\]
we may deliver the type of the application with the expression
\[
\Bc{T/\ra{\Bc s}{\Bc{S}}}
\]


\section{Judgements and Rules}

A judgement form is a crude variety of `session type', characterising the data involved as
\emph{inputs}, \emph{subjects} or \emph{outputs}. A subject is something to be validated by
the judgement form: not all judgements require a subject, but for every sort of thing, there
must be a judgement form for which it is the subject. An input must have a \emph{client promise},
being a judgement with that input as its subject, explaining what sort of validation the
client is expected to perform in advance of seeking to derive a judgement with that input. An
output must have a corresponding \emph{server promise}, being a judgment with that output as its subject,
explaining what sort of validation must be ensured by whoever claims to have derived the judgement

\newcommand{\type}[1]{\texttt{type}\;#1}
\newcommand{\chek}[2]{#1\;\texttt{:>}\;#2}
\newcommand{\synth}[2]{#1\;\texttt{<:}\;#2}
\newcommand{\subty}[2]{#1\;\texttt{<=}\;#2}
\newcommand{\redu}[2]{#1\;\texttt{\raisebox{0.035in}{\texttildelow}>}\;#2}

There are four judgement forms which are considered primitive. I list them in their ascii notation, with their associated
promises given in braces:
\begin{enumerate}
\item type formation
\[\type{T_{\mbox{subj}}}
\]
takes a putative type as its subject;
\item type checking
\[\{\type{T_{\mbox{in}}\}}\qquad
\chek{T_{\mbox{in}}}{t_{\mbox{subj}}}
\]
takes a putative term as its subject and checks it in a given type;
\item type synthesis
\[\synth{e_{\mbox{subj}}}{T_{\mbox{out}}}\qquad
\{\type{T_{\mbox{out}}}\}
\]
takes an elimination as its subject and tries to find a type for it;
\item subtyping
\[\{\type{S_{\mbox{in}}}\quad \type{T_{\mbox{in}}}\}\qquad
\subty{S_{\mbox{in}}}{T_{\mbox{in}}}
\]
takes two given types and checks whether one is subsumed by the other.
\end{enumerate}


\section{Computation}

Making no presumption of strong normalization, I adopt a \emph{small-step} notion of reduction, generated by the contextual closure of contraction schemes. Whatever other constructs and contractions we may legitimize, we shall certainly have the following:

\begin{definition}[$\upsilon$-contraction]
\[[\ra tT] \leadsto_\upsilon t\]
\end{definition}

A spineless radical ceases to be a radical at all. It loses its type annotation and delivers its now deactivated result into the term which is its host.




\bibliography{TypesNi}

\end{document}